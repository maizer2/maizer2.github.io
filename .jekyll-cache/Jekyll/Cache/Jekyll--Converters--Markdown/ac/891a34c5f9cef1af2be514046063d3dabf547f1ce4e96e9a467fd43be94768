I"
<h3 id="평균-제곱근-오차-rmse-root-mean-square-error">평균 제곱근 오차 (RMSE, Root Mean Square Error)</h3>

<h4 id="핸즈온-머신러닝-2"><a href="https://tensorflow.blog/핸즈온-머신러닝-1장-2장/2-2-큰-그림-보기/">핸즈온 머신러닝 2</a></h4>

<p>회귀 문제의 전형적인 성능 지표로서 오차가 커질수록 값이 커짐으로서 얼마나 많은 오류가 있는지 가늠하게 해준다.</p>

<h4 id="wikipedia"><a href="https://ko.wikipedia.org/wiki/평균_제곱근_편차">wikipedia</a></h4>

<p>추정 값 또는 모델이 예측한 값과 실제 환경에서 관찰되는 값의 차이를 다룰 때 흔히 사용되는 측도이다.</p>

<h3 id="rmse-식">RMSE 식</h3>

<p>$$ RMSE(\hat{\theta}) = \sqrt{MSE(\hat{\theta})} = \sqrt{E((\hat{\theta}-\theta)^2)}\; \;, \; \; \; \; 
\theta_{1} = \begin{bmatrix} 
x_{1, 1} <br />
x_{1, 2} <br />
\vdots <br />
x_{1, n}\end{bmatrix} \; \; \; \; and \; \; \; \; \theta_{2} = \begin{bmatrix} 
x_{2, 1} <br />
x_{2, 2} <br />
\vdots <br />
x_{2, n}\end{bmatrix} $$</p>

<p>$$ RMSE(\theta_{1}, \theta_{2}) = \sqrt{MSE(\theta_{1}, \theta_{2})} = \sqrt{E((\theta_{1} - \theta_{2})^2)} = \sqrt{\frac{\sum_{i=1}^{n}(x_{1, i} - x_{2, i})^2}{n}}. $$</p>

<hr />

<h4 id="해석">해석</h4>

<p>위 식은 처음 보면 어려울 수 있어 핸즈온 머신러닝 2에 써져있는 수식을 가져와서 쓰겠다.</p>

<p>$$ RMSE(X, h) = \sqrt{\frac{1}{m}\sum_{i=1}^{m}(h(x^i)-y^i)^2} $$</p>

<ul>
  <li>X = 데이터셋에 있는 모든 샘플의 모든 특성값을 포함하는 행렬</li>
  <li>m = 데이터셋 샘플 수</li>
  <li>$ x^{i} $ 는 데이터셋에 있는 i번째 샘플의 전체 특성값의 벡터이다.</li>
  <li>$ y^{i} $ 는 해당 레이블(해당 샘플의 기대 출력값)이다.</li>
  <li>h = 시스템의 <strong><em>예측 함수</em></strong>이며 가설이라고도 한다.
    <ul>
      <li>$ h(x^{i}) $ 은 $ x^{i} $ 벡터를 예측 함수에 넣었을 때 출력하는 <strong><em>예측값</em></strong>이다.</li>
    </ul>
  </li>
</ul>

<p>$$ h(x^{(i)}) = \hat{y}^{(i)} \;\; , \;\;\;\; \sqrt{\frac{1}{m}\sum_{i=1}^{m}(\hat{y}^{(i)}-y^{(i)})^{2}} $$</p>

<p>$ \hat{y}^{(i)} $ 는 에측 값이며 $ y^{(i)} $ 는 기대 출력값이다.<br />
두 값을 빼면 예측 오차를 얻을 수 있다.</p>

<p>따라서 예측 오차가 커질수록 1부터 m 까지(모든 샘플 수) 예측 오차의 합의 제곱은 더욱 커진다.</p>

<hr />

<h5 id="참고문헌">참고문헌</h5>

<p>1) wikipedia, 평균 제곱근 편차, <a href="https://ko.wikipedia.org/wiki/평균_제곱근_편차">https://ko.wikipedia.org/wiki/평균_제곱근_편차</a></p>
:ET