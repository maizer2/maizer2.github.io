---
layout: post 
title: "(VITON)VITON: An Image-based Virtual Try-on Network Translation"
categories: [1. Computer Engineering]
tags: [1.7. Paper Review]
---

### [VITON Paper List](https://maizer2.github.io/1.%20computer%20engineering/2022/08/01/Paper-of-VITON.html)

### [$$\mathbf{VITON:\;An\;Image-based\;Virtual\;Try-on\;Network}$$](https://arxiv.org/pdf/1711.08447v4.pdf)

##### $$\mathbf{Bochao\;Wang,\;Huabin\;Zheng,\;Xiaodan\;Liang,\;Yimin\;Chen,\;Liang}$$

##### $$\mathbf{Xintong Han,\;Zuxuan\;Wu,\;Zhe\;Wu,\;Ruichi\;Yu,\;Larry\;S.\;Davis}$$

##### $$\mathbf{University\;of\;Maryland,\;College\;Park}$$

### $\mathbf{Abstract}$

> We present an image-based VIirtual Try-On Network (VITON) without using 3D information in any form, which seamlessly transfers a desired clothing item onto the corresponding region of a person using a coarse-to-fine strategy. Conditioned upon a new clothing-agnostic yet descriptive person representation, our framework first generates a coarse synthesized image with the target clothing item overlaid on that same person in the same pose. We further enhance the initial blurry clothing area with a refinement network. The network is trained to learn how much detail to utilize from the target clothing item, and where to apply to the person in order to synthesize a photo-realistic image in which the target item deforms naturally with clear visual patterns. Experiments on our newly collected dataset demonstrate its promise in the image-based virtual try-on task over state-of-the-art generative models.
>> 우리는 어떤 형태로든 3D 정보를 사용하지 않고 이미지 기반 VITON(Virtual Try-On Network)을 제시하는데, 이는 거친 전략을 사용하여 원하는 의류 항목을 사람의 해당 영역으로 원활하게 전송한다. 새로운 의복에 구애받지 않지만 서술적인 인물 표현을 조건으로 하여, 우리의 프레임워크는 먼저 대상 의복 아이템이 동일한 포즈로 동일한 사람에게 겹쳐진 거친 합성 이미지를 생성한다. 우리는 정제 네트워크를 통해 초기 흐릿한 옷 영역을 더욱 개선한다. 네트워크는 대상 아이템이 선명한 시각적 패턴으로 자연스럽게 변형되는 사진 사실적 이미지를 합성하기 위해 대상 의류 아이템에서 얼마나 많은 디테일을 활용할 수 있는지, 대상자에게 어디에 적용해야 하는지 학습하도록 훈련된다. 새로 수집된 데이터 세트에 대한 실험은 최첨단 생성 모델에 대한 이미지 기반 가상 트라이온 작업에서 그것의 가능성을 입증한다.

### $\mathbf{1\;Introduction}$

> Recent years have witnessed the increasing demands of online shopping for fashion items. Online apparel and accessories sales in US are expected to reach 123 billion in 2022 from 72 billion in 2016 [1]. Despite the convenience online fashion shopping provides, consumers are concerned about how a particular fashion item in a product image would look on them when buying apparel online. Thus, allowing consumers to virtually try on clothes will not only enhance their shopping experience, transforming the way people shop for clothes, but also save cost for retailers. Motivated by this, various virtual fitting rooms/mirrors have been developed by different companies such as TriMirror, Fits Me, etc. However, the key enabling factor behind them is the use of 3D measurements of body shape, either captured directly by depth cameras [40] or inferred from a 2D image using training data [4, 45]. While these 3D modeling techniques enable realistic clothing simulations on the person, the high costs of installing hardwares and collecting 3D annotated data inhibit their large-scale deployment.
>> 최근 몇 년 동안 패션 아이템에 대한 온라인 쇼핑의 수요가 증가하고 있습니다. 미국의 온라인 의류 및 액세서리 매출은 2016년 720억 개에서 2022년 1230억 개에 이를 것으로 예상됩니다 [1]. 온라인 패션 쇼핑이 제공하는 편리함에도 불구하고, 소비자들은 온라인에서 옷을 살 때 제품 이미지에서 특정 패션 아이템이 어떻게 보일지 걱정합니다. 따라서, 소비자들이 가상으로 옷을 입어볼 수 있도록 하는 것은 그들의 쇼핑 경험을 향상시켜 사람들이 옷을 사는 방식을 변화시킬 뿐만 아니라 소매업자들의 비용도 절약할 수 있습니다. 이에 자극을 받아 트라이미러, 핏미러 등 다양한 회사에서 다양한 가상 피팅 룸/미러를 개발했습니다. 그러나 이면의 핵심 활성화 요인은 깊이 카메라[40]에 의해 직접 촬영되거나 훈련 데이터를 사용하여 2D 이미지에서 유추된 체형의 3D 측정을 사용하는 것입니다[4,45]. 이러한 3D 모델링 기술은 사람에 대한 현실적인 의류 시뮬레이션을 가능케 하지만, 하드웨어를 설치하고 3D 주석이 달린 데이터를 수집하는 데 드는 높은 비용은 대규모 배포를 억제합니다.

![Figure 1](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-08-01-(VITON)VITON-An-Image-based-Virtual-Try-on-Network/Figure-1.PNG)

> Figure 1: Virtual try-on results generated by our method. Each row shows a person virtually trying on different clothing items. Our model naturally renders the items onto a person while retaining her pose and preserving detailed characteristics of the target clothing items.
>> 그림 1: 우리의 방법으로 생성된 Virtual try-on 결과입니다. 각 행은 한 사람이 다른 옷 아이템을 가상으로 입어보는 것을 보여줍니다. 우리의 모델은 포즈를 유지하고 대상 의류 아이템의 세부 특성을 유지하면서 자연스럽게 아이템을 사람에게 렌더링합니다.

> We present an image-based virtual try-on approach, relying merely on plain RGB images without leveraging any 3D information. Our goal is to synthesize a photo-realistic new image by overlaying a product image seamlessly onto the corresponding region of a clothed person (as shown in Figure 1). The synthetic image is expected to be perceptually convincing, meeting the following desiderata: (1) body parts and pose of the person are the same as in the original image; (2) the clothing item in the product image deforms naturally, conditioned on the pose and body shape of the person; (3) detailed visual patterns of the desired product are clearly visible, which include not only low-level features like color and texture but also complicated graphics like embroidery, logo, etc. The non-rigid nature of clothes, which are frequently subject to deformations and occlusions, poses a significant challenge to satisfying these requirements simultaneously, especially without 3D information.
>> 우리는 3D 정보를 활용하지 않고 단순한 RGB 이미지에만 의존하는 이미지 기반 가상 체험 방식을 제시합니다. 우리의 목표는 제품 이미지를 옷 입은 사람의 해당 부위에 매끄럽게 오버레이하여 사실적인 새로운 이미지를 합성하는 것입니다(그림 1 참조). 합성 이미지는 지각적으로 설득력이 있을 것으로 예상되며, (1) 인체의 신체 부위와 포즈가 원본 이미지와 동일함, (2) 제품 이미지 내의 의류 품목은 사람의 포즈와 체형에 따라 자연스럽게 변형됨, (3) 원하는 제품의 세부 시각적 패턴이 명확함.색상과 텍스쳐와 같은 낮은 수준의 기능뿐만 아니라 자수, 로고 등과 같은 복잡한 그래픽도 포함하는 초기 가시성을 제공합니다. 종종 변형과 폐색에 영향을 받는 옷의 비강성 특성은 이러한 요구 사항을 동시에 만족시키는 데 중요한 과제를 제기합니다. 특히 3D 정보 없이 말입니다.

> Conditional Generative Adversarial Networks (GANs), which have demonstrated impressive results on image generation [37, 26], image-to-image translation [20] and editing tasks [49], seem to be a natural approach for addressing this problem. In particular, they minimize an adversarial loss so that samples generated from a generator are indistinguishable from real ones as determined by a discriminator, conditioned on an input signal [37, 33, 20, 32]. However, they can only transform information like object classes and attributes roughly, but are unable to generate graphic details and accommodate geometric changes [50]. This limits their ability in tasks like virtual try-on, where visual details and realistic deformations of the target clothing item are required in generated samples.
>> 이미지 생성[37, 26], 이미지 대 이미지 변환[20] 및 편집 작업[49]에서 인상적인 결과를 보여준 조건부 생성 적대적 네트워크(GAN)는 이 문제를 해결하기 위한 자연스러운 접근 방식인 것 같습니다. 특히, 그들은 발생기에서 생성된 샘플이 입력 신호에 의해 조건화된 판별기에 의해 결정된 실제 샘플과 구별할 수 없도록 적대적 손실을 최소화합니다 [37, 33, 20, 32]. 그러나 객체 클래스 및 속성과 같은 정보만 대략적으로 변환할 수 있지만 그래픽 세부 정보를 생성하고 기하학적 변화를 수용할 수는 없습니다 [50]. 이는 생성된 샘플에서 대상 의류 아이템의 시각적 세부 정보와 현실적인 변형이 필요한 가상 트라이온과 같은 작업에서 그들의 능력을 제한합니다.

> To address these limitations, we propose a virtual try-on network (VITON), a coarse-to-fine framework that seamlessly transfers a target clothing item in a product image to the corresponding region of a clothed person in a 2D image. Figure 2 gives an overview of VITON. In particular, we first introduce a clothing-agnostic representation consisting of a comprehensive set of features to describe different characteristics of a person. Conditioned on this representation, we employ a multi-task encoder-decoder network to generate a coarse synthetic clothed person in the same pose wearing the target clothing item, and a corresponding clothing region mask. The mask is then used as a guidance to warp the target clothing item to account for deformations. Furthermore, we utilize a refinement network which is trained to learn how to composite the warped clothing item to the coarse image so that the desired item is transfered with natural deformations and detailed visual patterns. To validate our approach, we conduct a user study on our newly collected dataset and the results demonstrate that VITON generates more realistic and appealing virtual try-on results outperforming state-of-the-art methods.
>> 이러한 한계를 해결하기 위해 제품 이미지에서 대상 의류 품목을 2D 이미지에서 옷 입은 사람의 해당 영역으로 원활하게 전송하는 거친 대 미세한 프레임워크인 VITON(Virtual Try-on Network)을 제안한다. 그림 2는 VITON에 대한 개요를 제공합니다. 특히, 우리는 먼저 개인의 다양한 특성을 설명하기 위한 포괄적인 특징 집합으로 구성된 의복에 구애받지 않는 표현을 소개합니다. 이 표현을 조건으로, 우리는 다중 작업 인코더-디코더 네트워크를 사용하여 대상 의류 아이템과 해당 의류 영역 마스크를 착용하는 동일한 포즈의 거친 합성 의류 사람을 생성한다. 그런 다음 마스크는 변형을 설명하기 위해 대상 의류 항목을 뒤틀리는 지침으로 사용됩니다. 또한, 우리는 원하는 아이템이 자연스러운 변형과 세부적인 시각적 패턴으로 전달되도록 왜곡된 의류 아이템을 거친 이미지로 합성하는 방법을 학습하도록 훈련된 정제 네트워크를 활용합니다. 우리의 접근 방식을 검증하기 위해, 우리는 새로 수집된 데이터 세트에 대한 사용자 연구를 수행했으며, 그 결과는 VITON이 최첨단 방법을 능가하는 더 현실적이고 매력적인 가상 체험 결과를 생성한다는 것을 보여준다.

### $\mathbf{2.\;Related\;Work}$

> **Fashion analysis.** Extensive studies have been conducted on fashion analysis due to its huge profit potentials. Most existing methods focus on clothing parsing [44, 28], clothing recognition by attributes [31], matching clothing seen on the street to online products [30, 14], fashion recommendation [19], visual compatibility learning [43, 16], and fashion trend prediction [2]. Compared to these lines of work, we focus on virtual try-on with only 2D images as input. Our task is also more challenging compared to recent work on interactive search that simply modifies attributes (e.g., color and textures) of a clothing item [25, 48, 15], since virtual try-on requires preserving the details of a target clothing image as much as possible, including exactly the same style, embroidery, logo, text, etc.
>> **패션 분석입니다.** 막대한 수익 잠재력 때문에 패션 분석에 대한 광범위한 연구가 진행되었습니다. 대부분의 기존 방법은 의류 구문 분석 [44, 28], 속성에 의한 의류 인식 [31], 길에서 본 옷을 온라인 제품 [30, 14], 패션 추천 [19], 시각적 호환성 학습 [43, 16] 및 패션 트렌드 예측 [2]에 중점을 둡니다. 이러한 작업 라인에 비해 2D 이미지만 입력으로 가상 트라이온에 중점을 둡니다. 또한 가상 트라이온에서는 정확히 동일한 스타일, 자수, 로고, 텍스트 등을 포함하여 대상 의류 이미지의 세부 정보를 최대한 보존해야 하기 때문에 의류 항목 [25, 48, 15]의 속성(예: 색상 및 텍스처)을 간단히 수정하는 대화형 검색에 대한 최근 작업에 비해 우리의 작업은 더 어렵습니다.

> **Image synthesis.** GANs [12] are one of most popular deep generative models for image synthesis, and have demonstrated promising results in tasks like image generation [8, 36] and image editing [49, 34]. To incorporate desired properties in generated samples, researchers also utilize different signals, in the form of class labels [33], text [37], attributes [41], etc., as priors to condition the image generation process. There are a few recent studies investigating the problem of image-to-image translation using conditional GANs [20], which transform a given input image to another one with a different representation. For example, producing an RGB image from its corresponding edge map, semantic label map, etc., or vice versa. Recently, Chen and Kolton [6] trained a CNN using a regression loss as an alternative to GANs for this task without adversarial training. These methods are able to produce photo-realistic images, but have limited success when geometric changes occur [50]. Instead, we propose a refinement network that pays attention to clothing regions and deals with clothing deformations for virtual try-on.
>> **이미지 합성입니다.** GAN[12]은 이미지 합성을 위한 가장 인기 있는 심층 생성 모델 중 하나이며, 이미지 생성[8, 36] 및 이미지 편집[49, 34]과 같은 작업에서 유망한 결과를 입증했습니다. 생성된 샘플에 원하는 특성을 통합하기 위해 연구자들은 이미지 생성 프로세스를 조건화하기 전에 클래스 레이블 [33], 텍스트 [37], 속성 [41] 등의 다양한 신호를 사용합니다. 주어진 입력 이미지를 다른 표현을 가진 다른 이미지로 변환하는 조건부 GAN[20]을 사용한 이미지 간 변환 문제를 조사하는 최근 연구가 몇 가지 있습니다. 예를 들어, 해당 에지 맵, 의미 레이블 맵 등에서 RGB 이미지를 생성하거나 그 반대입니다. 최근, Chen과 Kolton[6]은 적대적 훈련 없이 이 작업을 위해 GAN의 대안으로 회귀 손실을 사용하여 CNN을 훈련했습니다. 이러한 방법은 사진 사실적인 이미지를 생성할 수 있지만 기하학적 변화가 발생할 때 성공에는 한계가 있습니다 [50]. 대신, 우리는 의류 영역에 주목하고 가상 시착을 위한 의류 변형을 다루는 정제 네트워크를 제안합니다.

> In the context of image synthesis for fashion applications, Yoo et al. [46] generated a clothed person conditioned on a product image and vice versa regardless of the person’s pose. Lassner et al. [26] described a generative model of people in clothing, but it is not clear how to control the fashion items in the generated results. A more related work is FashionGAN [51], which replaced a fashion item on a person with a new one specified by text descriptions. In contrast, we are interested in the precise replacement of the clothing item in a reference image with a target item, and address this problem with a novel coarse-to-fine framework.
>> 패션 애플리케이션을 위한 이미지 합성이라는 맥락에서 Yoo 등이 있습니다. [46] 제품 이미지에 따라 옷을 입은 사람을 생성했으며 그 반대의 경우도 해당 사람의 자세에 관계없이 생성했습니다. 라스너 외입니다. [26]는 옷을 입은 사람들의 생성 모델을 설명했지만, 생성된 결과에서 패션 아이템을 제어하는 방법은 명확하지 않습니다. 더 관련있는 일은 패션입니다.GAN [51]은 사람의 패션 아이템을 텍스트 설명으로 지정된 새 아이템으로 교체했습니다. 대조적으로, 우리는 참조 이미지에서 의류 항목을 대상 항목으로 정밀한 대체에 관심이 있으며, 새로운 거친 프레임워크로 이 문제를 해결합니다.

> **Virtual try-on.** There is a large body of work on virtual try-on, mostly conducted in computer graphics. Guan et al. proposed DRAPE [13] to simulate 2D clothing designs on 3D bodies in different shapes and poses. Hilsmann and P. Eisert [18] retextured the garment dynamically based on a motion model for real-time visualization in a virtual mirror environment. Sekine et al. [40] introduced a virtual fitting system that adjusts 2D clothing images to users through inferring their body shapes with depth images. Recently, Pons-Moll et al. [35] utilized a multi-part 3D model of clothed bodies for clothing capture and retargeting. Yang et al. [45] recovered a 3D mesh of the garment from a single view 2D image, which is further re-targeted to other human bodies. In contrast to relying on 3D measurements to perform precise clothes simulation, in our work, we focus on synthesizing a perceptually correct photo-realistic image directly from 2D images, which is more computationally efficient. In computer vision, limited work has explored the task of virtual try-on. Recently, Jetchev and Bergmann [21] proposed a conditional analogy GAN to swap fashion articles. However, during testing, they require the product images of both the target item and the original item on the person, which makes it infeasible in practical scenar ios. Moreover, without injecting any person representation or explicitly considering deformations, it fails to generate photo-realistic virtual try-on results.
>> **Virtual try-on.** 대부분 컴퓨터 그래픽에서 수행되는 가상 체험에 대한 많은 작업이 있습니다. Guan 등은 DRAPE[13]를 제안하여 다양한 모양과 포즈의 3D 신체에서 2D 의류 디자인을 시뮬레이션했습니다. 힐스만과 P입니다. 아이저트[18]는 가상 미러 환경에서 실시간 시각화를 위한 모션 모델을 기반으로 의복을 동적으로 재 텍스처링했습니다. 세키네 외입니다. [40] 깊이 이미지로 체형을 유추하여 사용자에게 2D 의류 이미지를 조정하는 가상 피팅 시스템을 도입하였습니다. 최근에 폰스몰 등이 있습니다. [35] 의복 캡처 및 리타겟팅을 위해 의복 몸체의 다중 파트 3D 모델을 활용했습니다. 양 외에요. [45] 단일 보기 2D 영상에서 의복의 3D 메시를 복구하여 다른 인체에 추가로 재표적했습니다. 정밀한 의류 시뮬레이션을 수행하기 위해 3D 측정에 의존하는 것과 달리, 우리의 작업에서는 2D 이미지에서 직접 지각적으로 정확한 사진 사실적 이미지를 합성하는 데 중점을 두는데, 이는 계산 효율성이 더 높습니다. 컴퓨터 비전에서는 제한된 작업이 가상 시도 작업을 탐구했습니다. 최근, Jetchev와 Bergmann[21]은 패션 기사를 교환하기 위해 조건부 유사 GAN을 제안했습니다. 그러나 테스트하는 동안 대상 품목과 원본 품목의 제품 이미지가 모두 필요하므로 실제 시나리오에서는 사용할 수 없습니다. 더욱이, 어떠한 인물 표현도 주입하지 않거나 변형을 명시적으로 고려하지 않으면, 그것은 사진 사실적인 가상 체험 결과를 생성하지 못합니다.

![Figure 2](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-08-01-(VITON)VITON-An-Image-based-Virtual-Try-on-Network/Figure-2.PNG)

> Figure 2: An overview of VITON. VITON consists of two stages: (a) an encoder-decoder generator stage (Sec 3.2), and (b) a refinement stage (Sec 3.3).
>> 그림 2: VITON의 개요. VITON은 (a) 인코더-디코더 생성기 단계 (3.2항)와 (b) 정제 단계 (3.3항)의 두 단계로 구성됩니다.

### $\mathbf{3.\;VITON}$

> The goal of VITON is, given a reference image $I$ with a clothed person and a target clothing item $c$, to synthesize a new image $\hat{I}$, where $c$ is transferred naturally onto the corresponding region of the same person whose body parts and pose information are preserved. Key to a high-quality synthesis is to learn a proper transformation from product images to clothes on the body. A straightforward approach is to leverage training data of a person with fixed pose wearing different clothes and the corresponding product images, which, however, is usually difficult to acquire.
>> VITON의 목표는 옷을 입은 사람과 대상 의류 품목 $c$와 함께 참조 이미지 $I$가 주어지면 새로운 이미지 $\hat{I}$를 합성하는 것이며, 여기서 $c$는 신체 부위 및 자세 정보가 보존된 동일한 사람의 해당 영역으로 자연스럽게 전송됩니다. 고품질 합성의 핵심은 제품 이미지에서 몸에 있는 옷으로 적절한 변환을 배우는 것입니다. 간단한 접근 방식은 다른 옷을 입은 고정된 자세를 가진 사람과 해당 제품 이미지를 활용하는 것이지만, 일반적으로 획득하기 어렵습니다.

> In a practical virtual try-on scenario, only a reference image and a desired product image are available at test time. Therefore, we adopt the same setting for training, where a reference image $I$ with a person wearing $c$ and the product image of $c$ are given as inputs (we will use $c$ to refer to the product image of $c$ in the following paper). Now the problem becomes given the product image $c$ and the person’s information, how to learn a generator that not only produces $I$ during training, but more importantly is able to generalize at test time – synthesizing a perceptually convincing image with an arbitrary desired clothing item.
>> 실제 가상 체험 시나리오에서는 테스트 시 참조 이미지와 원하는 제품 이미지만 사용할 수 있습니다. 따라서, 우리는 $c$를 입은 사람이 있는 참조 이미지 $I$와 $c$의 제품 이미지가 입력으로 제공되는 동일한 교육 설정을 채택합니다(다음 논문에서 $c$의 제품 이미지를 참조하기 위해 $c$를 사용할 것입니다). 이제 문제는 제품 이미지 $c$와 개인의 정보, 훈련 중에 $I$를 생성할 뿐만 아니라 테스트 시간에 일반화할 수 있는 생성기를 학습하는 방법, 즉 임의의 원하는 옷 아이템으로 지각적으로 납득할 수 있는 이미지를 합성하는 방법이 됩니다.

> To this end, we first introduce a clothing-agnostic person representation (Sec 3.1). We then synthesize the reference image with an encoder-decoder architecture (Sec 3.2), conditioned on the person representation as well as the target clothing image. The resulting coarse result is further improved to account for detailed visual patterns and deformations with a refinement network (Sec 3.3). The overall framework is illustrated in Figure 2.
>> 이를 위해 먼저 옷에 구애받지 않는 사람 표현을 도입합니다(3.1항). 그런 다음 우리는 대상 의류 이미지뿐만 아니라 인물 표현에 따라 조정된 인코더-디코더 아키텍처(Sec 3.2)와 기준 이미지를 합성합니다. 결과 거친 결과는 정제 네트워크를 통해 상세한 시각적 패턴과 변형을 고려하여 더욱 개선됩니다(3.3항). 전체적인 프레임워크는 그림 2에 나와 있습니다.

![Figure 3](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-08-01-(VITON)VITON-An-Image-based-Virtual-Try-on-Network/Figure-3.PNG)

> Figure 3: A clothing-agnostic person representation. Given a reference image $I$, we extract the pose, body shape and face and hair regions of the person, and use this information as part of input to our generator.
>> 그림 3: 옷에 구애받지 않는 사람의 표현입니다. 참조 이미지 $I$이 주어지면, 우리는 그 사람의 포즈, 체형, 얼굴과 머리카락 영역을 추출하고, 이 정보를 우리의 발전기에 대한 입력의 일부로 사용합니다.

#### $\mathbf{3.1.\;Person\;Representation}$

> A main technical challenge of a virtual try-on synthesis is to deform the target clothing image to fit the pose of a person. To this end, we introduce a clothing-agnostic person representation, which contains a set of features (Figure 3), including pose, body parts, face and hair, as a prior to constrain the synthesis process.
>> 가상 트라이온 합성의 주요 기술적 과제는 대상 의류 이미지를 사람의 포즈에 맞게 변형하는 것입니다. 이를 위해, 우리는 합성 과정을 제약하기 전에 포즈, 신체 부위, 얼굴 및 머리카락을 포함한 일련의 특징(그림 3)을 포함하는 의복에 구애받지 않는 사람 표현을 도입합니다.

> **Pose heatmap.** Variations in human poses lead to different deformations of clothing, and hence we explicitly model pose information with a state-of-the-art pose estimator [5]. The computed pose of a person is represented as coordinates of 18 keypoints. To leverage their spatial layout, each keypoint is further transformed to a heatmap, with an 11×11 neighborhood around the keypoint filled in with ones and zeros elsewhere. The heatmaps from all keypoints are further stacked into an 18-channel pose heatmap.
>> **히트맵을 배치합니다.** 인간 포즈의 변형은 다양한 옷의 변형을 초래하므로, 우리는 최첨단 포즈 추정기로 포즈 정보를 명시적으로 모델링합니다[5]. 계산된 사람의 자세는 18개의 핵심 포인트의 좌표로 표현됩니다. 공간 레이아웃을 활용하기 위해 각 키포인트는 열 지도(heatmap)로 더욱 변환되며 키포인트 주변의 11 x 11 이웃은 다른 곳에서는 1과 0으로 채워집니다. 모든 핵심 지점의 히트 맵은 18채널 포즈 히트 맵에 추가로 쌓입니다.

> **Human body representation.** The appearance of clothing highly depends on body shapes, and thus how to transfer the target fashion item depends on the location of different body parts (e.g., arms or torso) and the body shape. A state-of-the-art human parser [11] is thus used to compute a human segmentation map, where different regions represent different parts of human body like arms, legs, etc. We further convert the segmentation map to a 1-channel binary mask, where ones indicate human body (except for face and hair) and zeros elsewhere. This binary mask derived directly from $I$ is downsampled to a lower resolution (16×12 as shown in Figure 3) to avoid the artifacts when the body shape and target clothing conflict as in [51].
>> **인체의 표현입니다.** 옷의 외관은 체형에 따라 크게 달라지며, 따라서 타겟 패션 아이템을 전달하는 방법은 서로 다른 신체 부위(예: 팔이나 몸통)와 체형에 따라 달라집니다. 따라서 최첨단 인간 파서[11]는 서로 다른 영역이 팔, 다리 등과 같은 인체의 다른 부분을 나타내는 인간 분할 맵을 계산하는 데 사용됩니다. 우리는 또한 분할 맵을 1채널 이진 마스크로 변환하는데, 여기서 1채널은 사람의 신체(얼굴과 머리카락 제외)를 나타내고 다른 곳에서는 0을 나타냅니다. I에서 직접 파생된 이 이진 마스크는 [51]에서와 같이 체형과 대상 의복이 충돌할 때 아티팩트를 피하기 위해 더 낮은 해상도(16×12)로 다운샘플링됩니다.

> **Face and hair segment.** To maintain the identity of the person, we incorporate physical attributes like face, skin color, hair style, etc. We use the human parser [11] to extract the RGB channels of face and hair regions of the person to inject identity information when generating new images.
>> **얼굴과 머리부분입니다.** 그 사람의 정체성을 유지하기 위해 얼굴, 피부색, 헤어스타일 등 신체적 특성을 통합합니다. 우리는 새로운 이미지를 생성할 때 신원 정보를 주입하기 위해 사람의 얼굴과 머리 부분의 RGB 채널을 추출하기 위해 휴먼 파서[11]를 사용합니다.

> Finally, we resize these three feature maps to the same resolution and then concatenate them to form a clothing agnostic person representation $p$ such that $p∈R^{m×n×k}$, where $m=256$ and $n=192$ denote the height and width of the feature map, and $k=18+1+3=22$ represents the number of channels. The representation contains abundant information about the person upon which convolutions are performed to model their relations. Note that our representation is more detailed than previous work [32, 51].
>> 마지막으로, 우리는 이 세 개의 피처 맵의 크기를 동일한 해상도로 조정한 다음 이들을 연결하여 $p×R^{m×n×k}$, 여기서 $m=256$ 및 $n=192$는 피처 맵의 높이와 폭을 나타내고 $k=18+1+3=22$는 채널 수를 나타내도록 합니다. 표현에는 관계를 모델링하기 위해 컨볼루션이 수행되는 사람에 대한 풍부한 정보가 포함되어 있습니다. 우리의 표현은 이전 작품보다 더 상세합니다 [32, 51].

#### $\mathbf{3.2.\;Multi-task\;Encoder-Decoder\;Generator}$

> Given the clothing-agnostic person representation $p$ and the target clothing image $c$, we propose to synthesize the reference image $I$ through reconstruction such that a natural transfer from $c$ to the corresponding region of $p$ can be learned. In particular, we utilize a multi-task encoderdecoder framework that generates a clothed person image along with a clothing mask of the person as well. In addition to guiding the network to focus on the clothing region, the predicted clothing mask will be further utilized to refine the generated result, as will be discussed in Sec 3.3. The encoder-decoder is a general type of U-net architecture [38] with skip connections to directly share information between layers through bypassing connections.
>> 의복에 구애받지 않는 사람 표현 $p$와 대상 의복 이미지 $c$를 고려하여 $c$에서 $p$의 해당 영역으로의 자연스러운 이동을 학습할 수 있도록 재구성을 통해 기준 이미지 $I$를 합성할 것을 제안합니다. 특히, 우리는 그 사람의 옷 마스크와 함께 옷을 입은 사람 이미지를 생성하는 멀티태스킹 인코더 디코더 프레임워크를 활용합니다. 네트워크가 의류 영역에 초점을 맞추도록 안내하는 것 외에도, 3.3항에서 논의될 것처럼 예측된 의류 마스크는 생성된 결과를 개선하기 위해 더욱 활용될 것입니다. 인코더-디코더(encoder-decoder)는 건너뛰기 연결이 있는 U-net 아키텍처의 일반적인 유형입니다[38]. 우회 연결을 통해 계층 간에 정보를 직접 공유합니다.

> Formally, let $G_{C}$ denote the function approximated by the encoder-decoder generator. It takes the concatenated $c$ and $p$ as its input and generates a 4-channel output $(I′, M)=G_{C}(c, p)$, where the first 3 channels represent a synthesized image $I′$ and the last channel $M$ represents a segmentation mask of the clothing region as shown at the top of Figure 2. We wish to learn a generator such that $I′$ is close to the reference image $I$ and $M$ is close to $M_{0}$ ($M_{0}$ is the pseudo ground truth clothing mask predicted by the human parser on $I$). A simple way to achieve this is to train the network with an L1 loss, which generates decent results when the target is a binary mask like $M_{0}$. However, when the desired output is a colored image, L1 loss tends to produce blurry images [20]. Following [22, 27, 7], we utilize a perceptual loss that models the distance between the corresponding feature maps of the synthesized image and the ground truth image, computed by a visual perception network. The loss function of the encoder-decoder can now be written as the sum of a perceptual loss and an L1 loss:
>> 형식적으로 $G_{C}$가 인코더-디코더 생성기가 근사한 함수를 나타내도록 합니다. 연결된 $c$와 $p$를 입력으로 사용하고 4채널 출력 $(I′, M)=G_{C}(c, p)$를 생성합니다. 여기서 처음 3채널은 합성 이미지 $I'$를 나타내고 마지막 채널 $M$은 그림 2의 상단에 표시된 것처럼 의류 영역의 분할 마스크를 나타냅니다. 우리는 $I'$가 기준 이미지 $I$에 가깝고 $M_{0}$($M_{0}$은 인간 파서가 $I$에 대해 예측한 의사 지상 진실 의류 마스크)에 가깝도록 생성기를 학습하고자 합니다. 이를 달성하는 간단한 방법은 L1 손실로 네트워크를 훈련하는 것이며, 대상이 $M_{0}$과 같은 이진 마스크일 때 적절한 결과를 생성합니다. 그러나 원하는 출력이 컬러 이미지인 경우 L1 손실은 흐릿한 이미지를 생성하는 경향이 있습니다 [20]. [22, 27, 7]에 이어 시각적 인식 네트워크에 의해 계산된 합성 이미지의 해당 특징 맵과 실측 이미지 사이의 거리를 모델링하는 지각 손실을 활용합니다. 인코더-디코더 손실 함수는 이제 지각 손실과 L1 손실의 합으로 기록될 수 있습니다.

$$L_{G_{C}}=\sum_{i=0}^{5} λ_{i}\vert{}\vert{}φ_{i}(I′)−φ_{i}(I)\vert{}\vert{}1+\vert{}\vert{}M−M_{0}\vert{}\vert{}1,$$

> where $φ_{i}(y)$ in the first term is the feature map of image $y$ of the i-th layer in the visual perception network $φ$, which is a VGG19 [42] network pre-trained on ImageNet. For layers $i>1$, we utilize ‘conv1_2’, ‘conv2_2’, ‘conv3_2’, ‘conv4_2’, ‘conv5_2’ of the VGG model while for layer 0, we directly use RGB pixel values. 
>> 여기서 첫 번째 용어의 $φ_{i}(y)$는 ImageNet에서 사전 훈련된 VGG19[42] 네트워크인 시각적 인식 네트워크 $φ$에 있는 i번째 계층의 이미지 $y$의 기능 맵입니다. 레이어 $i>1$의 경우 VGG 모델의 'conv1_2', 'conv2_2', 'conv3_2', 'conv4_2', 'conv5_2'를 사용하고 레이어 0의 경우 RGB 픽셀 값을 직접 사용합니다.

> The hyperparameter $λ_{i}$ controls the contribution of the i-th layer to the total loss. The perceptual loss forces the synthesized image to match RGB values of the ground truth image and their activations at different layers in a visual perception model as well, allowing the synthesis network to learn realistic patterns. The second term in Eqn. 1 is a regression loss that encourages the predicted clothing mask $M$ to be the same as $M_{0}$.
>> 하이퍼 파라미터 $λ_{i}$는 총 손실에 대한 i번째 계층의 기여를 제어합니다. 지각 손실은 합성된 이미지가 시각적 인식 모델의 다른 계층에서 RGB 값과 일치하도록 하여 합성 네트워크가 현실적인 패턴을 학습할 수 있도록 합니다. Eqn.1의 두 번째 항은 예측된 의류 마스크 $M$이 $M_{0}$과 동일하도록 권장하는 회귀 손실입니다.

![Figure 4](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-08-01-(VITON)VITON-An-Image-based-Virtual-Try-on-Network/Figure-4.PNG)

> Figure 4: Warping a clothing image. Given the target clothing image and a clothing mask predicted in the first stage, we use shape context matching to estimate the TPS transformation and generate a warped clothing image.
>> 그림 4: 옷 이미지를 뒤틀립니다. 첫 번째 단계에서 예측된 대상 의류 이미지와 의류 마스크가 주어졌을 때, 우리는 형상 컨텍스트 매칭을 사용하여 TPS 변환을 추정하고 왜곡된 의류 이미지를 생성합니다.

> By minimizing Eqn. 1, the encoder-decoder learns how to transfer the target clothing conditioned on the person representation. While the synthetic clothed person conforms to the pose, body parts and identity in the original image (as illustrated in the third column of Figure 5), details of the target item such as text, logo, etc. are missing. This might be attributed to the limited ability to control the process of synthesis in current state-of-the-art generators. They are typically optimized to synthesize images that look similar globally to the ground truth images without knowing where and how to generate details. To address this issue, VITON uses a refinement network together with the predicted clothing mask $M$ to improve the coarse result $I′$.
>> Eqn.1을 최소화함으로써 인코더-디코더는 인물 표현에 따라 조정된 대상 의류를 전달하는 방법을 학습합니다. 합성 옷을 입은 사람은 원본 이미지의 자세, 신체 부위 및 정체성을 준수하지만(그림 5의 세 번째 열에 표시됨), 텍스트, 로고 등과 같은 대상 항목의 세부 정보가 누락됩니다. 이는 현재의 최첨단 발전기에서 합성 과정을 제어하는 능력이 제한되었기 때문일 수 있습니다. 이들은 일반적으로 어디에서 어떻게 세부 정보를 생성해야 하는지 알지 못한 채 전체적으로 지상 진실 이미지와 유사하게 보이는 이미지를 합성하도록 최적화됩니다. 이 문제를 해결하기 위해 VITON은 예측된 의류 마스크 $M$과 함께 정제 네트워크를 사용하여 거친 결과 $I'$를 개선합니다.

#### $\mathbf{3.3.\;Refinement\;Network}$

> The refinement network $G_{R}$ in VITON is trained to render the coarse blurry region leveraging realistic details from a deformed target item.
>> VITON의 정제 네트워크 $G_{R}$는 변형된 대상 항목에서 현실적인 세부 정보를 활용하여 거칠고 흐릿한 영역을 렌더링하도록 훈련됩니다.

> **Warped clothing item.** We borrow information directly from the target clothing image $c$ to fill in the details in the generated region of the coarse sample. However, directly pasting the product image is not suitable as clothes deform conditioned on the person pose and body shape. Therefore, we warp the clothing item by estimating a thin plate spline (TPS) transformation with shape context matching [3], as illustrated in Figure 4. More specifically, we extract the foreground mask of $c$ and compute shape context TPS warps [3] between this mask and the clothing mask $M$ of the person, estimated with Eqn. 1. These computed TPS parameters are further applied to transform the target clothing image $c$ into a warped version $c′$. As a result, the warped clothing image conforms to pose and body shape information of the person and fully preserves the details of the target item. The idea is similar to recent 2D/3D texture warping methods for face synthesis [52, 17], where 2D facial keypoints and 3D pose estimation are utilized for warping. In contrast, we rely on the shape context-based warping due to the lack of accurate annotations for clothing items. Note that a potential alternative to estimating TPS with shape context matching is to learn TPS parameters through a Siamese network as in [23]. However, this is particularly challenging for nonrigid clothes, and we empirically found that directly using context shape matching offers better warping results for virtual try-on.
>> **Warped clothing item.** 우리는 거친 샘플의 생성된 영역에 세부 정보를 채우기 위해 대상 의류 이미지 $c$에서 직접 정보를 빌립니다. 그러나 제품 이미지를 직접 붙이는 것은 사람의 자세와 체형에 따라 옷이 변형되기 때문에 적합하지 않습니다. 따라서 그림 4와 같이 형태 컨텍스트와 일치하는 [3]의 얇은 판 스플라인(TPS) 변환을 추정하여 의류 품목을 왜곡시킵니다. 보다 구체적으로, 우리는 $c$의 전경 마스크를 추출하고 Eqn.1로 추정된 이 마스크와 사람의 의류 마스크 $M$ 사이의 형상 컨텍스트 TPS 뒤틀림[3]을 계산한다. 이러한 계산된 TPS 매개 변수는 대상 의류 이미지 $c$를 왜곡된 버전 $c'$로 변환하기 위해 추가로 적용됩니다. 결과적으로, 뒤틀린 옷 이미지는 사람의 자세와 체형 정보에 부합하며 대상 아이템의 디테일을 완벽하게 보존합니다. 이 아이디어는 얼굴 합성을 위한 최근의 2D/3D 텍스처 워핑 방법[52, 17]과 유사하며, 2D 얼굴 키포인트와 3D 포즈 추정이 워핑에 사용됩니다. 대조적으로, 우리는 옷 아이템에 대한 정확한 주석이 없기 때문에 형태 컨텍스트 기반 뒤틀림에 의존합니다. 형상 컨텍스트 일치를 사용하여 TPS를 추정하는 잠재적인 대안은 [23]에서와 같이 샴 네트워크를 통해 TPS 매개 변수를 학습하는 것입니다. 그러나, 이것은 비강성 옷의 경우 특히 어렵고, 우리는 경험적으로 문맥 형상 일치를 직접 사용하면 가상 시도에 더 나은 뒤틀림 결과를 제공한다는 것을 발견했습니다.

![Figure 5](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-08-01-(VITON)VITON-An-Image-based-Virtual-Try-on-Network/Figure-5.PNG)

> Figure 5: Output of different steps in our method. Coarse sythetic results generated by the encoder-decoder are further improved by learning a composition mask to account for details and deformations.
>> 그림 5: 우리 방법의 다양한 단계의 출력입니다. 인코더-디코더에 의해 생성된 거친 합성 결과는 세부 사항 및 변형을 설명하기 위해 합성 마스크를 학습함으로써 더욱 개선됩니다.

> **Learn to composite.** The composition of the warped clothing item $c′$ onto the coarse synthesized image $I′$ is expected to combine $c′$ seamlessly with the clothing region and handle occlusion properly in cases where arms or hair are in front of the body. Therefore, we learn how to composite with a refinement network. As shown at the bottom of Figure 2, we first concatenate $c′$ and the coarse output $I′$ as the input of our refinement network GR. The refinement network then generates a 1-channel composition mask $α∈(0, 1)^{m×n}$, indicating how much information is utilized from each of the two sources, i.e., the warped clothing item $c′$ and the coarse image $I′$. The final virtual try-on output of VITON $\hat{I}$ is a composition of $c′$ and $I′$:
>> **합성 방법을 배웁니다.** 거친 합성 이미지 $I'$에 왜곡된 의류 항목 $c'$의 구성은 $c'$를 의류 영역과 매끄럽게 결합하고 팔이나 머리카락이 몸 앞에 있는 경우 폐색을 적절하게 처리할 것으로 예상됩니다. 따라서, 우리는 정제 네트워크와 합성하는 방법을 배웁니다. 그림 2의 하단에 표시된 것처럼, 우리는 먼저 $c'$와 거친 출력 $I'$를 정제 네트워크 GR의 입력으로 연결합니다. 그런 다음 정제 네트워크는 뒤틀린 의류 항목 $c'$와 거친 이미지 $I'$와 같은 두 소스 각각에서 얼마나 많은 정보가 활용되는지 나타내는 1채널 합성 마스크 $α∈(0, 1)^{m×n}$를 생성합니다. VITONA $\hat{I}$의 최종 가상 트라이온 출력은 $c'$와 $I'$의 조합입니다.

$$\hat{I}=α⊙c′+(1−α)⊙I′,$$

> where $⊙$ represents element-wise matrix multiplication. To learn the optimal composition mask, we minimize the discrepancy between the generated result $\hat{I}$ and the reference image $I$ with a similar perceptual loss $L_{perc}$ as Eqn. 1:
>> 여기서 $⊙$는 요소별 행렬 곱셈을 나타냅니다. 최적의 합성 마스크를 배우기 위해 생성된 결과 $\hat{I}$와 기준 이미지 $I$ 사이의 불일치를 최소화하며, 지각 손실 $L_{perc}$는 Eqn.1과 유사합니다.

$$L_{perc}(\hat{I}, I)=\sum_{i=3}^{5}λ_{i}\vert{}\vert{}φ_{i}(\hat{I})−φ_{i}(I)\vert{}\vert{}1,$$

> where $φ$ denotes the visual perception network VGG19. Here we only use ‘conv3_2’, ‘conv4_2’, ‘conv5_2’ for calculating this loss. Since lower layers of a visual perception network care more about the detailed pixel-level information of an image instead of its content [10], small displacements between $I$ and $\hat{I}$ (usually caused by imperfect warping) will lead to a large mismatch between the feature maps of lower layers (‘conv1’ and ‘conv2’), which, however, is acceptable in a virtual try-on setting. Hence, by only using higher layers, we encourage the model to ignore the effects of imperfect warping, and hence it is able to select the warped target clothing image and preserve more details.
>> 여기서 $φ$는 시각 인식 네트워크 VGG19를 나타냅니다. 여기서는 이 손실을 계산할 때 'conv3_2', 'conv4_2', 'conv5_2'만 사용합니다. 시각적 인식 네트워크의 하위 계층은 그것의 내용[10] 대신 이미지의 상세한 픽셀 수준 정보에 더 신경을 쓰기 때문에, $I$와 $\hat{I}$ 사이의 작은 변위는 (보통 불완전한 뒤틀림에 의해 발생) 하위 계층의 특징 맵들 사이의 큰 불일치로 이어질 것입니다. 그러나, 이것은 ('conv1'과 'conv2') 그러나, 에서 허용됩니다. 가상 평가판 설정입니다. 따라서, 높은 레이어만 사용함으로써, 우리는 모델이 불완전한 뒤틀림의 영향을 무시하도록 권장하고, 따라서 뒤틀린 대상 의류 이미지를 선택하고 더 많은 디테일을 보존할 수 있습니다.

> We further regularize the generated composition mask output by $G_{R}$ with an L1 norm and a total-variation (TV) norm. The full objective function for the refinement network then becomes:
>> 생성된 합성 마스크 출력을 L1 노름과 총 변동(TV) 노름을 사용하여 $G_{R}$로 더욱 정규화합니다. 정제 네트워크의 전체 목적 함수는 다음과 같습니다.

$$L_{G_{R}}=L_{perc}(\hat{I}, I)−λ_{warp}\vert{}\vert{}α\vert{}\vert{}1+λ_{TV}\vert{}\vert{}∇α\vert{}\vert{}1,$$

> where $λ_{warp}$ and $λ_{TV}$ denote the weights for the L1 norm and the TV norm, respectively. Minimizing the negative L1 term encourages our model to utilize more information from the warped clothing image and render more details. The total-variation regularizer $\vert{}\vert{}∇α\vert{}\vert{}$ penalizes the gradients of the generated composition mask $α$ to make it spatially smooth, so that the transition from the warped region to the coarse result looks more natural.
>> 여기서 $λ_{warp}$ 및 $λ_{TV}$는 각각 L1 노름과 TV 노름에 대한 가중치를 나타냅니다. 음의 L1 항을 최소화하면 모델이 뒤틀린 의류 이미지에서 더 많은 정보를 활용하고 더 많은 디테일을 렌더링할 수 있습니다. 총 변동 정규화기 $\vert{}\vert{}∇α\vert{}\vert{}$는 생성된 합성 마스크 $α$의 그라데이션에 불이익을 주어 공간적으로 매끄럽게 하여 왜곡된 영역에서 거친 결과로의 전환이 보다 자연스럽게 보입니다.

> Figure 5 visualizes the results generated at different steps from our method. Given the target clothing item and the representation of a person, the encoder-decoder produces a coarse result with pose, body shape and face of the person preserved, while details like graphics and textures on the target clothing item are missing. Based on the clothing mask, our refinement stage warps the target clothing image and predicts a composition mask to determine which regions should be replaced in the coarse synthesized image. Consequentially, important details (material in the 1st example, text in 2nd example, and patterns in the 3rd example) “copied” from the target clothing image are “pasted” to the corresponding clothing region of the person.
>> 그림 5는 우리의 방법과 다른 단계에서 생성된 결과를 시각화합니다. 대상 의류 품목과 사람의 표현을 고려할 때, 인코더-디코더는 대상 의류 품목의 그래픽과 텍스처와 같은 세부 사항이 누락되는 동안 포즈, 체형 및 얼굴로 대략적인 결과를 생성합니다. 의류 마스크를 기반으로, 우리의 정제 단계는 대상 의류 이미지를 뒤틀리고 합성 마스크를 예측하여 거친 합성 이미지에서 대체해야 할 영역을 결정합니다. 결과적으로, 대상 의류 이미지에서 "복제된" 중요한 세부 정보(첫 번째 예제의 재료, 두 번째 예제의 텍스트, 세 번째 예제의 패턴)가 해당 개인의 해당 의류 영역에 "붙여넣기"됩니다.

### $\mathbf{4.\;Experiments}$

#### $\mathbf{4.1.\;Dataset}$

> The dataset used in [21] is a good choice for conducting experiments for virtual try-on, but it is not publicly available. We therefore collected a similar dataset. We first crawled around 19,000 frontal-view woman and top2 clothing image pairs and then removed noisy images with no parsing results, yielding 16,253 pairs. The remaining images are further split into a training set and a testing set with 14,221 and 2,032 pairs respectively. Note that during testing, the person should wear a different clothing item than the target one as in real-world scenarios, so we randomly shuffled the clothing product images in these 2,032 test pairs for evaluation.
>> [21]에 사용된 데이터 세트는 가상 시용 실험을 수행할 때 좋은 선택이지만 공개적으로 사용되지는 않습니다. 따라서 유사한 데이터 세트를 수집했습니다. 우리는 먼저 19,000명의 정면 여성 및 상위 2개의 의류 이미지 쌍을 탐색한 다음 구문 분석 결과 없이 노이즈가 많은 이미지를 제거하여 16,253쌍을 산출했습니다. 나머지 이미지는 각각 14,221 및 2,032쌍의 훈련 세트와 테스트 세트로 더 분할됩니다. 테스트 중에는 실제 시나리오와 같이 대상 품목과 다른 의류 품목을 착용해야 하므로 평가를 위해 2,032개 테스트 쌍의 의류 제품 이미지를 무작위로 섞었습니다.

#### $\mathbf{4.2.\;Implementation\;Details}$

> Training setup. Following recent work using encoderdecoder structures [21, 36], we use the Adam [24] optimizer with $β_{1}=0.5, β_{2}=0.999$, and a fixed learning rate of 0.0002. We train the encoder-decoder generator for 15K steps and the refinement network for 6K steps both with a batch size of 16. The resolution of the synthetic samples is 256×192.
>> 교육 설정입니다. 인코더 디코더 구조[21, 36]를 사용한 최근 연구에 이어, $β_{1}=0.5, β_{2}=0.999$, 고정 학습률 0.0002를 가진 Adam [24] 최적화 도구를 사용합니다. 배치 크기가 16인 15K 단계에 대한 인코더-디코더 생성기와 6K 단계에 대한 정제 네트워크를 훈련합니다. 합성 샘플의 분해능은 256x192입니다.

> **Encoder-decoder generator.** Our network for the coarse stage contains 6 convolutional layers for encoding and decoding, respectively. All encoding layers consist of 4×4 spatial filters with a stride of 2, and their numbers of filters are 64, 128, 256, 512, 512, 512, respectively. For decoding, similar 4×4 spatial filters are adopted with a stride of 1/2 for all layers, whose number of channels are 512, 512, 256, 128, 64, 4. The choice of activation functions and batch normalizations are the same as in [20]. Skip connections [38] are added between encoder and decoder to improve the performance. $λ_{i}$ in Eqn. 1 is chosen to scale the loss of each term properly [6].
>> **인코더-디코더 생성기입니다.** 거친 단계에 대한 우리의 네트워크는 각각 인코딩 및 디코딩을 위한 6개의 컨볼루션 레이어를 포함합니다. 모든 인코딩 레이어는 스트라이드 2의 4x4 공간 필터로 구성되며 필터 수는 각각 64, 128, 256, 512, 512, 512입니다. 디코딩을 위해 채널 수가 512, 512, 256, 128, 64, 4인 모든 레이어에 대해 1/2의 스트라이드로 유사한 4x4 공간 필터를 채택합니다. 활성화 기능 및 배치 정규화의 선택은 [20]과 동일합니다. 성능을 향상시키기 위해 인코더와 디코더 사이에 건너뛰기 연결[38]이 추가되었습니다. 각 항의 손실을 적절히 확장하기 위해 Eqn.1의 $λ_{i}$가 선택됩니다[6].

> **Refinement network.** The network is a four-layer fully convolutional model. Each of the first three layers has 3×3×64 filters followed by Leaky ReLUs and the last layer outputs the composition mask with 1×1 spatial filters followed by a sigmoid activation function to scale the output to (0, 1). $λ_{i}$ in Eqn. 4 is the same as in Eqn. 1, $λ_{warp}=0.1$ and $λ_{TV}=5e−6$.
>> **정비 네트워크입니다.** 네트워크는 4계층 완전 컨볼루션 모델입니다. 처음 세 개의 레이어에는 각각 3×3×64 필터가 있고, 그 다음에 Leaky ReLU가 있으며, 마지막 레이어는 1×1 공간 필터와 함께 합성 마스크를 출력하고 시그모이드 활성화 기능을 통해 출력을 (0, 1)로 스케일링합니다. Eqn. 4의 $λ_{i}$는 Eqn. 1과 동일합니다. $λ_{warp}=0.1$ 및 $λ_{TV}=5e-6$입니다.

> **Runtime.** The runtime of each component in VITON: Human Parsing (159ms), Pose estimation (220ms), EncoderDecoder (27ms), TPS (180ms), Refinement (20ms). Results other than TPS are obtained on a K40 GPU. We expect further speed up of TPS when implemented in GPU.
>> **런타임입니다.** VITON에서 각 구성 요소의 런타임: 인간 구문 분석(159ms), 포즈 추정(220ms), 인코더 디코더(27ms), TPS(180ms), 정제(20ms)입니다. TPS 이외의 결과는 K40 GPU에서 얻을 수 있습니다. GPU에서 구현될 경우 TPS의 속도가 더욱 빨라질 것으로 예상됩니다.

#### $\mathbf{4.3.\;Compared\;Approaches}$

> To validate the effectiveness of our framework, we compare with the following alternative methods.
>> 프레임워크의 효과를 검증하기 위해 다음과 같은 대안 방법과 비교합니다.

> **GANs with Person Representation (PRGAN)** [32, 51]. Existing methods that leverage GANs conditioned on either poses [32] or body shape information [51] are not directly comparable since they are not designed for the virtual tryon task. To achieve fair comparisons, we enrich the input of [51, 32] to be the same as our model (a 22-channel representation, p+target clothing image c) and adopt their GAN structure to synthesize the reference image.
>> **GANs with Person Representation (PRGAN)**[32, 51]. 포즈[32] 또는 체형 정보[51]에 따라 조건화된 GAN을 활용하는 기존 방법은 가상 트라이온 작업을 위해 설계되지 않았기 때문에 직접 비교할 수 없습니다. 공정한 비교를 위해 [51, 32]의 입력을 모델과 동일하도록 풍부하게 하고(22채널 표현, p+표적 의류 이미지 c) GAN 구조를 채택하여 기준 이미지를 합성합니다.

> **Conditional Analogy GAN (CAGAN)** [21]. CAGAN formulates the virtual try-on task as an image analogy problem - it treats the original item and the target clothing itemtogether as a condition when training a Cycle-GAN [50]. However, at test time, it also requires the product image of the original clothing in the reference image, which makes it infeasible in practical scenarios. But we compare with this approach for completeness. Note that for fairness, we modify their encoder-decoder generator to have the same structure as ours, so that it can also generate 256×192 images. Other implementation details are the same as in [21].
>> ** 조건부 유추 GAN (CAGAN)***[21]. CAGAN은 가상 체험 작업을 이미지 유사 문제로 공식화합니다. CAGAN은 Cycle-GAN [50]을 훈련할 때 원래 아이템과 대상 의류 아이템을 하나의 조건으로 취급합니다. 그러나 테스트 시에는 참조 이미지에서 원본 의류의 제품 이미지가 필요하므로 실제 시나리오에서는 사용할 수 없습니다. 그러나 우리는 완성도를 위해 이 접근법과 비교합니다. 공정성을 위해 256×192개의 이미지를 생성할 수 있도록 인코더-디코더 생성기를 우리의 것과 동일한 구조로 수정합니다. 기타 구현 세부 사항은 [21]과 동일합니다.

> **Cascaded Refinement Network (CRN)** [6]. CRN leverages a cascade of refinement modules, and each module takes the output from its previous module and a downsampled version of the input to generate a high-resolution synthesized image. Without adversarial training, CRN regresses to a target image using a CNN network. To compare with CRN, we feed the same input of our generator to CRN and output a 256×192 synthesized image.
>> **캐스케이드 정제 네트워크(CRN)****[6]. CRN은 일련의 정제 모듈을 활용하며, 각 모듈은 이전 모듈과 다운샘플링된 버전의 입력에서 출력을 가져와 고해상도 합성 이미지를 생성합니다. 적대적 훈련 없이 CRN은 CNN 네트워크를 사용하여 대상 이미지로 회귀합니다. CRN과 비교하기 위해 동일한 제너레이터 입력을 CRN에 공급하고 256×192 합성 이미지를 출력합니다.

> **Encoder-decoder generator.** We only use the network of our first stage to generate the target virtual try-on effect, without the TPS warping and the refinement network.
>> **인코더-디코더 생성기입니다.** 우리는 TPS 뒤틀림 및 정제 네트워크 없이 목표 가상 트라이온 효과를 생성하기 위해 첫 번째 단계의 네트워크만 사용합니다.

> **Non-parametric warped synthesis.** Without using the coarse output of our encoder-decoder generator, we estimate the TPS transformation using shape context matching and paste the warped garment on the reference image. A similar baseline is also presented in [51]. The first three state-of-the-art approaches are directly compared with our encoder-decoder generator without explicitly modeling deformations with warping, while the last Non-parametric warped synthesis method is adopted to demonstrate the importance of learning a composition based on the coarse results.
>> **비모수 왜곡된 합성입니다.** 인코더-디코더 생성기의 거친 출력을 사용하지 않고 형상 컨텍스트 일치를 사용하여 TPS 변환을 추정하고 왜곡된 의복을 참조 이미지에 붙여넣습니다. 유사한 기준선이 [51]에도 제시되어 있습니다. 처음 세 가지 최첨단 접근 방식은 워핑으로 변형을 명시적으로 모델링하지 않고 인코더-디코더 생성기와 직접 비교되는 반면, 마지막 비모수 워프 합성 방법은 거친 결과를 기반으로 구성을 학습하는 것의 중요성을 보여주기 위해 채택됩니다.

#### $\mathbf{4.4.\;Qualitative\;Results}$

> Figure 6 presents a visual comparison of different methods. CRN and encoder-decoder create blurry and coarse results without knowing where and how to render the details of target clothing items. Methods with adversarial training produce shaper edges, but also cause undesirable artifacts. Our Non-parametric baseline directly pastes the warped target image to the person regardless of the inconsistencies between the original and target clothing items, which results in unnatural images. In contrast to these methods, VITON accurately and seamlessly generates detailed virtual try-on results, confirming the effectiveness of our framework.
>> 그림 6은 다양한 방법을 시각적으로 비교한 것입니다. CRN과 인코더-디코더는 대상 의류 아이템의 세부 정보를 어디서 어떻게 렌더링해야 하는지 알지 못한 채 흐릿하고 거친 결과를 만듭니다. 적대적 훈련이 있는 방법은 더 나은 모서리를 생성하지만 바람직하지 않은 아티팩트를 유발합니다. 비모수 기준선은 원래 옷 아이템과 대상 옷 아이템 간의 불일치에 관계없이 왜곡된 대상 이미지를 직접 사람에게 붙여 부자연스러운 이미지를 만듭니다. 이러한 방법과는 대조적으로, VITON은 정확하고 원활하게 상세한 가상 체험 결과를 생성하여 프레임워크의 효과를 확인합니다.

> However, there are some artifacts around the neckline in the last row, which results from the fact that our model cannot determine which regions near the neck should be visible (e.g., the neck tag should be hided in the final result, see supplementary material for more discussions). In addition, pants, without providing any product images of them, are also generated by our model. This indicates that our model implicitly learns the co-occurrence between different fashion items. VITON is also able to keep the original pants if the pants regions are handled in the similar way as face and hair (i.e., extract pants regions and take them as the input to the encoder). More results and analysis are present in the supplementary material.
>> 그러나 마지막 행의 목선 주변에는 일부 아티팩트가 있으며, 이는 모델이 목 근처의 어느 부위를 볼 수 있어야 하는지 결정할 수 없기 때문입니다(예: 목 태그를 최종 결과에서 숨겨야 합니다. 자세한 설명은 보충 자료 참조). 또한 바지는 제품 이미지를 제공하지 않고 우리 모델에 의해 생성되기도 합니다. 이것은 우리의 모델이 다른 패션 아이템 간의 공존을 암묵적으로 학습한다는 것을 나타냅니다. VITON은 또한 바지 영역이 얼굴과 머리카락과 유사한 방식으로 처리될 경우(즉, 바지 영역을 추출하여 인코더로 입력) 원래 바지를 유지할 수 있습니다. 보충 자료에 더 많은 결과와 분석이 있습니다.

![Figure 6](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-08-01-(VITON)VITON-An-Image-based-Virtual-Try-on-Network/Figure-6.PNG)

> Figure 6: Qualitative comparisons of different methods. Our method effectively renders the target clothing on to a person.
>> 그림 6: 다양한 방법의 질적 비교입니다. 우리의 방법은 대상 의류를 효과적으로 사람에게 렌더링합니다.

![Figure 7](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-08-01-(VITON)VITON-An-Image-based-Virtual-Try-on-Network/Figure-7.PNG)

> Figure 7: Effect of removing pose and body shape from the person representation. For each method, we show its coarse result and predicted clothing mask output by the corresponding encoder-decoder generator
>> 그림 7: 인물 표현에서 포즈와 체형을 제거하는 효과입니다. 각 방법에 대해 해당 인코더-디코더 생성기에 의한 대략적인 결과와 예측된 의류 마스크 출력을 보여줍니다.

> **Person representation analysis.** To investigate the effectiveness of pose and body shape in the person representation, we remove them from the representation individually and compare with our full representation. Sampled coarse results are illustrated in Figure 7. We can see that for a person with a complicated pose, using body shape information alone is not sufficient to handle occlusion and pose ambiguity. Body shape information is also critical to adjust the target item to the right size. This confirms the proposed clothing-agnostic representation is indeed more comprehensive and effective than prior work.
>> **인물 표현 분석입니다.** 인물 표현에서 포즈와 체형의 효과를 조사하기 위해, 우리는 그것들을 표현에서 개별적으로 제거하고 우리의 전체 표현과 비교합니다. 샘플 거친 결과는 그림 7에 나와 있습니다. 자세가 복잡한 사람의 경우 체형 정보만으로는 폐색 및 자세 모호성을 처리하기에 충분하지 않음을 알 수 있습니다. 대상 항목을 올바른 크기로 조정하려면 신체 형태 정보도 중요합니다. 이는 제안된 의복에 구애받지 않는 표현이 이전 작업보다 더 포괄적이고 효과적이라는 것을 확인시켜 줍니다.

![Figure 8](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-08-01-(VITON)VITON-An-Image-based-Virtual-Try-on-Network/Figure-8.PNG)

> Figure 8: Failure cases of our method.
>> 그림 8:우리의 메서드의 고장 사례.

> **Failure cases.** Figure 8 demonstrates two failure cases of our method due to rarely-seen poses (example on the left) or a huge mismatch in the current and target clothing shapes (right arm in the right example). 
>> **고장 사례입니다.** 그림 8은 거의 보이지 않는 자세(왼쪽의 예) 또는 현재 및 목표 옷 모양(오른쪽 예시의 오른팔)의 큰 불일치로 인한 우리 방법의 두 가지 실패 사례를 보여줍니다.

> **In the wild results.** In addition to experimenting with constrained images, we also utilize in the wild images from the COCO dataset [29], by cropping human body regions and running our method on them. Sample results are shown in Figure 9, which suggests our method has potentials in applications like generating people in clothing [26].
>> **무슨 결과가 나왔나요?** 제한된 이미지로 실험하는 것 외에도, 우리는 인간의 신체 영역을 잘라내고 그것들에 대한 우리의 방법을 실행함으로써 COCO 데이터 세트[29]의 야생 이미지에도 활용합니다. 샘플 결과는 그림 9에 나와 있으며, 이는 우리의 방법이 옷을 입은 사람을 생성하는 것과 같은 응용 분야에서 잠재력을 가지고 있음을 시사합니다 [26].

![Figure 9](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-08-01-(VITON)VITON-An-Image-based-Virtual-Try-on-Network/Figure-9.PNG)

> Figure 9: In the wild results. Our method is applied to images on COCO.
>> 그림 9: 엉뚱한 결과입니다. 우리의 방법은 COCO의 이미지에 적용됩니다.

![Table 1](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-08-01-(VITON)VITON-An-Image-based-Virtual-Try-on-Network/Table-1.PNG)

> Table 1: Quantitative evaluation on our virtual try-on dataset.
>> 표 1: 가상 평가판 데이터 세트에 대한 정량적 평가입니다.

### $\mathbf{4.5.\;Quantitative\;Results}$

> We also compare VITON with alternative methods quantitatively based on Inception Score [39] and a user study.
>> 또한 Inception Score [39]와 사용자 연구를 바탕으로 정량적으로 VITON을 대안 방법과 비교합니다.

> **Inception Score.** Inception Score (IS) [39] is usually used to quantitatively evaluate the synthesis quality of image generation models [32, 33, 47]. Models producing visually diverse and semantically meaningful images will have higher Inception Scores, and this metric correlates well with human evaluations on image datasets like CIFAR10.
>> **초기 점수입니다.** Inception Score (IS) [39]는 일반적으로 이미지 생성 모델의 합성 품질을 정량적으로 평가하는 데 사용됩니다 [32, 33, 47]. 시각적으로 다양하고 의미론적으로 의미 있는 이미지를 생성하는 모델은 인셉션 점수가 높아지며, 이 메트릭은 CIFAR10과 같은 이미지 데이터 세트에 대한 인간 평가와 잘 상관됩니다.

> **Perceptual user study.** Although Inception Score can be used as an indicator of the image synthesis quality, it cannot reflect whether the details of the target clothing are naturally transferred or the pose and body of the clothed person are preserved in the synthesized image. Thus, simialr to [6, 9], we conducted a user study on the Amazon Mechanical Turk (AMT) platform. On each trial, a worker is given a person image, a target clothing image and two virtual try-on results generated by two different methods (both in 256×192). The worker is then asked to choose the one that is more realistic and accurate in a virtual try-on situation. Each AMT job contains 5 such trials with a time limit of 200 seconds. The percentage of trials in which one method is rated better than other methods is adopted as the Human evaluation metric following [6] (chance is 50%).
>> **지각적 사용자 연구입니다.** Inception Score는 이미지 합성 품질의 지표로 사용할 수 있지만, 대상 의류의 디테일이 자연스럽게 전달되는지 아니면 합성 이미지에 옷을 입은 사람의 자세와 몸이 보존되는지 반영할 수 없습니다. 따라서 [6,9]와 유사하게 Amazon Mechanical Turk(AMT) 플랫폼에 대한 사용자 연구를 수행했습니다. 각 시험마다 작업자에게 사람 이미지, 대상 옷 이미지 및 두 가지 다른 방법(둘 다 256×192)에 의해 생성된 두 가지 가상 체험 결과가 주어집니다. 그런 다음 작업자는 가상 시용 상황에서 더 현실적이고 정확한 것을 선택하도록 요구됩니다. 각 AMT 작업에는 200초의 제한 시간으로 5개의 이러한 시도가 포함됩니다. 한 방법이 다른 방법보다 더 나은 평가를 받은 시행의 백분율은 [6](확률은 50%)에 이어 인간 평가 지표로 채택됩니다.

> Quantitative comparisons are summarized in Table 1. Note that the human score evaluates whether the virtual tryon results, synthetic images with a person wearing the target item, are realistic. However, we don’t have such groundtruth images - the same person in the same pose wearing the target item (IS measures the characteristics of a set, so we use all reference images in the test set to estimate the IS of real data).
>> 정량적 비교는 표 1에 요약되어 있습니다. 인간 점수는 가상 체험 결과, 대상 항목을 착용한 사람이 있는 합성 이미지가 현실적인지 여부를 평가합니다. 그러나, 우리는 그러한 실측 이미지를 가지고 있지 않습니다 - 대상 항목을 착용하는 동일한 자세를 가진 사람 (IS는 세트의 특성을 측정하므로 실제 데이터의 IS를 추정하기 위해 테스트 세트의 모든 기준 이미지를 사용합니다.)

> According to this table, we make the following observations: (a) Automatic measures like Inception Score are not suitable for evaluating tasks like virtual try-on. The reasons are two-fold. First, these measures tend to reward sharper image content generated by adversarial training or direct image pasting, since they have higher activation values of neurons in Inception model than those of smooth images. This even leads to a higher IS of the Non-parametric baseline over real images. Moreover, they are not aware of the task and cannot measure the desired properties of a virtual try-on system. For example, CRN has the lowest IS, but ranked the 2nd place in the user study. Similar phenomena are also observed in [27, 6]; (b) Person representation guided methods (PRGAN, CRN, EncoderDecoder, VITON) are preferred by humans. CAGAN and Non-parametric directly take the original person image as inputs, so they cannot deal with cases when there are inconsistencies between the original and target clothing item, e.g., rendering a short-sleeve T-shirt on a person wearing a long-sleeve shirt; (c) By compositing the coarse result with a warped clothing image, VITON performs better than each individual component. VITON also obtains a higher human evaluation score than state-of-the-art generative models and outputs more photo-realistic virtual try-on effects. 
>> 이 표에 따르면, 우리는 다음과 같은 관찰을 합니다. (a) Inception Score와 같은 자동 측정은 가상 체험과 같은 작업을 평가하는 데 적합하지 않습니다. 그 이유는 두 가지입니다. 첫째, 이러한 측정은 인셉션 모델에서 뉴런의 활성화 값이 매끄러운 이미지의 활성화 값보다 높기 때문에 적대적 훈련 또는 직접적인 이미지 붙여넣기에 의해 생성된 더 날카로운 이미지 콘텐츠를 보상하는 경향이 있습니다. 따라서 실제 영상보다 비모수 기준의 IS가 더 높아집니다. 더욱이, 그들은 작업을 인식하지 못하며 가상 평가판 시스템의 원하는 속성을 측정할 수 없습니다. 예를 들어, CRN은 IS가 가장 낮지만 사용자 연구에서 2위를 차지했습니다. 유사한 현상이 [27, 6]에서도 관찰됩니다. (b) 사람이 선호하는 인물 표현 유도 방법(PRGAN, CRN, 인코더 디코더, VITON)입니다. CAGAN과 Non-parametric은 원본 인물 이미지를 입력으로 직접 받아들이기 때문에, 긴 소매 셔츠를 입은 사람에게 반팔 티셔츠를 렌더링하는 것과 같이 원본과 대상 옷 아이템 사이에 불일치가 있는 경우를 처리할 수 없습니다. (c) 거친 결과를 뒤틀린 옷 이미지와 합성하여 VITON을 수행합니다.각 개별 성분보다 더 높습니다. VITON은 또한 최첨단 생성 모델보다 높은 인간 평가 점수를 획득하고 더 많은 사진 사실적인 가상 체험 효과를 출력합니다.

> To better understand the noise of the study, we follow [6, 32] to perform time-limited (0.25s) real or fake test on AMT, which shows 17.18% generated images are rated as real, and 11.46% real images are rated as generated.
>> 연구의 노이즈를 더 잘 이해하기 위해 [6, 32]에 따라 AMT에 대해 시간 제한(0.25s) 실제 또는 가짜 테스트를 수행했는데, 이는 생성된 이미지가 17.18%로 평가되고 실제 이미지가 생성된 것으로 평가되는 것을 보여줍니다.

### $\mathbf{5.\;Conclusion}$

> We presented a virtual try-on network (VITON), which is able to transfer a clothing item in a product image to a person relying only on RGB images. A coarse sample is first generated with a multi-task encoder-decoder conditioned on a detailed clothing-agnostic person representation. The coarse results are further enhanced with a refinement network that learns the optimal composition. We conducted experiments on a newly collected dataset, and promising results are achieved both quantitatively and qualitatively. This indicates that our 2D image-based synthesis pipeline can be used as an alternative to expensive 3D based methods.
>> RGB 이미지에만 의존하는 사람에게 제품 이미지 속 의류 아이템을 전송할 수 있는 VITON(Virtual Try-on Network)을 제시했습니다. 거친 샘플은 먼저 복장에 구애받지 않는 사람 표현에 따라 조정된 다중 작업 인코더-디코더로 생성됩니다. 거친 결과는 최적의 구성을 학습하는 정제 네트워크를 통해 더욱 개선됩니다. 새롭게 수집된 데이터 세트에 대한 실험을 수행했으며, 양적, 질적으로 유망한 결과를 달성했습니다. 이는 2D 이미지 기반 합성 파이프라인이 값비싼 3D 기반 방법의 대안으로 사용될 수 있음을 나타냅니다.

---

### $\mathbf{References}$

[1] U.S. fashion and accessories e-retail revenue 2016-2022. https://www.statista.com/statistics/278890/us-apparel-andaccessories-retail-e-commerce-revenue/. 1

[2] Z. Al-Halah, R. Stiefelhagen, and K. Grauman. Fashion forward: Forecasting visual style in fashion. In ICCV, 2017. 2

[3] S. Belongie, J. Malik, and J. Puzicha. Shape matching and object recognition using shape contexts. IEEE TPAM$I$, 2002. 4

[4] F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero, and M. J. Black. Keep it smpl: Automatic estimation of 3d human pose and shape from a single image. In ECCV, 2016. 1

[5] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh. Realtime multiperson 2d pose estimation using part affinity fields. In CVPR, 2017. 3

[6] Q. Chen and V. Koltun. Photographic image synthesis with cascaded refinement networks. In ICCV, 2017. 2, 6, 8

[7] A. Dosovitskiy and T. Brox. Generating images with perceptual similarity metrics based on deep networks. In NIPS, 2016. 4

[8] A. Dosovitskiy, J. Tobias Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. In CVPR, 2015. 2

[9] C. Gan, Z. Gan, X. He, J. Gao, and L. Deng. Stylenet: Generating attractive visual captions with styles. In CVPR, 2017. 8

[10] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In CVPR, 2016. 5

[11] K. Gong, X. Liang, X. Shen, and L. Lin. Look into person: Self-supervised structure-sensitive learning and a new benchmark for human parsing. In CVPR, 2017. 3

[12] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014. 2

[13] P. Guan, L. Reiss, D. A. Hirshberg, A. Weiss, and M. J. Black. Drape: Dressing any person. ACM TOG, 2012. 2

[14] M. Hadi Kiapour, X. Han, S. Lazebnik, A. C. Berg, and T. L. Berg. Where to buy it: Matching street clothing photos in online shops. In ICCV, 2015. 2

[15] X. Han, Z. Wu, P. X. Huang, X. Zhang, M. Zhu, Y. Li, Y. Zhao, and L. S. Davis. Automatic spatially-aware fashion concept discovery. In ICCV, 2017. 2

[16] X. Han, Z. Wu, Y.-G. Jiang, and L. S. Davis. Learning fashion compatibility with bidirectional lstms. In ACM Multimedia, 2017. 2

[17] T. Hassner, S. Harel, E. Paz, and R. Enbar. Effective face frontalization in unconstrained images. In CVPR, 2015. 5

[18] A. Hilsmann and P. Eisert. Tracking and retexturing cloth for real-time virtual clothing applications. In MIRAGE, 2009. 2

[19] Y. Hu, X. Yi, and L. S. Davis. Collaborative fashion recommendation: a functional tensor factorization approach. In ACM Multimedia, 2015. 2

[20] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017. 2, 4, 6

[21] N. Jetchev and U. Bergmann. The conditional analogy gan: Swapping fashion articles on people images. In ICCVW, 2017. 2, 5, 6, 8

[22] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. 4

[23] A. Kanazawa, D. W. Jacobs, and M. Chandraker. Warpnet: Weakly supervised matching for single-view reconstruction. In CVPR, 2016. 5

[24] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6

[25] A. Kovashka, D. Parikh, and K. Grauman. Whittlesearch: Image search with relative attribute feedback. In CVPR, 2012. 2

[26] C. Lassner, G. Pons-Moll, and P. V. Gehler. A generative model of people in clothing. In ICCV, 2017. 2, 7

[27] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunning- ´ham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi. Photo-realistic single image super-resolution using a generative adversarial network. In CVPR, 2017. 4, 8

[28] X. Liang, L. Lin, W. Yang, P. Luo, J. Huang, and S. Yan. Clothes co-parsing via joint image segmentation and labeling with application to clothing retrieval. IEEE TMM, 2016. 2

[29] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft coco: Com- ´mon objects in context. In ECCV, 2014. 7

[30] S. Liu, Z. Song, G. Liu, C. Xu, H. Lu, and S. Yan. Street-toshop: Cross-scenario clothing retrieval via parts alignment and auxiliary set. In CVPR, 2012. 2

[31] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In CVPR, 2016. 2

[32] L. Ma, X. Jia, Q. Sun, B. Schiele, T. Tuytelaars, and L. Van Gool. Pose guided person image generation. In NIPS, 2017. 2, 4, 6, 8

[33] A. Odena, C. Olah, and J. Shlens. Conditional image synthesis with auxiliary classifier gans. In ICML, 2017. 2, 8 

[34] G. Perarnau, J. van de Weijer, B. Raducanu, and J. M. Alvarez. Invertible conditional gans for image editing. In ´NIPS Workshop, 2016. 2

[35] G. Pons-Moll, S. Pujades, S. Hu, and M. Black. Clothcap: Seamless 4d clothing capture and retargeting. ACM TOG, 2017. 2

[36] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015. 2, 6

[37] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee. Generative adversarial text to image synthesis. In ICML, 2016. 2

[38] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCA$I$, 2015. 4, 6

[39] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training gans. In NIPS, 2016. 8


[40] M. Sekine, K. Sugita, F. Perbet, B. Stenger, and M. Nishiyama. Virtual fitting by single-shot body shape estimation. In 3D Body Scanning Technologies, 2014. 1, 2

[41] W. Shen and R. Liu. Learning residual images for face attribute manipulation. CVPR, 2017. 2

[42] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. 4

[43] A. Veit, B. Kovacs, S. Bell, J. McAuley, K. Bala, and S. Belongie. Learning visual clothing style with heterogeneous dyadic co-occurrences. In CVPR, 2015. 2

[44] K. Yamaguchi, M. Hadi Kiapour, and T. L. Berg. Paper doll parsing: Retrieving similar styles to parse clothing items. In ICCV, 2013. 2

[45] S. Yang, T. Ambert, Z. Pan, K. Wang, L. Yu, T. Berg, and M. C. Lin. Detailed garment recovery from a single-view image. In ICCV, 2017. 1, 2

[46] D. Yoo, N. Kim, S. Park, A. S. Paek, and I. S. Kweon. Pixellevel domain transfer. In ECCV, 2016. 2

[47] H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and D. Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In ICCV, 2017. 8

[48] B. Zhao, J. Feng, X. Wu, and S. Yan. Memory-augmented attribute manipulation networks for interactive fashion search. In CVPR, 2017. 2

[49] J.-Y. Zhu, P. Krahenb ¨ uhl, E. Shechtman, and A. A. Efros. ¨Generative visual manipulation on the natural image manifold. In ECCV, 2016. 2

[50] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired imageto-image translation using cycle-consistent adversarial networks. In ICCV, 2017. 2, 6

[51] S. Zhu, S. Fidler, R. Urtasun, D. Lin, and C. L. Chen. Be your own prada: Fashion synthesis with structural coherence. In ICCV, 2017. 2, 3, 4, 6, 8

[52] X. Zhu, Z. Lei, J. Yan, D. Yi, and S. Z. Li. High-fidelity pose and expression normalization for face recognition in the wild. In CVPR, 2015. 5