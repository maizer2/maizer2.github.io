---
layout: post 
title: "(GAN)PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION Translation"
categories: [1. Computer Engineering]
tags: [1.7. Literature Review, 1.2.2.5. GAN]
---

### [GAN Literature List](https://maizer2.github.io/1.%20computer%20engineering/2022/05/23/Literature-of-GAN.html)

## <center>$$\mathbf{PROGRESSIVE\;GROWING\;OF\;GANS\;FOR\;IMPROVED}$$</center>
## <center>$$\mathbf{QUALITY,\;STABILITY,\;AND\;VARIATION}\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;$$</center>

### $$\mathbf{Abstract}$$

> We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 10242 . We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image
quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.
>> 우리는 생성적 적대 네트워크를 위한 새로운 훈련 방법론을 설명한다. 핵심 아이디어는 생성기와 판별기를 점진적으로 성장시키는 것이다. 낮은 해상도에서 시작하여 교육이 진행됨에 따라 점점 더 미세한 세부 사항을 모델링하는 새로운 계층을 추가한다. 이는 훈련 속도를 높이고 이를 크게 안정시켜 10242에서 CELEBA 이미지와 같은 전례 없는 품질의 이미지를 생성할 수 있게 한다. 또한 생성된 이미지의 변동을 늘리고 감독되지 않은 CIFAR10에서 8.80의 기록 개시 점수를 달성하는 간단한 방법을 제안한다. 또한, 우리는 발전기와 판별기 사이의 불건전한 경쟁을 막는 데 중요한 몇 가지 구현 세부 사항을 설명한다. 마지막으로, 이미지 측면에서 GAN 결과를 평가하기 위한 새로운 메트릭을 제안한다.
품질과 변화 추가적인 기여로, 우리는 CELEBA 데이터 세트의 고품질 버전을 구성한다.

### $\mathbf{1\;Introduction}$

> Generative methods that produce novel samples from high-dimensional data distributions, such as images, are finding widespread use, for example in speech synthesis (van den Oord et al., 2016a), image-to-image translation (Zhu et al., 2017; Liu et al., 2017; Wang et al., 2017), and image inpainting (Iizuka et al., 2017). Currently the most prominent approaches are autoregressive models (van den Oord et al., 2016b;c), variational autoencoders (VAE) (Kingma & Welling, 2014), and generative adversarial networks (GAN) (Goodfellow et al., 2014). Currently they all have significant strengths and weaknesses. Autoregressive models – such as PixelCNN – produce sharp images but are slow to evaluate and do not have a latent representation as they directly model the conditional distribution over pixels, potentially limiting their applicability. VAEs are easy to train but tend to produce blurry results due to restrictions in the model, although recent work is improving this (Kingma et al., 2016). GANs produce sharp images, albeit only in fairly small resolutions and with somewhat limited variation, and the training continues to be unstable despite recent progress (Salimans et al., 2016; Gulrajani et al., 2017; Berthelot et al., 2017; Kodali et al., 2017). Hybrid methods combine various strengths of the three, but so far lag behind GANs in image quality (Makhzani & Frey, 2017; Ulyanov et al., 2017; Dumoulin et al., 2016).
>> 이미지와 같은 고차원 데이터 분포에서 새로운 샘플을 생성하는 생성 방법은 음성 합성(van den Oord et al., 2016a), 이미지 대 이미지 번역(Zhu et al., 2017; Liu et al., 2017; Wang et al., 2017), 이미지 인페인팅(Iizuka et al., 2017)에서 널리 사용되고 있다. 현재 가장 눈에 띄는 접근 방식은 자기 회귀 모델(vanden Oord 등, 2016b;c), 변형 자동 인코더(VAE)(Kingma & Welling, 2014), 생성 적대적 네트워크(GAN)이다(Goodfellow 등, 2014). 현재 그들은 모두 상당한 장점과 단점을 가지고 있다. PixelCNN과 같은 자기 회귀 모델은 날카로운 이미지를 생성하지만 픽셀에 대한 조건부 분포를 직접 모델링하기 때문에 평가가 느리고 잠재 표현이 없어 적용 가능성을 제한할 수 있다. VAE는 훈련하기 쉽지만 모델의 제약으로 인해 흐릿한 결과를 낳는 경향이 있다(Kingma et al., 2016). GAN은 상당히 작은 해상도와 다소 제한된 변화지만 날카로운 이미지를 생성하며, 훈련은 최근의 진전에도 불구하고 계속 불안정하다(Salimans et al., 2016; Gulrajani et al., 2017; Bertelot et al., 2017; Kodali et al., 2017). 하이브리드 방법은 세 가지 장점을 결합하지만, 지금까지 이미지 품질에서 GAN보다 뒤떨어진다(Makhzani & Frey, 2017; Ulyanov et al., 2017; Dumoulin et al., 2016).

> Typically, a GAN consists of two networks: generator and discriminator (aka critic). The generator produces a sample, e.g., an image, from a latent code, and the distribution of these images should ideally be indistinguishable from the training distribution. Since it is generally infeasible to engineer a function that tells whether that is the case, a discriminator network is trained to do the assessment, and since networks are differentiable, we also get a gradient we can use to steer both networks to the right direction. Typically, the generator is of main interest – the discriminator is an adaptive loss function that gets discarded once the generator has been trained.
>> 일반적으로 GAN은 두 개의 네트워크, 즉 생성기와 판별기(크리틱)로 구성된다. 생성기는 잠재 코드로부터 샘플(예: 이미지)을 생성하며, 이러한 이미지의 분포는 훈련 분포와 이상적으로 구분할 수 없어야 한다. 그것이 사실인지 여부를 알려주는 기능을 설계하는 것은 일반적으로 불가능하기 때문에, 판별기 네트워크는 평가를 수행하도록 훈련되고, 네트워크는 차별화 가능하기 때문에, 우리는 또한 두 네트워크를 올바른 방향으로 조정하는 데 사용할 수 있는 기울기를 얻는다. 일반적으로 발전기는 주요 관심사이다. 판별기는 발전기가 훈련되면 폐기되는 적응형 손실 함수이다.

> There are multiple potential problems with this formulation. When we measure the distance between the training distribution and the generated distribution, the gradients can point to more or less random directions if the distributions do not have substantial overlap, i.e., are too easy to tell apart (Arjovsky & Bottou, 2017). Originally, Jensen-Shannon divergence was used as a distance metric (Goodfellow et al., 2014), and recently that formulation has been improved (Hjelm et al., 2017) and a number of more stable alternatives have been proposed, including least squares (Mao et al., 2016b), absolute deviation with margin (Zhao et al., 2017), and Wasserstein distance (Arjovsky et al., 2017; Gulrajaniet al., 2017). Our contributions are largely orthogonal to this ongoing discussion, and we primarily use the improved Wasserstein loss, but also experiment with least-squares loss.
>> 이 공식에는 여러 잠재적인 문제가 있다. 훈련 분포와 생성된 분포 사이의 거리를 측정할 때, 분포가 실질적으로 겹치지 않는 경우, 즉, 구분이 너무 쉬운 경우 그레이디언트는 다소 무작위적인 방향을 가리킬 수 있다(Arjovsky & Bottou, 2017). 원래 Jensen-Shannon 분기는 거리 메트릭으로 사용되었으며(Goodfellow et al., 2014), 최근에는 공식화가 개선되었으며(Hjelm et al., 2017), 최소 제곱(Mao et al., 2016b), 마진을 갖는 절대 편차(Zhao et al., 2017), Wasserstein 거리(A)를 포함한 여러 가지 더 안정적인 대안이 제안되었다.Rjovsky et al., 2017; Gulrajaniet et al., 2017). 우리의 기여는 이 진행 중인 논의와 크게 직교하며, 우리는 주로 개선된 와서스테인 손실을 사용하지만 최소 제곱 손실로 실험하기도 한다.

> The generation of high-resolution images is difficult because higher resolution makes it easier to tell the generated images apart from training images (Odena et al., 2017), thus drastically amplifying the gradient problem. Large resolutions also necessitate using smaller minibatches due to memory constraints, further compromising training stability. Our key insight is that we can grow both the generator and discriminator progressively, starting from easier low-resolution images, and add new layers that introduce higher-resolution details as the training progresses. This greatly speeds up training and improves stability in high resolutions, as we will discuss in Section 2.
>> 고해상도 이미지 생성은 해상도가 높으면 생성된 이미지를 훈련 이미지와 구별하는 것이 더 쉬워져 그레이디언트 문제를 크게 증폭시키기 때문에 어렵다. 또한 해상도가 크면 메모리 제약으로 인해 더 작은 미니 배치를 사용해야 하므로 훈련 안정성이 더욱 저하된다. 우리의 핵심 통찰력은 보다 쉬운 저해상도 이미지에서 시작하여 생성기와 판별기를 모두 점진적으로 확장하고 교육이 진행됨에 따라 고해상도 세부 정보를 도입하는 새로운 계층을 추가할 수 있다는 것이다. 이렇게 하면 교육 속도가 크게 빨라지고 고해상도에서의 안정성이 향상됩니다(섹션 2에서 설명하겠습니다.

> The GAN formulation does not explicitly require the entire training data distribution to be represented by the resulting generative model. The conventional wisdom has been that there is a tradeoff between image quality and variation, but that view has been recently challenged (Odena et al., 2017). The degree of preserved variation is currently receiving attention and various methods have been suggested for measuring it, including inception score (Salimans et al., 2016), multi-scale structural similarity (MS-SSIM) (Odena et al., 2017; Wang et al., 2003), birthday paradox (Arora & Zhang, 2017), and explicit tests for the number of discrete modes discovered (Metz et al., 2016). We will describe our method for encouraging variation in Section 3, and propose a new metric for evaluating
the quality and variation in Section 5.
>> GAN 공식은 전체 훈련 데이터 분포를 결과 생성 모델로 나타낼 것을 명시적으로 요구하지 않는다. 이미지 품질과 변화 사이에는 절충이 있다는 것이 통념이었지만, 최근 그러한 관점이 도전받고 있다(Odena et al., 2017). 보존된 변동의 정도는 현재 주목을 받고 있으며 초기 점수(Salimans et al., 2016), 다중 스케일 구조적 유사성(MS-SSIM)(Odena et al., 2017; Wang et al., 2003), 생일 역설(Arora & Zhang, 2017), 이산 모 수에 대한 명시적 테스트 등 다양한 방법이 제안되었다.descovered (Metz et al., 2016). 우리는 섹션 3에서 변화를 장려하기 위한 우리의 방법을 설명하고, 평가를 위한 새로운 측정 기준을 제안할 것이다.

> Section 4.1 discusses a subtle modification to the initialization of networks, leading to a more balanced learning speed for different layers. Furthermore, we observe that mode collapses traditionally plaguing GANs tend to happen very quickly, over the course of a dozen minibatches. Commonly they start when the discriminator overshoots, leading to exaggerated gradients, and an unhealthy competition follows where the signal magnitudes escalate in both networks. We propose a mechanism to stop the generator from participating in such escalation, overcoming the issue (Section 4.2).
>> 4.1절에서는 네트워크 초기화에 대한 미묘한 수정에 대해 논의하여 서로 다른 계층에 대해 보다 균형 잡힌 학습 속도를 제공한다. 또한, 우리는 전통적으로 문제를 일으키는 GAN의 모드 붕괴가 12개의 미니 배치에 걸쳐 매우 빠르게 발생하는 경향이 있음을 관찰한다. 일반적으로 그것들은 판별기가 오버슈트를 할 때 시작되어 과장된 그레이디언트로 이어지며, 두 네트워크에서 신호 크기가 상승하는 불건전한 경쟁이 뒤따른다. 우리는 발전기가 이러한 에스컬레이션에 참여하여 문제를 극복하는 것을 중단하는 메커니즘을 제안한다(4.2절).

> We evaluate our contributions using the CELEBA, LSUN, CIFAR10 datasets. We improve the best published inception score for CIFAR10. Since the datasets commonly used in benchmarking generative methods are limited to a fairly low resolution, we have also created a higher quality version of the CELEBA dataset that allows experimentation with output resolutions up to 1024 × 1024 pixels. This dataset and our full implementation are available at [https://github.com/tkarras/progressive_growing_of_gans](https://github.com/tkarras/progressive_growing_of_gans), trained networks can be found at [https://drive.google.com/open?id=0B4qLcYyJmiz0NHFULTdYc05lX0U](https://drive.google.com/open?id=0B4qLcYyJmiz0NHFULTdYc05lX0U) along with result images, and a supplementary video illustrating the datasets, additional results, and latent space interpolations is at [https://youtu.be/G06dEcZ-QTg](https://youtu.be/G06dEcZ-QTg).
>> 우리는 CELEBA, LSUN, CIFAR10 데이터 세트를 사용하여 기여도를 평가한다. 우리는 CIFAR10에 대해 가장 잘 발표된 초기 점수를 향상시킨다. 벤치마킹 생성 방법에 일반적으로 사용되는 데이터 세트는 상당히 낮은 해상도로 제한되므로, 최대 1024 × 1024 픽셀의 출력 해상도로 실험을 할 수 있는 더 높은 품질의 CELEBA 데이터 세트 버전도 만들었다. 이 데이터 세트와 전체 구현은 [https://github.com/tkarras/progressive_growing_of_gans](https://github.com/tkarras/progressive_growing_of_gans),에서 사용할 수 있으며, 훈련된 네트워크는 [https://drive.google.com/open?id=0B4qLcYyJmiz0NHFULTdYc05lX0U](https://drive.google.com/open?id=0B4qLcYyJmiz0NHFULTdYc05lX0U)에서 결과와 함께 찾을 수 있다. 데이터 세트, 추가 결과 및 잠재 공간 보간을 보여주는 보충 비디오는 [https://youtu.be/G06dEcZ-QTg](https://youtu.be/G06dEcZ-QTg)에 있다.

### $\mathbf{2\;PROGRESSIVE\;GROWING\;OF\;GANS}$

> Our primary contribution is a training methodology for GANs where we start with low-resolution images, and then progressively increase the resolution by adding layers to the networks as visualized in Figure 1. This incremental nature allows the training to first discover large-scale structure of the image distribution and then shift  attention to increasingly finer scale detail, instead of having to learn all scales simultaneously.
>> 우리의 주요 공헌은 저해상도 이미지로 시작한 다음 그림 1에서 시각화된 것처럼 네트워크에 레이어를 추가하여 점진적으로 해상도를 높이는 GAN을 위한 훈련 방법론이다. 이러한 증분 특성은 훈련이 먼저 이미지 분포의 대규모 구조를 발견한 다음 모든 척도를 동시에 학습할 필요 없이 점점 더 미세한 스케일 세부 사항으로 주의를 전환할 수 있도록 한다.

> We use generator and discriminator networks that are mirror images of each other and always grow in synchrony. All existing layers in both networks remain trainable throughout the training process. When new layers are added to the networks, we fade them in smoothly, as illustrated in Figure 2. This avoids sudden shocks to the already well-trained, smaller-resolution layers. Appendix A describes structure of the generator and discriminator in detail, along with other training parameters.
>> 우리는 서로의 거울 이미지이고 항상 동기적으로 성장하는 발전기 및 판별기 네트워크를 사용한다. 두 네트워크의 모든 기존 계층은 교육 과정 내내 훈련할 수 있다. 새로운 레이어가 네트워크에 추가되면 그림 2와 같이 부드럽게 페이드 인합니다. 이는 이미 잘 훈련된 작은 해상도 계층에 대한 갑작스러운 충격을 방지한다. 부록 A는 발전기 및 판별기의 구조와 기타 훈련 매개변수를 상세히 설명한다.

> We observe that the progressive training has several benefits. Early on, the generation of smaller images is substantially more stable because there is less class information and fewer modes (Odena et al., 2017). By increasing the resolution little by little we are continuously asking a much simpler question compared to the end goal of discovering a mapping from latent vectors to e.g. 10242 images. This approach has conceptual similarity to recent work by Chen & Koltun (2017). In practice it stabilizes the training sufficiently for us to reliably synthesize megapixel-scale images using WGAN-GP loss (Gulrajani et al., 2017) and even LSGAN loss (Mao et al., 2016b).
>> 우리는 점진적인 훈련이 몇 가지 이점을 가지고 있다는 것을 관찰한다. 초기에, 더 작은 이미지의 생성은 클래스 정보가 적고 모드가 더 적기 때문에 훨씬 더 안정적이다(Odena et al., 2017). 해상도를 조금씩 높임으로써 잠재 벡터로부터 10242 이미지로의 매핑을 발견하는 최종 목표에 비해 훨씬 간단한 질문을 지속적으로 던지고 있다. 이 접근 방식은 Chen & Koltun의 최근 연구(2017)와 개념적으로 유사하다. 실제로 WGAN-GP 손실(Gulrajani et al., 2017)과 LSGAN 손실(Mao et al., 2016b)을 사용하여 메가픽셀 규모의 이미지를 안정적으로 합성할 수 있을 정도로 훈련을 안정화시킨다.

![Figure 1](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-1.JPG)

> Figure 1: Our training starts with both the generator (G) and discriminator (D) having a low spatial resolution of 4×4 pixels. As the training advances, we incrementally add layers to G and D, thus increasing the spatial resolution of the generated images. All existing layers remain trainable throughout the process. Here $N\times{N}$ refers to convolutional layers operating on $N\times{N}$ spatial resolution. This allows stable synthesis in high resolutions and also speeds up training considerably. One the right we show six example images generated using progressive growing at 1024 × 1024.
>> 그림 1: 우리의 훈련은 4×4 픽셀의 낮은 공간 해상도를 가진 생성기(G)와 판별기(D)에서 시작한다. 훈련이 진행됨에 따라 G와 D에 레이어를 점진적으로 추가하여 생성된 이미지의 공간 해상도를 높인다. 모든 기존 계층은 프로세스 내내 교육할 수 있습니다. 여기서 $N\times{N}$는 $N\times{N}$ 공간 해상도에서 작동하는 컨볼루션 레이어를 의미한다. 이를 통해 고해상도에서도 안정적인 합성이 가능하고 훈련 속도도 상당히 빨라진다. 오른쪽 중 하나는 1024 × 1024에서 점진적 성장을 사용하여 생성된 6개의 예제 이미지를 보여준다.

> Another benefit is the reduced training time. With progressively growing GANs most of the iterations are done at lower resolutions, and comparable result quality is often obtained up to 2–6 times faster, depending on the final output resolution.
>> 또 다른 이점은 교육 시간이 줄어든다는 것입니다. 점진적으로 성장하는 GAN에서 대부분의 반복은 낮은 분해능에서 수행되며, 비교 결과 품질은 종종 최종 출력 분해능에 따라 최대 2-6배 더 빠르게 획득된다.

> The idea of growing GANs progressively is related to the work of Wang et al. (2017), who use multiple discriminators that operate on different spatial resolutions. That work in turn is motivated by Durugkar et al. (2016) who use one generator and multiple discriminators concurrently, and Ghosh et al. (2017) who do the opposite with multiple generators and one discriminator. Hierarchical GANs (Denton et al., 2015; Huang et al., 2016; Zhang et al., 2017) define a generator and discriminator for each level of an image pyramid. These methods build on the same observation as our work – that the complex mapping from latents to high-resolution images is easier to learn in steps – but the crucial difference is that we have only a single GAN instead of a hierarchy of them. In contrast to early work on adaptively growing networks, e.g., growing neural gas (Fritzke, 1995) and neuro evolution of augmenting topologies (Stanley & Miikkulainen, 2002) that grow networks greedily, we simply defer the introduction of pre-configured layers. In that sense our approach resembles layer-wise training of autoencoders (Bengio et al., 2007).
>> GANs를 점진적으로 성장시키는 아이디어는 서로 다른 공간 해상도로 작동하는 다중 판별기를 사용하는 Wang 외 연구진(2017)의 작업과 관련이 있다. 차례로 이 작업은 하나의 발전기와 여러 판별기를 동시에 사용하는 Durugkar 외 연구진(2016)과 다중 발전기와 하나의 판별기로 그 반대의 일을 하는 Ghosh 외 연구진(2017)에 의해 동기 부여된다. 계층적 GAN(Denton et al., 2015; Huang et al., 2016; Zhang et al., 2017)은 이미지 피라미드의 각 수준에 대한 생성기와 판별기를 정의한다. 이러한 방법은 잠재 이미지에서 고해상도 이미지로의 복잡한 매핑을 단계별로 학습하는 것이 더 쉽다는 우리의 작업과 동일한 관찰을 기반으로 하지만, 중요한 차이점은 이러한 방법의 계층이 아니라 단일 GAN만 있다는 것이다. 적응적으로 성장하는 네트워크에 대한 초기 연구(예: 신경 가스 성장(Fritzke, 1995) 및 네트워크를 탐욕스럽게 성장시키는 증강 토폴로지의 신경 진화(Stanley & Miikkulainen, 2002)와 달리, 우리는 사전 구성된 계층의 도입을 단순히 연기한다. 그런 의미에서 우리의 접근 방식은 자동 인코더의 계층별 훈련과 유사하다(Bengio et al., 2007).

### $\mathbf{3\;INCREASING\;VARIATION\;USING\;MINIBATCH\;STANDARD\;DEVIATION}$

> GANs have a tendency to capture only a subset of the variation found in training data, and Salimans et al. (2016) suggest “minibatch discrimination” as a solution. They compute feature statistics not only from individual images but also across the minibatch, thus encouraging the minibatches of generated and training images to show similar statistics. This is implemented by adding a minibatch layer towards the end of the discriminator, where the layer learns a large tensor that projects the input activation to an array of statistics. A separate set of statistics is produced for each example in a minibatch and it is concatenated to the layer’s output, so that the discriminator can use the statistics  internally. We simplify this approach drastically while also improving the variation.
>> GAN은 훈련 데이터에서 발견된 변동의 하위 집합만 캡처하는 경향이 있으며, Salimans 외 연구진(2016)은 해결책으로 "미니 배치 차별"을 제안한다. 그들은 개별 이미지뿐만 아니라 미니 배치에서 특징 통계를 계산하여 생성된 이미지와 훈련 이미지의 미니 배치가 유사한 통계를 보여주도록 장려한다. 이것은 판별기의 끝에 미니 배치 계층을 추가하여 구현되며, 여기서 계층은 입력 활성화를 통계 배열에 투영하는 큰 텐서를 학습한다. 미니 배치의 각 예제에 대해 별도의 통계 집합이 생성되며, 이는 계층의 출력에 연결되므로 판별자가 내부적으로 통계를 사용할 수 있다. 우리는 이 접근법을 획기적으로 단순화하는 동시에 변동성을 개선한다.

> Our simplified solution has neither learnable parameters nor new hyperparameters. We first compute the standard deviation for each feature in each spatial location over the minibatch. We then average these estimates over all features and spatial locations to arrive at a single value. We replicate the value and concatenate it to all spatial locations and over the minibatch, yielding one additional (constant) feature map. This layer could be inserted anywhere in the discriminator, but we have found it best to insert it towards the end (see Appendix A.1 for details). We experimented with a richer set of statistics, but were not able to improve the variation further. In parallel work, Lin et al. (2017) provide theoretical insights about the benefits of showing multiple images to the discriminator.
>> 우리의 단순화된 솔루션은 학습 가능한 매개 변수나 새로운 하이퍼 매개 변수가 없다. 먼저 미니 배치를 통해 각 공간 위치의 각 특징에 대한 표준 편차를 계산한다. 그런 다음 모든 특징 및 공간 위치에 대해 이러한 추정치를 평균하여 단일 값을 얻는다. 값을 복제하고 모든 공간 위치와 미니 배치에 연결하여 하나의 추가(상수) 피쳐 맵을 산출한다. 이 레이어는 판별기의 아무 곳에나 삽입할 수 있지만, 끝을 향해 삽입하는 것이 가장 좋다는 것을 발견했다(자세한 내용은 부록 A.1 참조). 우리는 더 풍부한 통계로 실험을 했지만 변동을 더 개선할 수는 없었다. 병렬 작업에서, Lin 등(2017)은 판별자에게 여러 이미지를 보여줄 경우의 이점에 대한 이론적 통찰력을 제공한다.

![Figure 2](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-2.JPG)

> Figure 2: When doubling the resolution of the generator (G) and discriminator (D) we fade in the new layers smoothly. This example illustrates the transition from 16 × 16 images (a) to 32 × 32 images (c). During the transition (b) we treat the layers that operate on the higher resolution like a residual block, whose weight α increases linearly from 0 to 1. Here 2× and 0.5× refer to doubling and halving the image resolution using nearest neighbor filtering and average pooling, respectively. The toRGB represents a layer that projects feature vectors to RGB colors and fromRGB does the reverse; both use 1 × 1 convolutions. When training the discriminator, we feed in real images that are downscaled to match the current resolution of the network. During a resolution transition, we interpolate between two resolutions of the real images, similarly to how the generator output combines two resolutions.
>> 그림 2: 생성기(G)와 판별기(D)의 분해능을 두 배로 늘리면 새로운 계층에서 부드럽게 희미해진다. 이 예는 16 × 16 이미지(a)에서 32 × 32 이미지(c)로의 전환을 보여줍니다. 전환 중 (b) 우리는 더 높은 해상도로 작동하는 층을 0에서 1로 선형적으로 증가하는 잔류 블록처럼 처리한다. 여기서 2배와 0.5배는 각각 가장 가까운 이웃 필터링과 평균 풀링을 사용하여 이미지 해상도를 두 배로 하고 반으로 줄이는 것을 의미한다. ToRGB는 RGB 색상에 특징 벡터를 투영하는 레이어를 나타내며, RGB는 그 반대이다. 판별기를 훈련할 때, 우리는 네트워크의 현재 해상도와 일치하도록 축소된 실제 이미지를 공급한다. 해상도 전환 중에, 우리는 제너레이터 출력이 두 해상도를 결합하는 방법과 유사하게 실제 이미지의 두 해상도 사이에 보간한다.

> Alternative solutions to the variation problem include unrolling the discriminator (Metz et al., 2016) to regularize its updates, and a “repelling regularizer” (Zhao et al., 2017) that adds a new loss term to the generator, trying to encourage it to orthogonalize the feature vectors in a minibatch. The multiple generators of Ghosh et al. (2017) also serve a similar goal. We acknowledge that these solutions may increase the variation even more than our solution – or possibly be orthogonal to it – but leave a detailed comparison to a later time.
>> 변동 문제에 대한 대안적인 해결책으로는 판별기(Metz et al., 2016)를 해제하여 업데이트를 정규화하고, 미니 배치에서 특징 벡터를 직교하도록 유도하는 새로운 손실 항을 제너레이터에 추가하는 "반발 정규화"(Zhao et al., 2017)가 있다. Ghosh 외 연구진(2017)의 다중 발전기도 유사한 목표를 달성한다. 우리는 이러한 솔루션이 우리의 솔루션보다 훨씬 더 변동성을 증가시킬 수 있다는 것을 인정하지만, 나중에 자세히 비교한다.

### $\mathbf{4\;NORMALIZATION\;IN\;GENERATOR\;AND\;DISCRIMINATOR}$

> GANs are prone to the escalation of signal magnitudes as a result of unhealthy competition between the two networks. Most if not all earlier solutions discourage this by using a variant of batch normalization (Ioffe & Szegedy, 2015; Salimans & Kingma, 2016; Ba et al., 2016) in the generator, and often also in the discriminator. These normalization methods were originally introduced to eliminate covariate shift. However, we have not observed that to be an issue in GANs, and thus believe that the actual need in GANs is constraining signal magnitudes and competition. We use a different approach that consists of two ingredients, neither of which include learnable parameters.
>> GAN은 두 네트워크 간의 불건전한 경쟁의 결과로 신호 크기가 증가하는 경향이 있다. 모든 이전 솔루션은 아니지만, 대부분 발전기에서 배치 정규화의 변형(Ioffe & Szegdy, 2015; Salimans & Kingma, 2016; Ba et al., 2016)을 사용하고 종종 판별기에서 이를 방지한다. 이러한 정규화 방법은 원래 공변량 이동을 제거하기 위해 도입되었습니다. 그러나 우리는 그것이 GAN에서 문제가 되는 것을 관찰하지 못했기 때문에 GAN의 실제 요구는 신호 크기와 경쟁을 제한하는 것이라고 믿는다. 우리는 학습 가능한 매개 변수를 포함하지 않는 두 가지 요소로 구성된 다른 접근 방식을 사용한다.

#### $\mathbf{4.1\;EQUALIZED\;LEARNING\;RATE}$

> We deviate from the current trend of careful weight initialization, and instead use a trivial $N(0,1)$ initialization and then explicitly scale the weights at runtime. To be precise, we set $\hat{w}_{i} = w_{i}/c$, where $w_{i}$ are the weights and c is the per-layer normalization constant from He’s initializer (He et al., 2015). The benefit of doing this dynamically instead of during initialization is somewhat subtle, and relates to the scale-invariance in commonly used adaptive stochastic gradient descent methods such as RMSProp (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2015). These methods normalize a gradient update by its estimated standard deviation, thus making the update independent of the scale of the parameter. As a result, if some parameters have a larger dynamic range than others, they will take longer to adjust. This is a scenario modern initializers cause, and thus it is possible that a learning rate is both too large and too small at the same time. Our approach ensures that the dynamic range, and thus the learning speed, is the same for all weights. A similar reasoning was independently used by van Laarhoven (2017).
>> 신중한 가중치 초기화의 현재 추세에서 벗어나 대신 사소한 $N(0,1)$ 초기화를 사용한 다음 런타임에 가중치를 명시적으로 스케일링한다. 정확히 말하면, 우리는 $\hat{w}_{i} = w_{i}/c$를 설정하는데, 여기서 $w_{i}$는 가중치이고 c는 He's Initializer의 계층별 정규화 상수이다(He et al., 2015). 초기화 중 대신 동적으로 이 작업을 수행하는 이점은 다소 미묘하며, RMSProp(Tieleman & Hinton, 2012) 및 Adam(Kingma & Ba, 2015)과 같이 일반적으로 사용되는 적응형 확률적 그레이디언트 강하 방법의 스케일 불변성과 관련이 있다. 이러한 방법은 추정된 표준 편차에 의해 그레이디언트 업데이트를 정규화하여 업데이트를 매개 변수의 규모와 무관하게 만든다. 따라서 일부 파라미터의 동적 범위가 다른 파라미터보다 클 경우 조정하는 데 시간이 더 오래 걸립니다. 이것은 현대 이니셜라이저가 야기하는 시나리오이며, 따라서 학습 속도가 너무 크면서도 동시에 너무 작을 수 있다. 우리의 접근 방식은 동적 범위, 따라서 학습 속도가 모든 가중치에 대해 동일하도록 보장한다. 비슷한 추론이 판 라르호벤(2017)에 의해 독립적으로 사용되었다.

#### $\mathbf{4.2\;PIXELWISE\;FEATURE\;VECTOR\;NORMALIZATION\;IN\;GENERATOR}$

> To disallow the scenario where the magnitudes in the generator and discriminator spiral out of control as a result of competition, we normalize the feature vector in each pixel to unit length in the generator after each convolutional layer. 
>> 경쟁의 결과로 제너레이터와 판별기의 크기가 제어 불능으로 나선형인 시나리오를 허용하지 않기 위해, 우리는 각 픽셀의 특징 벡터를 각 컨볼루션 레이어 후 제너레이터의 단위 길이로 정규화한다.

> We do this using a variant of “local response normalization” (Krizhevsky et al., 2012), configured as $b_{x,y}=a_{x,y}/\sqrt{\frac{1}{N}\sum_{j=0}^{N-1}(a_{x,y}^{j})^{2}+\epsilon}$, where $\epsilon=10^{−8}$, $N$ is the number of feature maps, and $a_{x,y}$ and $b_{x,y}$ are the original and normalized feature vector in pixel $(x,y)$, respectively. 
>> 우리는 이것을 $b_{x,y}=a_{x,y}/\sqrt{\frac{1}{N}\sum_{j=0}^{N-1}(a_{x,y}^{j})^{2}+\epsilon}$로 구성된 "로컬 응답 정규화"(Krizhevsky et al., 2012)의 변형을 사용하여 한다. 여기서 $\epsilon=10^{−8}$이며 N은 피처 맵의 수이고, $a_{x,y}$와 $b_{x,y}$ 는 각각 픽셀 $(x,y)$의 원래 및 정규화된 피처 벡터이다. 

> We find it surprising that this heavy-handed constraint does not seem to harm the generator in any way, and indeed with most datasets it does not change the results much, but it prevents the escalation of signal magnitudes very effectively when needed.
>> 우리는 이 고압적인 제약이 어떤 식으로든 발전기를 해치지 않는 것처럼 보인다는 것이 놀랍고, 실제로 대부분의 데이터 세트에서는 결과를 크게 바꾸지는 않지만 필요할 때 신호 크기의 증가를 매우 효과적으로 방지한다.

### $\mathbf{5\;MULTI-SCALE\;STATISTICAL\;SIMILARITY\;FOR\;ASSESSING\;GAN\;RESULTS}$

> In order to compare the results of one GAN to another, one needs to investigate a large number of images, which can be tedious, difficult, and subjective. Thus it is desirable to rely on automatedmethods that compute some indicative metric from large image collections. We noticed that existing methods such as MS-SSIM (Odena et al., 2017) find large-scale mode collapses reliably but fail to react to smaller effects such as loss of variation in colors or textures, and they also do not directly assess image quality in terms of similarity to the training set.
>> 한 GAN의 결과를 다른 GAN과 비교하려면 많은 수의 이미지를 조사해야 하는데, 이는 지루하고 어렵고 주관적일 수 있다. 따라서 대규모 이미지 컬렉션에서 일부 지시 메트릭을 계산하는 자동화된 방법에 의존하는 것이 바람직하다. MS-SSIM(Odena et al., 2017)과 같은 기존 방법은 대규모 모드 붕괴를 신뢰성 있게 발견하지만 색상이나 질감의 변화 손실과 같은 더 작은 효과에 반응하지 않으며, 또한 훈련 세트와의 유사성 측면에서 이미지 품질을 직접 평가하지 않는다.

> We build on the intuition that a successful generator will produce samples whose local image structure is similar to the training set over all scales. We propose to study this by considering the multiscale statistical similarity between distributions of local image patches drawn from Laplacian pyramid (Burt & Adelson, 1987) representations of generated and target images, starting at a low-pass resolution of 16 × 16 pixels. As per standard practice, the pyramid progressively doubles until the full resolution is reached, each successive level encoding the difference to an up-sampled version of the previous level.
>> 우리는 성공적인 생성기가 모든 스케일에 걸쳐 훈련 세트와 유사한 로컬 이미지 구조를 가진 샘플을 생성할 것이라는 직관을 기반으로 한다. 우리는 16 × 16 픽셀의 낮은 패스 해상도에서 시작하여 생성된 이미지와 대상 이미지의 라플라시안 피라미드(Burt & Adelson, 1987) 표현에서 그려진 로컬 이미지 패치의 분포 간의 멀티 스케일 통계 유사성을 고려하여 이를 연구할 것을 제안한다. 표준 관례에 따라 피라미드는 완전한 해상도에 도달할 때까지 점진적으로 두 배로 증가하며, 각 연속 레벨은 이전 레벨의 업샘플링 버전으로 차이를 인코딩한다.

> A single Laplacian pyramid level corresponds to a specific spatial frequency band. We randomly sample 16384 images and extract 128 descriptors from each level in the Laplacian pyramid, giving us $2^{21}$ (2.1M) descriptors per level. Each descriptor is a 7 × 7 pixel neighborhood with 3 color channels, denoted by $x\in{R}^{7×7×3}=R^{147}$. We denote the patches from level $l$ of the training set and generated set as $x_{i=1}^{x^{21}}$ and $y_{i=1}^{2^{21}}$, respectively. We first normalize $x$ and $y$ w.r.t. the mean and standard deviation of each color channel, and then estimate the statistical similarity by computing their sliced Wasserstein distance SWD $(x_{i}^{l},\;y_{i}^{l})$, an efficiently computable randomized approximation to earthmovers distance, using 512 projections (Rabin et al., 2011).
>> 단일 라플라시안 피라미드 레벨은 특정 공간 주파수 대역에 해당한다. 우리는 무작위로 16384개의 이미지를 샘플링하고 라플라시안 피라미드의 각 수준에서 128개의 설명자를 추출하여 수준당 $2^{21}$(2.1M)의 설명자를 제공한다. 각 디스크립터는 3개의 컬러 채널을 가진 7 × 7 픽셀 이웃이며, $x\in{R}^{7×7×3}=R^{147}$로 표시된다. 우리는 훈련 세트와 생성된 세트의 수준 $l$의 패치를 각각 $x_{i=1}^{x^{21}}$와 $y_{i=1}^{2^{21}}$로 표시한다. 먼저 각 색상 채널의 평균 및 표준 편차인 $x$ 및 $y$ w.r.t를 정규화한 다음 512개의 투영법을 사용하여 지구 이동 거리에 대한 효율적으로 계산 가능한 무작위 근사치인 슬라이스된 와서스테인 거리 SWD $(x_{i}^{l},\;y_{i}^{l})$를 계산하여 통계적 유사성을 추정한다(Rabin et al., 2011).

> Intuitively a small Wasserstein distance indicates that the distribution of the patches is similar, meaning that the training images and generator samples appear similar in both appearance and variation at this spatial resolution. In particular, the distance between the patch sets extracted from the lowestresolution 16 × 16 images indicate similarity in large-scale image structures, while the finest-level patches encode information about pixel-level attributes such as sharpness of edges and noise.
>> 직관적으로 작은 와서스테인 거리는 패치의 분포가 유사하다는 것을 나타내며, 이는 훈련 이미지와 생성기 샘플이 이 공간 해상도에서 모양과 변화 모두에서 유사하게 보인다는 것을 의미한다. 특히, 가장 낮은 해상도 16 × 16 이미지에서 추출된 패치 세트 사이의 거리는 대규모 이미지 구조에서 유사성을 나타내는 반면, 최고급 레벨 패치는 가장자리의 선명도 및 노이즈와 같은 픽셀 레벨 속성에 대한 정보를 인코딩한다.

### $\mathbf{6\;EXPERIMENTS}$

> In this section we discuss a set of experiments that we conducted to evaluate the quality of our results. Please refer to Appendix A for detailed description of our network structures and training configurations. We also invite the reader to consult the accompanying video ([https://youtu.be/G06dEcZ-QTg](https://youtu.be/G06dEcZ-QTg)) for additional result images and latent space interpolations. In this section we will distinguish between the network structure (e.g., convolutional layers, resizing), training configuration (various normalization layers, minibatch-related operations), and training loss (WGAN-GP, LSGAN).
>> 이 섹션에서는 결과의 품질을 평가하기 위해 수행한 일련의 실험에 대해 논의합니다. 네트워크 구조 및 교육 구성에 대한 자세한 설명은 부록 A를 참조하십시오. 또한 추가 결과 이미지 및 잠재 공간 보간은 동봉된 비디오([https://youtu.be/G06dEcZ-QTg](https://youtu.be/G06dEcZ-QTg)))를 참조하도록 독자를 초대한다. 이 섹션에서는 네트워크 구조(예: 컨볼루션 계층, 크기 조정), 훈련 구성(다양한 정규화 계층, 미니 배치 관련 작업) 및 훈련 손실(WGAN-GP, LSGAN)을 구별한다.

#### $\mathbf{6.1\;IMPORTANCE\;OF\;INDIVIDUAL\;CONTRIBUTIONS\;IN\;TERMS\;OF\;STATISTICAL\;SIMILARITY}$

> We will first use the sliced Wasserstein distance (SWD) and multi-scale structural similarity (MSSSIM) (Odena et al., 2017) to evaluate the importance our individual contributions, and also perceptually validate the metrics themselves. We will do this by building on top of a previous state-of-theart loss function (WGAN-GP) and training configuration (Gulrajani et al., 2017) in an unsupervised setting using CELEBA (Liu et al., 2015) and LSUN BEDROOM (Yu et al., 2015) datasets in $128^{2}$ resolution. CELEBA is particularly well suited for such comparison because the training images contain noticeable artifacts (aliasing, compression, blur) that are difficult for the generator to reproduce faithfully. In this test we amplify the differences between training configurations by choosing a relatively low-capacity network structure (Appendix A.2) and terminating the training once the discriminator has been shown a total of 10M real images. As such the results are not fully converged.
>> 먼저 슬라이스된 와서스테인 거리(SWD)와 다중 스케일 구조적 유사성(MSSIM)(Odena et al., 2017)을 사용하여 개별 기여의 중요성을 평가하고 메트릭 자체를 지각적으로 검증한다. $128^{2}$ 해상도의 CELEBA(Liu 등, 2015) 및 LSUN BEDRINE(Yu 등, 2015) 데이터 세트를 사용하여 감독되지 않은 설정에서 이전 최첨단 손실 함수(WGAN-GP) 및 훈련 구성(Gulrajani 등, 2017)을 기반으로 이를 수행한다. CELEBA는 훈련 이미지에 생성기가 충실하게 재현하기 어려운 눈에 띄는 아티팩트(앨리어싱, 압축, 블러)가 포함되어 있기 때문에 이러한 비교에 특히 적합하다. 이 테스트에서 우리는 비교적 낮은 용량의 네트워크 구조(부록 A.2)를 선택하고 판별기가 총 10M개의 실제 이미지를 보여주면 훈련을 종료함으로써 훈련 구성 간의 차이를 증폭한다. 따라서 결과가 완전히 수렴되지 않는다.

![Table 1](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Table-1.JPG)

> Table 1: Sliced Wasserstein distance (SWD) between the generated and training images (Section 5) and multi-scale structural similarity (MS-SSIM) among the generated images for several training setups at 128 × 128. For SWD, each column represents one level of the Laplacian pyramid, and the last one gives an average of the four distances.
>> 표 1: 128 × 128의 여러 훈련 설정에 대해 생성된 이미지 중 생성된 이미지와 훈련 이미지 사이의 슬라이스된 와서스테인 거리(SWD)와 다중 스케일 구조적 유사성(MS-SSIM)이다. SWD의 경우, 각 열은 라플라시안 피라미드의 한 수준을 나타내며, 마지막 열은 4개의 거리를 평균한다.

![Figure 3](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-3.JPG)

> Figure 3: (a) – (g) CELEBA examples corresponding to rows in Table 1. These are intentionally non-converged. (h) Our converged result. Notice that some images show aliasing and some are not sharp – this is a flaw of the dataset, which the model learns to replicate faithfully.
>> 그림 3: (a) – (g) 표 1의 행에 해당하는 CELEBA 예 이것들은 의도적으로 수렴되지 않았다. (h) 수렴된 결과입니다. 일부 이미지는 앨리어싱을 보여주고 일부는 선명하지 않다는 점에 주목하십시오. 이것은 모델이 충실하게 복제하는 방법을 배우는 데이터 세트의 결함입니다.

> Table 1 lists the numerical values for SWD and MS-SSIM in several training configurations, where our individual contributions are cumulatively enabled one by one on top of the baseline (Gulrajani et al., 2017). The MS-SSIM numbers were averaged from 10000 pairs of generated images, and SWD was calculated as described in Section 5. Generated CELEBA images from these configurations are shown in Figure 3. Due to space constraints, the figure shows only a small number of examples for each row of the table, but a significantly broader set is available in Appendix H. Intuitively, a good evaluation metric should reward plausible images that exhibit plenty of variation in colors, textures, and viewpoints. However, this is not captured by MS-SSIM: we can immediately see that configuration (h) generates significantly better images than configuration (a), but MS-SSIM remains approximately unchanged because it measures only the variation between outputs, not similarity to the training set. SWD, on the other hand, does  indicate a clear improvement.
>> 표 1에는 여러 교육 구성에서 SWD 및 MS-SSIM에 대한 수치가 나열되어 있으며, 여기서 개별 기여는 기준선 위에 하나씩 누적적으로 활성화된다(Gulrajani et al., 2017). MS-SSIM 숫자는 생성된 이미지 10000쌍에서 평균화되었으며, SWD는 섹션 5에 설명된 대로 계산되었다. 이러한 구성에서 생성된 CELEBA 이미지는 그림 3에 나와 있습니다. 공간 제약으로 인해 이 그림은 표의 각 행에 대한 소수의 예만 보여주지만 부록 H에서는 상당히 광범위한 집합을 사용할 수 있다. 직관적으로 좋은 평가 지표는 색상, 질감 및 관점에 많은 변화를 보이는 그럴듯한 이미지를 보상해야 한다. 그러나 이것은 MS-SSIM에 의해 캡처되지 않는다. 구성(h)이 구성(a)보다 훨씬 더 나은 이미지를 생성한다는 것을 즉시 알 수 있지만, MS-SSIM은 훈련 세트와 유사성이 아닌 출력 간의 변동만 측정하기 때문에 거의 변경되지 않았다. 반면에, 사회 복지부는 분명한 개선을 나타낸다.

> The first training configuration (a) corresponds to Gulrajani et al. (2017), featuring batch normalization in the generator, layer normalization in the discriminator, and minibatch size of 64. (b) enables progressive growing of the networks, which results in sharper and more believable output images. SWD correctly finds the distribution of generated images to be more similar to the training set.
>> 첫 번째 훈련 구성(a)은 Gulrajani et al. (2017)에 해당하며, 생성기의 배치 정규화, 판별기의 계층 정규화 및 64의 미니 배치 크기를 특징으로 한다. (b) 네트워크의 점진적인 성장을 가능하게 하여 더 선명하고 신뢰할 수 있는 출력 이미지를 생성한다. SWD는 생성된 이미지의 분포가 교육 세트와 더 유사하다는 것을 올바르게 찾습니다.

> Our primary goal is to enable high output resolutions, and this requires reducing the size of minibatches in order to stay within the available memory budget. We illustrate the ensuing challenges in (c) where we decrease the minibatch size from 64 to 16. The generated images are unnatural, which is clearly visible in both metrics. In (d), we stabilize the training process by adjusting the hyperparameters as well as by removing batch normalization and layer normalization (Appendix A.2). As an intermediate test (e∗), we enable minibatch discrimination (Salimans et al., 2016), which somewhat surprisingly fails to improve any of the metrics, including MS-SSIM that measures output variation. In contrast, our minibatch standard deviation (e) improves the average SWD scores and images. We then enable our remaining contributions in (f) and (g), leading to an overall improvement in SWD and subjective visual quality. Finally, in (h) we use a non-crippled network and longer training – we feel the quality of the generated images is at least comparable to the best published results so far.
>> 우리의 주요 목표는 높은 출력 해상도를 가능하게 하는 것이며, 이는 사용 가능한 메모리 예산 내에서 유지하려면 미니배치 크기를 줄여야 한다. 우리는 미니 배치 크기를 64에서 16으로 줄이는 (c)의 후속 과제를 설명한다. 생성된 이미지는 부자연스러우며, 이는 두 메트릭에서 분명하게 볼 수 있습니다. (d)에서, 우리는 배치 정규화 및 계층 정규화를 제거함으로써 하이퍼 매개 변수를 조정함으로써 훈련 과정을 안정화시킨다(부록 A.2). 중간 테스트(e²)로서 미니 배치 판별(Salimans et al., 2016)을 가능하게 하는데, 이는 출력 변동을 측정하는 MS-SSIM을 포함하여 어느 메트릭도 개선하지 못한다. 대조적으로, 우리의 미니 배치 표준 편차(e)는 평균 SWD 점수와 이미지를 향상시킨다. 그런 다음 (f)와 (g)에서 나머지 기여를 활성화하여 SWD와 주관적인 시각적 품질을 전반적으로 향상시킨다. 마지막으로, (h)에서 우리는 손상되지 않은 네트워크와 더 긴 훈련을 사용한다. 생성된 이미지의 품질은 적어도 지금까지 발표된 최고의 결과와 비교할 수 있다고 느낀다.

![Figure 4](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-4.JPG)

> Figure 4: Effect of progressive growing on training speed and convergence. The timings were measured on a single-GPU setup using NVIDIA Tesla P100. (a) Statistical similarity with respect to wall clock time for Gulrajani et al. (2017) using CELEBA at 128 × 128 resolution. Each graph represents sliced Wasserstein distance on one level of the Laplacian pyramid, and the vertical line indicates the point where we stop the training in Table 1. (b) Same graph with progressive growing enabled. The dashed vertical lines indicate points where we double the resolution of G and D. (c) Effect of progressive growing on the raw training speed in 1024 × 1024 resolution.
>> 그림 4: 점진적인 성장이 교육 속도와 융합에 미치는 영향 타이밍은 NVIDIA Tesla P100을 사용한 단일 GPU 설정에서 측정되었다. (a) 128 × 128 해상도의 CELEBA를 사용한 Gulrajani 외(2017)의 벽시계 시간에 대한 통계적 유사성. 각 그래프는 라플라시안 피라미드의 한 수준에서 분할된 와서스테인 거리를 나타내며, 수직선은 우리가 훈련을 중단하는 지점을 표 1에서 나타낸다. (b) 점진적 성장이 활성화된 동일한 그래프이다. 점선 수직선은 G와 D의 분해능을 두 배로 하는 지점을 나타낸다. (c) 1024 × 1024 해상도의 원시 훈련 속도에 대한 점진적 성장 효과

#### $\mathbf{6.2\;CONVERGENCE\;AND\;TRAINING\;SPEED}$

> Figure 4 illustrates the effect of progressive growing in terms of the SWD metric and raw image throughput. The first two plots correspond to the training configuration of Gulrajani et al. (2017) without and with progressive growing. We observe that the progressive variant offers two main benefits: it converges to a considerably better optimum and also reduces the total training time by about a factor of two. The improved convergence is explained by an implicit form of curriculum learning that is imposed by the gradually increasing network capacity. Without progressive growing, all layers of the generator and discriminator are tasked with simultaneously finding succinct intermediate representations for both the large-scale variation and the small-scale detail. With progressive growing, however, the existing low-resolution layers are likely to have already converged early on, so the networks are only tasked with refining the representations by increasingly smaller-scale effects as new layers are introduced. Indeed, we see in Figure 4(b) that the largest-scale statistical similarity curve (16) reaches its optimal value very quickly and remains consistent throughout the rest of the training. The smaller-scale curves (32, 64, 128) level off one by one as the resolution is increased, but the convergence of each curve is equally consistent. With non-progressive training in Figure 4(a), each scale of the SWD metric converges roughly in unison, as could be expected.
>> 그림 4는 SWD 메트릭과 원시 이미지 처리량 측면에서 점진적 성장의 효과를 보여줍니다. 처음 두 개의 플롯은 점진적 성장이 없는 굴라자니 외(2017)의 훈련 구성에 해당한다. 우리는 점진적 변형이 두 가지 주요 이점을 제공한다는 것을 관찰한다. 그것은 상당히 더 나은 최적 상태로 수렴되고 또한 총 훈련 시간을 약 2배 단축시킨다. 향상된 수렴은 점차 증가하는 네트워크 용량에 의해 부과되는 암시적 형태의 커리큘럼 학습으로 설명된다. 점진적인 성장 없이, 생성기와 판별기의 모든 계층은 대규모 변동과 소규모 세부 사항 모두에 대한 간결한 중간 표현을 동시에 찾는 임무를 수행한다. 그러나 점진적인 성장과 함께 기존의 저해상도 계층은 이미 초기에 수렴되었을 가능성이 높기 때문에 네트워크는 새로운 계층이 도입됨에 따라 점점 더 작은 규모의 효과로 표현을 다듬는 작업만 수행하게 된다. 실제로, 우리는 그림 4(b)에서 가장 큰 규모의 통계 유사성 곡선(16)이 매우 빠르게 최적 값에 도달하고 나머지 훈련 내내 일관성을 유지한다는 것을 볼 수 있다. 크기가 작은 곡선(32, 64, 128)은 분해능이 증가함에 따라 하나씩 수평을 유지하지만 각 곡선의 수렴은 동일하게 일치합니다. 그림 4(a)의 비진행형 교육을 통해, 예상대로 SWD 메트릭의 각 척도는 대략적으로 일치한다.

> The speedup from progressive growing increases as the output resolution grows. Figure 4(c) shows training progress, measured in number of real images shown to the discriminator, as a function of training time when the training progresses all the way to 10242 resolution. We see that progressive growing gains a significant head start because the networks are shallow and quick to evaluate at the beginning. Once the full resolution is reached, the image throughput is equal between the two methods. The plot shows that the progressive variant reaches approximately 6.4 million images in 96 hours, whereas it can be extrapolated that the non-progressive variant would take about 520 hours to reach the same point. In this case, the progressive growing offers roughly a 5.4× speedup.
>> 점진적 성장으로부터의 속도는 출력 해상도가 증가함에 따라 증가합니다. 그림 4(c)는 훈련이 1,0242 해상도까지 진행될 때 훈련 시간의 함수로써 판별자에게 보여지는 실제 이미지의 수로 측정된 훈련 진행률을 보여준다. 네트워크가 얕고 초기에 평가하는 것이 빠르기 때문에 점진적 성장이 상당한 선행 출발점을 얻는다는 것을 알 수 있다. 전체 해상도에 도달하면 이미지 처리량이 두 방법 사이에서 동일합니다. 그래프는 프로그레시브 변형이 96시간 만에 약 640만 장의 이미지에 도달하는 반면, 비프로그레시브 변형이 동일한 지점에 도달하는 데 약 520시간이 걸린다는 것을 추정할 수 있다. 이 경우 점진적인 성장은 대략 5.4배의 속도를 제공한다.

#### $\mathbf{6.3\;HIGH-RESOLUTION\;IMAGE\;GENERATION\;USING\;CELEBA-HQ\;DATASET}$

> To meaningfully demonstrate our results at high output resolutions, we need a sufficiently varied high-quality dataset. However, virtually all publicly available datasets previously used in GAN literature are limited to relatively low resolutions ranging from $32^{2}$ to $480^{2}$ . To this end, we created a high-quality version of the CELEBA dataset consisting of 30000 of the images at 1024 × 1024 resolution. We refer to Appendix C for further details about the generation of this dataset.
>> 높은 출력 해상도에서 우리의 결과를 의미 있게 입증하기 위해서는 충분히 다양한 고품질 데이터 세트가 필요하다. 그러나 GAN 문헌에서 이전에 사용된 사실상 공개적으로 사용 가능한 모든 데이터 세트는 $32^{2}$에서 $480^{2}$ 사이의 비교적 낮은 해상도로 제한된다. 이를 위해 1024 × 1024 해상도의 이미지 3만 개로 구성된 고품질 버전의 CELEBA 데이터 세트를 만들었다. 이 데이터 세트의 생성에 대한 자세한 내용은 부록 C를 참조한다.

![Figure 5](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-5.JPG)

> Figure 5: 1024 × 1024 images generated using the CELEBA-HQ dataset. See Appendix F for a larger set of results, and the accompanying video for latent space interpolations.
>그림 5: CELEBA-HQ 데이터 세트를 사용하여 생성된 1024 × 1024 이미지 더 큰 결과 집합은 부록 F를 참조하고 잠재 공간 보간은 첨부 비디오를 참조하십시오.

![Figure 6](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-6.JPG)

> Figure 6: Visual quality comparison in LSUN BEDROOM; pictures copied from the cited articles.
>> 그림 6: LSUN BEDROOM의 시각적 품질 비교. 인용된 기사를 복사한 사진.

> Our contributions allow us to deal with high output resolutions in a robust and efficient fashion. Figure 5 shows selected 1024 × 1024 images produced by our network. While megapixel GAN results have been shown before in another dataset (Marchesi, 2017), our results are vastly more varied and of higher perceptual quality. Please refer to Appendix F for a larger set of result images as well as the nearest neighbors found from the training data. The accompanying video shows latent space interpolations and visualizes the progressive training. The interpolation works so that we first randomize a latent code for each frame (512 components sampled individually from $N(0,1)$), then blur the latents across time with a Gaussian (σ = 45 frames @ 60Hz), and finally normalize each vector to lie on a hypersphere.
>> 우리의 기여를 통해 강력하고 효율적인 방식으로 고출력 해상도를 처리할 수 있다. 그림 5는 네트워크에서 생성된 선택된 1024 × 1024 이미지를 보여줍니다. 메가픽셀 GAN 결과는 이전에 다른 데이터 세트(Marchesi, 2017)에서 보여졌지만, 우리의 결과는 훨씬 더 다양하고 지각 품질이 높다. 교육 데이터에서 발견된 더 큰 결과 영상 세트와 가장 가까운 이웃에 대해서는 부록 F를 참조하십시오. 첨부된 비디오는 잠재된 공간 보간을 보여주고 점진적인 훈련을 시각화한다. 보간은 먼저 각 프레임에 대한 잠재 코드를 무작위화한 다음($N(0,1)$에서 개별적으로 샘플링된 구성 요소를 추출한 후 가우스(가우스 = 45 프레임 @ 60Hz)로 잠재 코드를 흐리게 하고 마지막으로 각 벡터를 초구상에 놓이도록 정규화하도록 작동한다.

> We trained the network on 8 Tesla V100 GPUs for 4 days, after which we no longer observed qualitative differences between the results of consecutive training iterations. Our implementation used an adaptive minibatch size depending on the current output resolution so that the available memory budget was optimally utilized.
>> 우리는 4일 동안 8개의 Tesla V100 GPU에 대해 네트워크를 훈련시켰으며, 이후 연속 훈련 반복 결과 간의 질적 차이를 더 이상 관찰하지 못했다. 우리의 구현은 사용 가능한 메모리 예산이 최적으로 활용되도록 현재 출력 해상도에 따라 적응형 미니 배치 크기를 사용했다.

> In order to demonstrate that our contributions are largely orthogonal to the choice of a loss function, we also trained the same network using LSGAN loss instead of WGAN-GP loss. Figure 1 shows six examples of 10242 images produced using our method using LSGAN. Further details of this setup are given in Appendix B.
>> 우리의 기여가 손실 함수의 선택과 크게 직교한다는 것을 보여주기 위해, 우리는 또한 WGAN-GP 손실 대신 LSGAN 손실을 사용하여 동일한 네트워크를 훈련시켰다. 그림 1은 LSGAN을 사용하여 우리의 방법을 사용하여 생성된 10242 이미지의 여섯 가지 예를 보여준다. 이 설정에 대한 자세한 내용은 부록 B에 나와 있습니다.

![Figure 7](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-7.JPG)

> Figure 7: Selection of 256 × 256 images generated from different LSUN categories.
>> 그림 7: 다양한 LSUN 범주에서 생성된 256 × 256 이미지 선택

#### $\mathbf{6.4\;LSUN\;RESULTS}$

> Figure 6 shows a purely visual comparison between our solution and earlier results in LSUN BEDROOM. Figure 7 gives selected examples from seven very different LSUN categories at 2562 . A larger, non-curated set of results from all 30 LSUN categories is available in Appendix G, and the video demonstrates interpolations. We are not aware of earlier results in most of these categories, and while some categories work better than others, we feel that the overall quality is high.
>> 그림 6은 LSUN BEDROOM의 솔루션과 이전 결과 사이의 순수한 시각적 비교를 보여준다. 그림 7은 2562의 매우 다른 7개의 LSUN 범주에서 선택된 예를 보여준다. 모든 30개의 LSUN 범주에서 더 큰 비경화 결과 집합을 부록 G에서 사용할 수 있으며, 비디오는 보간법을 보여준다. 우리는 대부분의 이러한 범주에서 초기 결과를 알지 못하며, 일부 범주가 다른 범주보다 더 잘 작동하지만, 우리는 전반적인 품질이 높다고 느낀다.

#### $\mathbf{6.5\;CIFAR10\;INCEPTION\;SCORES}$

> The best inception scores for CIFAR10 (10 categories of 32 × 32 RGB images) we are aware of are 7.90 for unsupervised and 8.87 for label conditioned setups (Grinblat et al., 2017). The large difference between the two numbers is primarily caused by “ghosts” that necessarily appear between classes in the unsupervised setting, while label conditioning can remove many such transitions. When all of our contributions are enabled, we get 8.80 in the unsupervised setting. Appendix D shows a representative set of generated images along with a more comprehensive list of results from earlier methods. The network and training setup were the same as for CELEBA, progression limited to 32 × 32 of course. The only customization was to the WGAN-GP’s regularization term $E_{\hat{x}\sim{P_{\hat{x}}}}[(\parallel{}\triangledown{}_{\hat{x}}D(\hat{x})\parallel{}-\gamma)^{2}/\gamma{}^{2} ]$. Gulrajani et al. (2017) used $\gamma{}=1.0$, which corresponds to 1-Lipschitz, but we noticed that it is in fact significantly better to prefer fast transitions $(\gamma{}=750)$ to minimize the ghosts. We have not tried this trick with other datasets.
>> 우리가 알고 있는 CIFAR10(32 × 32 RGB 이미지의 10개 범주)에 대한 최고 시작 점수는 감독되지 않은 7.90점과 레이블 조건 설정의 8.87점이다(Grinblat et al., 2017). 두 숫자 사이의 큰 차이는 주로 감독되지 않은 설정에서 클래스 사이에 반드시 나타나는 "유령"에 의해 발생하는 반면, 레이블 조건화는 그러한 많은 전환을 제거할 수 있다. 모든 기여가 활성화되면 감독되지 않은 환경에서 8.80을 얻습니다. 부록 D는 생성된 이미지의 대표적인 집합을 이전 방법의 결과 목록과 함께 보여준다. 네트워크와 훈련 설정은 CELEBA와 동일하며, 진행은 물론 32 × 32로 제한되었다. 유일한 사용자 지정은 WGAN-GP의 정규화 용어 $E_{\hat{x}\sim{P_{\hat{x}}}}[(\parallel{}\triangledown{}_{\hat{x}}D(\hat{x})\parallel{}-\gamma)^{2}/\gamma{}^{2} ]$에 대한 것이었다. 굴라자니 외 연구진(2017)은 1-립시츠에 해당하는 $\gamma{}=1.0$를 사용했지만, 유령을 최소화하기 위해 실제로 빠른 전이 $(\gamma{}=750)$를 선호하는 것이 훨씬 더 낫다는 것을 발견했다. 우리는 다른 데이터 세트에서는 이 트릭을 시도하지 않았다.

### $\mathbf{7\;DISCUSSION}$

> While the quality of our results is generally high compared to earlier work on GANs, and the training is stable in large resolutions, there is a long way to true photorealism. Semantic sensibility and understanding  dataset-dependent constraints, such as certain objects being straight rather than curved, leaves a lot to be desired. There is also room for improvement in the micro-structure of the images. That said, we feel that convincing realism may now be within reach, especially in CELEBA-HQ.
>> 우리 결과의 품질은 GAN에 대한 이전 작업에 비해 일반적으로 높고, 훈련은 큰 해상도에서 안정적이지만, 진정한 포토리얼리즘에는 갈 길이 멀다. 의미론적 민감성과 특정 물체가 곡선이 아닌 직선인 것과 같은 데이터 세트 의존적 제약 조건을 이해하는 것은 아쉬운 점이 많다. 영상의 미세 구조도 개선할 여지가 있다. 그렇긴 하지만, 특히 CELEBA-HQ에서 설득력 있는 현실주의가 이제 도달할 수 있다고 느낀다.

### $\mathbf{8.\;Acknowledgments}$

> We would like to thank Mikael Honkavaara, Tero Kuosmanen, and Timi Hietanen for the compute infrastructure. Dmitry Korobchenko and Richard Calderwood for efforts related to the CELEBA-HQ dataset. Oskar Elek, Jacob Munkberg, and Jon Hasselgren for useful comments.
>> Mikael Honkavaara, Tero Kuosmanen 및 Timi Hietanen에게 컴퓨팅 인프라를 제공해 주셔서 감사합니다. CELEBA-HQ 데이터 세트와 관련된 노력을 위한 Dmitry Korobchenco와 Richard Calderwood. 오스카 엘렉, 제이콥 멍크버그, 존 하셀그렌이 유용한 코멘트를 제공합니다.

![Figure 8](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-8.JPG)

> Figure 8: Creating the CELEBA-HQ dataset. We start with a JPEG image (a) from the CelebA inthe-wild dataset. We improve the visual quality (b,top) through JPEG artifact removal (b,middle) and 4x super-resolution (b,bottom). We then extend the image through mirror padding (c) and Gaussian filtering (d) to produce a visually pleasing depth-of-field effect. Finally, we use the facial landmark locations to select an appropriate crop region (e) and perform high-quality resampling to obtain the final image at 1024 × 1024 resolution (f).
>> 그림 8: CELEBA-HQ 데이터 세트 생성 우리는 야생 데이터 세트의 CellebA의 JPEG 이미지(a)로 시작한다. JPEG 아티팩트 제거(b,middle) 및 4배 초고해상도(b,bottom)를 통해 시각적 품질(b,top)을 향상시킨다. 그런 다음 미러 패딩(c) 및 가우스 필터링(d)을 통해 이미지를 확장하여 시각적으로 만족스러운 장 깊이 효과를 생성한다. 마지막으로, 얼굴 랜드마크 위치를 사용하여 적절한 크롭 영역(e)을 선택하고 고품질 재샘플링을 수행하여 1024 × 1024 해상도(f)에서 최종 이미지를 얻는다.

![Figure 9](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-9.JPG)

> Figure 9: CIFAR10 images generated using a network that was trained unsupervised (no label conditioning), and achieves a record 8.80 inception score.
>> 그림 9: 감독되지 않은(라벨 조정 없이) 훈련된 네트워크를 사용하여 생성된 CIFAR10 이미지는 기록적인 8.80 초기화 점수를 달성한다.

![Figure 10](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-10.JPG)

> Figure 10: Top: Our CELEBA-HQ results. Next five rows: Nearest neighbors found from the training data, based on feature-space distance. We used activations from five VGG layers, as suggested by Chen & Koltun (2017). Only the crop highlighted in bottom right image was used for comparison in order to exclude image background and focus the search on matching facial features.
>> 그림 10: 상단: CELEBA-HQ 결과 다음 다섯 행: 피처 공간 거리를 기준으로 교육 데이터에서 발견된 가장 가까운 이웃입니다. Chen & Koltun (2017)이 제안한 대로 5개의 VGG 계층에서 활성화를 사용했다. 이미지 배경을 제외하고 일치하는 얼굴 특징에 검색을 집중하기 위해 오른쪽 아래 이미지에서 강조 표시된 자르기만 비교에 사용되었습니다.

![Figure 11](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-11.JPG)

> Figure 11: Additional 1024×1024 images generated using the CELEBA-HQ dataset. Sliced Wasserstein Distance (SWD) 103 for levels 1024, . . . , 16: 7.48, 7.24, 6.08, 3.51, 3.55, 3.02, 7.22, for which the average is 5.44. Frechet  Inception Distance (FID) computed from 50K images was 7.30. See the video for latent space interpolations.
>> 그림 11: CELEBA-HQ 데이터 세트를 사용하여 생성된 추가 1024×1024 이미지. 수준 1024, ..., 16: 7.48, 7.24, 6.08, 3.51, 3.55, 3.02, 7.22에 대한 SWD(Slimed Wasserstein Distance) 103. 평균은 5.44입니다. 50K 이미지에서 계산된 FID(Frechet Inception Distance)는 7.30이었다. 잠재 공간 보간은 비디오를 참조하십시오.

![Figure 12](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-12.JPG)

> Figure 12: Example images generated at 256 × 256 from LSUN categories. Sliced Wasserstein Distance (SWD) ×103 is given for levels 256, 128, 64, 32 and 16, and the average is bolded. We also quote the Frechet Inception Distance (FID) computed from 50K images.
>> 그림 12: LSUN 범주에서 256 × 256에서 생성된 이미지 예 SWD × 103은 256, 128, 64, 32, 16 레벨에 대해 주어지며 평균은 굵게 표시됩니다. 또한 50K 이미지에서 계산된 FID(Frechet Inception Distance)를 인용한다.

![Figure 13](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-13.JPG)

> Figure 13: Example images generated at 256 × 256 from LSUN categories. Sliced Wasserstein Distance (SWD) ×103 is given for levels 256, 128, 64, 32 and 16, and the average is bolded. We also quote the Frechet Inception Distance (FID) computed from 50K images.
>> 그림 13: LSUN 범주에서 256 × 256에서 생성된 이미지 예 SWD × 103은 256, 128, 64, 32, 16 레벨에 대해 주어지며 평균은 굵게 표시됩니다. 또한 50K 이미지에서 계산된 FID(Frechet Inception Distance)를 인용한다.

![Figure 14](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-14.JPG)

> Figure 14: Example images generated at 256 × 256 from LSUN categories. Sliced Wasserstein Distance (SWD) ×103 is given for levels 256, 128, 64, 32 and 16, and the average is bolded. We also quote the Frechet Inception Distance (FID) computed from 50K images.
>> 그림 14: LSUN 범주에서 256 × 256에서 생성된 이미지 예 SWD × 103은 256, 128, 64, 32, 16 레벨에 대해 주어지며 평균은 굵게 표시됩니다. 또한 50K 이미지에서 계산된 FID(Frechet Inception Distance)를 인용한다.

![Figure 15](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-15.JPG)

> Figure 15: Example images generated at 256 × 256 from LSUN categories. Sliced Wasserstein Distance (SWD) ×103 is given for levels 256, 128, 64, 32 and 16, and the average is bolded. We also quote the Frechet Inception Distance (FID) computed from 50K images.
>> 그림 15: LSUN 범주에서 256 × 256에서 생성된 이미지 예 SWD × 103은 256, 128, 64, 32, 16 레벨에 대해 주어지며 평균은 굵게 표시됩니다. 또한 50K 이미지에서 계산된 FID(Frechet Inception Distance)를 인용한다.

![Figure 16](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-16.JPG)

> Figure 16: Example images generated at 256 × 256 from LSUN categories. Sliced Wasserstein Distance (SWD) ×103 is given for levels 256, 128, 64, 32 and 16, and the average is bolded. We also quote the Frechet Inception Distance (FID) computed from 50K images. 
>> 그림 16: LSUN 범주에서 256 × 256에서 생성된 이미지 예 SWD × 103은 256, 128, 64, 32, 16 레벨에 대해 주어지며 평균은 굵게 표시됩니다. 또한 50K 이미지에서 계산된 FID(Frechet Inception Distance)를 인용한다.

![Figure 17](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-17.JPG)

> Figure 17: Example images generated at 256 × 256 from LSUN categories. Sliced Wasserstein Distance (SWD) ×103 is given for levels 256, 128, 64, 32 and 16, and the average is bolded. We also quote the Frechet Inception Distance (FID) computed from 50K images.
>> 그림 17: LSUN 범주에서 256 × 256에서 생성된 이미지 예 SWD × 103은 256, 128, 64, 32, 16 레벨에 대해 주어지며 평균은 굵게 표시됩니다. 또한 50K 이미지에서 계산된 FID(Frechet Inception Distance)를 인용한다.

![Figure 18](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)ProGAN/Figure-18.JPG)

> Figure 18: A larger set of generated images corresponding to the non-converged setups in Table 1.
>> 그림 18: 표 1의 비융합 설정에 해당하는 더 큰 이미지 세트