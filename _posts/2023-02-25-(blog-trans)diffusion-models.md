---
layout: post
title: "(Blog-translation)What are Diffusion Models?"
categories: [1. Computer Engineering]
tags: [1.2. Artificial Intelligence, 1.9. Blog Translation]
---

##### [lilianweng.github.io - What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#what-are-diffusion-models)

> So far, I&rsquo;ve written about three types of generative models, <a href="https://lilianweng.github.io/posts/2017-08-20-gan/">GAN</a>, <a href="https://lilianweng.github.io/posts/2018-08-12-vae/">VAE</a>, and <a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">Flow-based</a> models. They have shown great success in generating high-quality samples, but each has some limitations of its own. GAN models are known for potentially unstable training and less diversity in generation due to their adversarial training nature. VAE relies on a surrogate loss. Flow models have to use specialized architectures to construct reversible transform.
>> ì§€ê¸ˆê¹Œì§€, ë‚˜ëŠ” ì„¸ê°€ì§€ generative model(ìƒì„± ëª¨ë¸)ë“¤ì¸, GAN, VAE ê·¸ë¦¬ê³  Flow-based ëª¨ë¸ë“¤ì„ í¬ìŠ¤íŒ… í–ˆë‹¤. ê·¸ê²ƒë“¤ì€ ì„±ê³µì  high-quality ê²°ê³¼ë“¤ì„ ì˜ ìƒì„±í•¨ì„ ë³´ì—¬ì¤€ë‹¤. í•˜ì§€ë§Œ ëª¨ë¸ë“¤ ê°ê°ì—ëŠ” í•œê³„ê°€ ì¡´ì¬í•œë‹¤. GANì€ adversarial í›ˆë ¨ì˜ ì„±ê²©ìƒ ê²°ê³¼ë¥¼ ìƒì„±í•  ë•Œ ê·¼ë³¸ì ìœ¼ë¡œ ë¶ˆì•ˆì „í•œ í›ˆë ¨ê³¼ ì ì€ ë‹¤ì–‘ì„±ìœ¼ë¡œ ì•Œë ¤ì ¸ìˆë‹¤. VAEëŠ” surrogate loss<sup>[1]</sup>ì— ì˜ì¡´í•œë‹¤. Flow-based modelì€ ê°€ì—­ì  ë³€í™˜(reversible transform)<sup>[2]</sup>ì„ êµ¬ì„±í•˜ê¸°ìœ„í•´ íŠ¹ë³„í•œ ì•„í‚¤í…ì³ë¥¼ ì‚¬ìš©í•´ì•¼í•œë‹¤.

1. VAEì˜ surrogate lossë€?
    * VAEì˜ surrogate lossë€, VAEì—ì„œ í•™ìŠµì„ ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ëŒ€ì²´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ë§í•©ë‹ˆë‹¤. VAEì—ì„œëŠ” ìƒì„± ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì§ì ‘ ìµœì í™”í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, ëŒ€ì‹  ìµœì í™” ê°€ëŠ¥í•œ ë‹¤ë¥¸ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ëŒ€ì²´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ surrogate loss functionì´ë¼ê³  í•©ë‹ˆë‹¤. [1]
2. Reversible transform
    * Flow-based ëª¨ë¸ì—ì„œ Reversible transformì€ ì—­ë³€í™˜ ê°€ëŠ¥í•œ ê°€ì—­í•¨ìˆ˜<sup>[4]</sup>ë¡œ, ì…ë ¥ê³¼ ì¶œë ¥ì„ ëª¨ë‘ ì¬ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì—­ë³€í™˜ ê°€ëŠ¥í•œ íŠ¹ì„±ì„ ì´ìš©í•˜ì—¬ ë°ì´í„° ë¶„í¬ë¥¼ í•™ìŠµí•˜ê³  ìƒì„±í•˜ëŠ”ë° í™œìš©ë©ë‹ˆë‹¤. ì´ëŠ” Generative modelì—ì„œë„ ë§ì´ í™œìš©ë˜ëŠ”ë°, ì´ë¥¼ í†µí•´ ì‹¤ì œê°™ì€ ì´ë¯¸ì§€ ìƒì„±ì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤. Reversible transformì€ 1x1 Convolution, NICE(Non-linear Independent Components Estimation), RealNVP(Real-valued Non-Volume Preserving) ë“± ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.[2][3] Flow-based ëª¨ë¸ì—ì„œ Reversible transformì€ NICEë¥¼ ê¸°ë°˜ìœ¼ë¡œí•œ Reversible Architecture ë“± ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ í™œìš©ë˜ë©°, ì´ë¥¼ ì´ìš©í•˜ì—¬ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„± ë° íš¨ìœ¨ì ì¸ ìƒ˜í”Œë§, ë°ì´í„°ì˜ íŠ¹ì„±ì„ ì¡°ì‘í•˜ëŠ” ë“±ì˜ ë‹¤ì–‘í•œ ì‘ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.[4]
3. Invertible function
    * ì—­í•¨ìˆ˜(inverse function)ëŠ” í•œ í•¨ìˆ˜ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì˜ ìˆœì„œë¥¼ ë°”ê¿”ì£¼ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ì¦‰, í•œ í•¨ìˆ˜ê°€ aë¥¼ bë¡œ ë³€í™˜ì‹œí‚¤ë©´, ì—­í•¨ìˆ˜ëŠ” bë¥¼ aë¡œ ë³€í™˜ì‹œí‚µë‹ˆë‹¤. ì´ëŸ¬í•œ ì—­í•¨ìˆ˜ë¥¼ ê°€ì§€ë ¤ë©´ ë‘ ê°€ì§€ ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•œë‹¤ê³  í•©ë‹ˆë‹¤. ì²«ì§¸, ì¼ëŒ€ì¼ ëŒ€ì‘(one-to-one)ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” í•¨ìˆ˜ê°€ ì„œë¡œ ë‹¤ë¥¸ ì…ë ¥ ê°’ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ì¶œë ¥ ê°’ì„ ë‚´ë†“ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë‘˜ì§¸, ì „ì‚¬(surjective)ì—¬ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë“  ì¶œë ¥ ê°’ì´ ì ì–´ë„ í•˜ë‚˜ì˜ ì…ë ¥ ê°’ê³¼ ì—°ê´€ë˜ì–´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. [5] ì˜ˆë¥¼ ë“¤ì–´, y = xÂ²ì™€ ê°™ì€ í•¨ìˆ˜ëŠ” ëª¨ë“  ì¶œë ¥ ê°’ì— ëŒ€í•´ ë‘ ê°œì˜ ì…ë ¥ ê°’ì´ ì—°ê´€ë˜ë¯€ë¡œ ì—­í•¨ìˆ˜ë¥¼ ê°€ì§ˆ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì—­í•¨ìˆ˜ë¥¼ ê°€ì§€ë ¤ë©´ í•¨ìˆ˜ê°€ ì¼ëŒ€ì¼ ëŒ€ì‘ì´ë©° ì „ì‚¬ í•¨ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤. í•¨ìˆ˜ f(x)ì˜ ì—­í•¨ìˆ˜ëŠ” f^-1(x)ë¡œ í‘œê¸°í•©ë‹ˆë‹¤. ì—­í•¨ìˆ˜ì˜ ê³µì‹ì„ ì°¾ëŠ” ë°©ë²•ì€ f(x) = yë¡œ ì •ì˜ëœ í•¨ìˆ˜ì—ì„œ xì— ëŒ€í•´ í’€ë©´ ë©ë‹ˆë‹¤. ì´ ë•Œ, ê²°ê³¼ ì‹ì„ y = xë¡œ ë‘ê³ , xì™€ yë¥¼ ë°”ê¿”ì£¼ë©´ ì—­í•¨ìˆ˜ì˜ ê³µì‹ì´ ë©ë‹ˆë‹¤. [6]

> Diffusion models are inspired by non-equilibrium thermodynamics. They define a Markov chain of diffusion steps to slowly add random noise to data and then learn to reverse the diffusion process to construct desired data samples from the noise. Unlike VAE or flow models, diffusion models are learned with a fixed procedure and the latent variable has high dimensionality (same as the original data).
>> Diffusion modelsì€ ë¹„í‰í˜• ì—´ì—­í•™(non-equilibrium thermodynamics)<sup>[4]</sup>ì—ì„œ ì˜ê°ì„ ë°›ì•˜ë‹¤. ê·¸ë“¤ì€ ë°ì´í„°ì— ëœë¤ ë…¸ì´ì¦ˆë¥¼ ì²œì²œíˆ ì¶”ê°€í•˜ëŠ” Markov chain of diffusion stepsë¥¼ ì •ì˜í•œ í›„ì— ë…¸ì´ì¦ˆë¡œë¶€í„° ì›í•˜ëŠ” ë°ì´í„° ìƒ˜í”Œ(input $x$)ë¥¼ êµ¬ì„±í•˜ê¸° ìœ„í•´ diffusion processë¥¼ ì—­ìˆœìœ¼ë¡œ í•™ìŠµí•œë‹¤. VAEì™€ Flow-based ëª¨ë¸ë“¤ê³¼ ë‹¬ë¦¬, Diffusionëª¨ë¸ì€ fixed procedure(ê³ ì •ëœ ìˆœì„œ)ëŒ€ë¡œ í•™ìŠµë˜ê³  latent vairable(ì ì¬ ë³€ìˆ˜)ëŠ” ê³ ì°¨ì„±(ì›ë³¸ ë°ì´í„°ì™€ ê°™ì€)ì„ ê°€ì§„ë‹¤.

4. Non-equilibrium thermodynamics
    * ë¹„í‰í˜• ì—´ì—­í•™(Non-equilibrium thermodynamics)ì€, ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” ë¹„í‰í˜•(non-equilibrium)<sup>[5]</sup> ìƒíƒœì—ì„œ ì—´ê³¼ ìš´ë™ ì—ë„ˆì§€, ì§ˆëŸ‰ ë“±ì˜ íë¦„ì„ ì—°êµ¬í•˜ëŠ” í•™ë¬¸ ë¶„ì•¼ì…ë‹ˆë‹¤. ì—´ì—­í•™ì€ ì›ë˜ ì—´ê³¼ ì—´ì—­í•™ì  í‰í˜• ìƒíƒœì— ëŒ€í•œ ì—°êµ¬ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ í•™ë¬¸ ë¶„ì•¼ì´ì§€ë§Œ, í˜„ì‹¤ ì„¸ê³„ì—ì„œëŠ” ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” ë¹„í‰í˜• ìƒíƒœê°€ ë§¤ìš° ë§ê¸° ë•Œë¬¸ì—, ë¹„í‰í˜• ì—´ì—­í•™ì˜ ì¤‘ìš”ì„±ì´ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤.
5. Non-equilibrium(ë¹„í‰í˜•)
    * ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” ìƒíƒœë¥¼ ëœ»í•©ë‹ˆë‹¤. ì—´ì—­í•™ì—ì„œ "í‰í˜• ìƒíƒœ(equilibrium state)"ë€, ì—´, ìš´ë™, í™”í•™ ë“±ì˜ ì—ë„ˆì§€ êµí™˜ì´ ë” ì´ìƒ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ” ìƒíƒœë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

![Fig 1](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png)

<figcaption>Fig. 1. Overview of different types of generative models.</figcaption>

# What are Diffusion Models?

> Several diffusion-based generative models have been proposed with similar ideas underneath, including <em>diffusion probabilistic models</em> (**DPM**; <a href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein et al., 2015</a>), <em>noise-conditioned score network</em> (<strong>NCSN</strong>; <a href="https://arxiv.org/abs/1907.05600">Yang &amp; Ermon, 2019</a>), and <em>denoising diffusion probabilistic models</em> (<strong>DDPM</strong>; <a href="https://arxiv.org/abs/2006.11239">Ho et al. 2020</a>).</p>
>> ëª‡ëª‡ì˜ diffusion-based ìƒì„± ëª¨ë¸ë“¤ì€  <em>diffusion probabilistic models</em> (**DPM**; <a href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein et al., 2015</a>), <em>noise-conditioned score network</em> (<strong>NCSN</strong>; <a href="https://arxiv.org/abs/1907.05600">Yang &amp; Ermon, 2019</a>), ê·¸ë¦¬ê³  <em>denoising diffusion probabilistic models</em> (<strong>DDPM</strong>; <a href="https://arxiv.org/abs/2006.11239">Ho et al. 2020</a>)ë“¤ì„ í¬í•¨í•œ ìœ ì‚¬í•œ ì•„ì´ë””ì–´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì œì•ˆë˜ì—ˆë‹¤.

## Forward diffusion process

> Given a data point sampled from a real data distribution $\mathbf{x}_0 \sim q(\mathbf{x})$, let us define a <em>forward diffusion process</em> in which we add small amount of Gaussian noise to the sample in $T$ steps, producing a sequence of noisy samples $\mathbf{x}_1, \dots, \mathbf{x}_T$. The step sizes are controlled by a variance schedule $\{\beta_t \in (0, 1)\}_{t=1}^T$.
>> ì‹¤ì œ dataì˜ ë¶„í¬ $\mathbf{x}_0 \sim q(\mathbf{x})$ë¡œë¶€í„° sampleëœ data pointê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ìš°ë¦¬ëŠ” $T$ stepë“¤ ë§ˆë‹¤ sampleì— ë§¤ìš° ì‘ì€ ì–‘ì˜ Gaussian noiseë¥¼ ì¶”ê°€í•˜ì—¬, noisy(ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ) sample $\mathbf{x}_1, \dots, \mathbf{x}_T$ sequence<sup>[6]</sup>ë¥¼ ìƒì‚°í•˜ëŠ” *forward diffusion process*ë¥¼ ì •ì˜í•œë‹¤.

6. Sequence in deep learning?
    * ë”¥ëŸ¬ë‹ì—ì„œ sequenceëŠ” ì—°ì†ì ì¸ ë°ì´í„° ì§‘í•©ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ ë°ì´í„° ì§‘í•©ì€ ì‹œê°„ì ìœ¼ë¡œ ì—°ì†ë˜ëŠ” ë°ì´í„° ë˜ëŠ” ê³µê°„ì ìœ¼ë¡œ ì—°ì†ë˜ëŠ” ë°ì´í„°ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œëŠ” ë¬¸ì¥ì´ë‚˜ ë‹¨ì–´ë“¤ì˜ ë‚˜ì—´ì„ ì‹œí€€ìŠ¤ ë°ì´í„°ë¡œ ë‹¤ë£¹ë‹ˆë‹¤. ì´ë¯¸ì§€ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œëŠ” ì´ë¯¸ì§€ì˜ í”½ì…€ ê°’ë“¤ì´ ê³µê°„ì ìœ¼ë¡œ ì—°ì†ëœ ë°ì´í„°ë¡œ ë‹¤ë£¹ë‹ˆë‹¤. ì´ì™€ ê°™ì´ ì‹œí€€ìŠ¤ ë°ì´í„°ëŠ” ë”¥ëŸ¬ë‹ì—ì„œ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë‹¤ë£¨ì–´ì§€ë©°, ì£¼ë¡œ ìˆœí™˜ ì‹ ê²½ë§(RNN)ì´ë‚˜ ë³€í™˜ ëª¨ë¸(Transformer) ë“±ì„ ì´ìš©í•˜ì—¬ í•™ìŠµë©ë‹ˆë‹¤.

$$
q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I}) \quad
q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) = \prod^T_{t=1} q(\mathbf{x}_t \vert \mathbf{x}_{t-1})
$$

> The data sample $\mathbf{x}_0$ gradually loses its distinguishable features as the step $t$ becomes larger. Eventually when $T \to \infty$, $\mathbf{x}_T$ is equivalent to an isotropic Gaussian distribution.
>> $t$ ë‹¨ê³„ê°€ ì»¤ì§ì— ë”°ë¼ ë°ì´í„° ìƒ˜í”Œ $\mathbf{x}_0$ëŠ” ì ì°¨ êµ¬ë³„ ê°€ëŠ¥í•œ featureë“¤ì„ ìƒëŠ”ë‹¤. ê²°êµ­ $T \to \infty$ì¼ ë•Œ, $\mathbf{x}_T$ëŠ” isotropic Gaussian distribution(ë“±ë°©ì„± ê°€ìš°ì‹œì•ˆ ë¶„í¬)ì™€ ê°™ë‹¤.

7. isotropic Gaussian distribution(ë“±ë°©ì„± ê°€ìš°ì‹œì•ˆ ë¶„í¬)
    * ì´ì†Œí† ë¡œí”½(isotropic)ì€ 'ë™ì¼í•œ ë°©í–¥(iso)ìœ¼ë¡œ'ë¥¼ ì˜ë¯¸í•˜ë©°, ê°€ìš°ì‹œì•ˆ ë¶„í¬(Gaussian distribution)ëŠ” ì •ê·œë¶„í¬(normal distribution)ì˜ ë‹¤ì°¨ì› ë²„ì „ì¸ ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬(multivariate normal distribution)ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬ëŠ” í•œ ì„ í˜• ê²°í•©(linear combination)ìœ¼ë¡œ ì´ë£¨ì–´ì§„ kê°œì˜ êµ¬ì„±ìš”ì†Œ(component)ê°€ ê°ê° ë‹¨ë³€ëŸ‰ ì •ê·œë¶„í¬(univariate normal distribution)ë¥¼ ë”°ë¥´ë©´, kì°¨ì› ë²¡í„°(random vector)ê°€ kë³€ëŸ‰ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ì •ì˜ë©ë‹ˆë‹¤ [<a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution" target="_new">7</a>]. ê°€ìš°ì‹œì•ˆ í•¨ìˆ˜(Gaussian function)ëŠ” ì´ë™ê³¼ íšŒì „ì— ëŒ€í•´ ë¶ˆë³€ì„±(invariance)ì„ ê°€ì§€ëŠ” ì •ê·œë¶„í¬ë¡œ, í™•ë¥ ë°€ë„í•¨ìˆ˜(probability density function)ê°€ ëª¨ë“  ë°©í–¥ì—ì„œ ë™ì¼í•œ ê²½ìš°ë¥¼ ì´ì†Œí† ë¡œí”½ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¼ê³  í•©ë‹ˆë‹¤ [<a href="https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic" target="_new">1</a>][<a href="https://en.wikipedia.org/wiki/Gaussian_function" target="_new">3</a>]. ë°˜ë©´, ì´ì†Œí† ë¡œí”½ ê°€ìš°ì‹œì•ˆ ë¶„í¬ê°€ ì•„ë‹Œ ê²½ìš°, ê³µë¶„ì‚°í–‰ë ¬(covariance matrix)ì˜ ëŒ€ê°ì„±ë¶„(diagonal elements)ì´ ê°™ì§€ ì•ŠìŠµë‹ˆë‹¤ [<a href="https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic" target="_new">8</a>].

![Fig 2](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/DDPM.png)

<figcaption>Fig. 2. The Markov chain of forward (reverse) diffusion process of generating a sample by slowly adding (removing) noise. (Image source: <a href="https://arxiv.org/abs/2006.11239" target="_blank">Ho et al. 2020</a> with a few additional annotations)</figcaption>

> A nice property of the above process is that we can sample $\mathbf{x}_t$ at any arbitrary time step $t$ in a closed form using <a href="https://lilianweng.github.io/posts/2018-08-12-vae/#reparameterization-trick">reparameterization trick.</a> Let $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$:
>> ìœ„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì¢‹ì€ ë³€ìˆ˜ëŠ” reparameterization trickì„ ì‚¬ìš©í•œ closed formì˜ ì„ì˜ì˜ time step $t$ì—ì„œ $\mathbf{x}_t$ì„ samleí•  ìˆ˜ ìˆë‹¤.

8. Reparameterization trick
    * reparameterization trickì€ Variational Autoencoder(VAE)ì—ì„œ ë§ì´ ì‚¬ìš©ë˜ëŠ” íŠ¸ë¦­ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. VAEëŠ” ìƒì„± ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¡œ, ë°ì´í„°ì˜ ì ì¬ ë³€ìˆ˜(latent variable)ë¥¼ í•™ìŠµí•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ì´ë•Œ, ì ì¬ ë³€ìˆ˜ì˜ ë¶„í¬ë¥¼ í•™ìŠµí•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” íŒŒë¼ë¯¸í„°ëŠ” í‰ê· ê³¼ ë¶„ì‚°ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë²¡í„°ì…ë‹ˆë‹¤. reparameterization trickì€ ì´ ë¶„í¬ì—ì„œ ë¬´ì‘ìœ„ ìƒ˜í”Œë§ì„ í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ë¬´ì‘ìœ„ ìƒ˜í”Œë§ì„ í•˜ë©´ ë¯¸ë¶„ì´ ë¶ˆê°€ëŠ¥í•´ì§€ê¸° ë•Œë¬¸ì—, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ reparameterization trickì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´, ë¯¸ë¶„ ê°€ëŠ¥í•œ í•¨ìˆ˜ë¡œ ì ì¬ ë³€ìˆ˜ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìœ¼ë©°, í•™ìŠµì´ ì•ˆì •ì ìœ¼ë¡œ ì´ë£¨ì–´ì§€ê³  ìƒì„±ëœ ìƒ˜í”Œì˜ í’ˆì§ˆì´ í–¥ìƒë©ë‹ˆë‹¤. [[<a href="https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic" target="_new">9</a>][<a href="https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic" target="_new">10</a>]]

$$
\begin{aligned}
\mathbf{x}_t 
&= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1} & \text{ ;where } \boldsymbol{\epsilon}_{t-1}, \boldsymbol{\epsilon}_{t-2}, \dots \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\
&= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2} & \text{ ;where } \bar{\boldsymbol{\epsilon}}_{t-2} \text{ merges two Gaussians (*).} \\
&= \dots \\
&= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon} \\
q(\mathbf{x}_t \vert \mathbf{x}_0) &= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})
\end{aligned}
$$

##### Reference

[1] "An example of a surrogate loss function could be $psi (h (x)) = max (1 - h (x), 0)$ (the so-called hinge loss in SVM), which is convex and easy to optimize using conventional methods. This function acts as a proxy for the actual loss we wanted to minimize in the first place. Obviously, it has its disadvantages, but in some cases a surrogate ..."
URL: https://stats.stackexchange.com/questions/263712/what-is-a-surrogate-loss-function

[2] "We introduce Glow, a reversible generative model which uses invertible 1x1 convolutions. It extends previous work on reversible generative models and simplifies the architecture. Our model can generate realistic high resolution images, supports efficient sampling, and discovers features that can be used to manipulate attributes of data."
URL: https://openai.com/blog/glow/

[3] "These can also be described as flow-based models, reversible generative models, or as performing nonlinear independent component estimation. ... This transform is where Invertible 1x1 ..."
URL: https://medium.com/ai-ml-at-symantec/introduction-to-reversible-generative-models-4f47e566a73

[4] "Reversible Architectures are a family of neural network architectures that are based on the NICE [12,13] reversible transformation model which are the precursors of the mod-ern day generative flow based image generation architec-tures [29,36]. Based on the NICE invertible transforma-tions, Gomez et al. [22] propose a Reversible ResNet ar-"
URL: https://openaccess.thecvf.com/content/CVPR2022/papers/Mangalam_Reversible_Vision_Transformers_CVPR_2022_paper.pdf

[5] "Formally speaking, there are two conditions that must be satisfied in order for a function to have an inverse. 1) A function must be injective (one-to-one). This means that for all values x and y in the domain of f, f (x) = f (y) only when x = y. So, distinct inputs will produce distinct outputs. 2) A function must be surjective (onto)."
URL: https://www.khanacademy.org/math/precalculus/x9e81a4f98389efdf:composite/x9e81a4f98389efdf:invertible/v/determining-if-a-function-is-invertible

[6] "Learn how to find the formula of the inverse function of a given function. For example, find the inverse of f (x)=3x+2. Inverse functions, in the most general sense, are functions that reverse each other. For example, if f f takes a a to b b, then the inverse, f^ {-1} f âˆ’1, must take b b to a a."
URL: https://www.khanacademy.org/math/algebra-home/alg-functions/alg-finding-inverse-functions/a/finding-inverse-functions

[7] URL: "https://en.wikipedia.org/wiki/Multivariate_normal_distribution"

[8] URL: "https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic"

[9] "The reparameterization trick is a powerful engineering trick. We have seen how it works and why it is useful for the VAE. We also justified its use mathematically and developed a deeper understanding on top of our intuition. Autoencoders, more generally, is an important topic in machine learning."
URL: https://www.baeldung.com/cs/vae-reparameterization

[10] "VAE network with and without the reparameterization trick . where, ğœ™ representations the distribution the network is trying to learn. The epsilon remains as a random variable (sampled from a standard normal distribution) with a very low value thereby not causing the network to shift away too much from the true distribution."
URL: https://towardsdatascience.com/reparameterization-trick-126062cfd3c3