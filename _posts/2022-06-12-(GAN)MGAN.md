---
layout: post 
title: "(GAN)Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks Translation"
categories: [1. Computer Engineering]
tags: [1.7. Literature Review, 1.2.2.5. GAN]
---

### [GAN Literature List](https://maizer2.github.io/1.%20computer%20engineering/2022/05/23/Literature-of-GAN.html)

## <center>$$\mathbf{Precomputed\;Real-Time\;Texture\;Synthesis\;with}$$</center>

## <center>$$\mathbf{Markovian\;Generative\;Adversarial\;Networks}$$</center>

### $$\mathbf{Abstract}$$

> This paper proposes Markovian Generative Adversarial Networks (MGANs), a method for training generative neural networks for efficient texture synthesis. While deep neural network approaches have recently demonstrated remarkable results in terms of synthesis quality, they still come at considerable computational costs (minutes of run-time for low-res images). Our paper addresses this efficiency issue. Instead of a numerical deconvolution in previous work, we precompute a feedforward, strided convolutional network that captures the feature statistics of Markovian patches and is able to directly generate outputs of arbitrary dimensions. Such network can directly decode brown noise to realistic texture, or photos to artistic paintings. With adversarial training, we obtain quality comparable to recent neural texture synthesis methods. As no optimization is required any longer at generation time, our run-time performance (0.25M pixel images at 25Hz) surpasses previous neural texture synthesizers by a significant margin (at least 500 times faster). We apply this idea to texture synthesis, style transfer, and video stylization.
>> 본 논문은 효율적인 텍스처 합성을 위한 생성 신경망을 훈련시키는 방법인 마르코프 생성 적대적 네트워크(MGANs)를 제안한다. 심층 신경망 접근 방식은 최근 합성 품질 측면에서 주목할 만한 결과를 보여 주었지만, 여전히 상당한 계산 비용(낮은 해상도의 이미지의 경우 실행 시간 분)이 든다. 우리의 논문은 이 효율성 문제를 다룬다. 이전 연구에서 수치 디콘볼루션 대신 마르코프 패치의 특징 통계를 캡처하고 임의 차원의 출력을 직접 생성할 수 있는 피드포워드, 스트라이드 컨볼루션 네트워크를 사전 계산한다. 이러한 네트워크는 갈색 소음을 사실적인 질감으로, 또는 사진을 예술적인 그림으로 직접 디코딩할 수 있다. 적대적 훈련을 통해, 우리는 최근의 신경 텍스처 합성 방법과 견줄 만한 품질을 얻는다. 생성 시 더 이상 최적화가 필요하지 않기 때문에, 우리의 런타임 성능(25Hz에서 0.25M 픽셀 이미지)은 이전의 신경 텍스처 합성기를 상당한 차이(최소 500배 더 빠름)로 능가한다. 우리는 이 아이디어를 텍스처 합성, 스타일 전송 및 비디오 스타일화에 적용한다.

> Keywords: Texture synthesis, Adversarial Generative Networks
>> 키워드: 텍스처 합성, 적대적 생성 네트워크

### $\mathbf{1\;Introduction}$

> Image synthesis is a classical problem in computer graphics and vision [6,33]. The key challenges are to capture the structure of complex classes of images in a concise, learnable model, and to find efficient algorithms for learning such models and synthesizing new image data. Most traditional “texture synthesis” methods address the complexity constraints using Markov random field (MRF) models that characterize images by statistics of local patches of pixels. 
>> 이미지 합성은 컴퓨터 그래픽과 비전[6,33]의 고전적인 문제이다. 핵심 과제는 복잡하고 클래스의 이미지 구조를 간결하고 학습 가능한 모델로 포착하고 이러한 모델을 학습하고 새로운 이미지 데이터를 합성하기 위한 효율적인 알고리듬을 찾는 것이다. 대부분의 전통적인 "질감 합성" 방법은 픽셀의 로컬 패치의 통계로 이미지를 특성화하는 마르코프 랜덤 필드(MRF) 모델을 사용하여 복잡도 제약 조건을 해결한다.

> Recently, generative models based on deep neural networks have shown exciting new perspectives for image synthesis [10,8,11]. Deep architectures capture appearance variations in object classes beyond the abilities of pixel-level approaches. However, there are still strong limitations of how much structure can be learned from limited training data. This currently leaves us with two main classes of “deep” generative models: 1) full-image models that generate whole images [10,3], and 2) Markovian models that also synthesize textures [8,21].
>> 최근, 심층 신경망을 기반으로 한 생성 모델은 이미지 합성에 대한 흥미로운 새로운 관점을 보여주었다[10, 8,11]. 딥 아키텍처는 픽셀 수준 접근 방식의 능력을 넘어 객체 클래스의 외관 변화를 포착한다. 그러나 제한된 훈련 데이터에서 얼마나 많은 구조를 배울 수 있는지에 대해서는 여전히 강력한 한계가 있다. 이것은 현재 우리에게 두 가지 주요 "딥" 생성 모델, 즉 1) 전체 이미지[10,3]를 생성하는 전체 이미지 모델과 2) 텍스처를 합성하는 마르코프 모델[8,21]을 남긴다.

> The first class, full-image models, are often designed as specially trained auto-encoders [16,11]. Results are impressive but limited to rather small images (typically around 64×64 pixels) with limited fidelity in details. The second class, the deep Markovian models, capture the statistics of local patches only and assemble them to high-resolution images. Consequently, the fidelity of details is good, but additional guidance is required if non-trivial global structure should be reproduced [6,12,1,8,21]. Our paper addresses this second approach of deep Markovian texture synthesis.
>> 첫 번째 클래스 전체 이미지 모델은 종종 특수하게 훈련된 자동 인코더로 설계된다[16,11]. 결과는 인상적이지만 세부 사항의 충실도가 제한된 다소 작은 이미지(일반적으로 약 64x64 픽셀)로 제한된다. 두 번째 클래스인 심층 마르코프 모델은 로컬 패치의 통계만 캡처하고 고해상도 이미지로 조립한다. 결과적으로, 세부 사항의 충실도는 양호하지만, 사소하지 않은 전역 구조가 재현되어야 하는 경우 추가 지침이 필요하다[6,12,1,8,21]. 우리의 논문은 심층 마르코프 텍스처 합성이라는 두 번째 접근 방식을 다룬다.

> Previous neural methods of this type [8,21] are built upon a deconvolutional framework [37,25]. This naturally provides blending of patches and permits reusing the intricate, emergent multi-level feature representations of large, discriminatively trained neural networks like the VGG network [30], repurposing them for image synthesis. As a side note, we will later observe that this is actually crucial for high-quality result (Figure 10). Gatys et al. [8] pioneer this approach by modeling patch statistics with a global Gaussian models of the higher-level feature vectors, and Li et al. [21] utilize dictionaries of extended local patches of neural activation, trading-off flexibility for visual realism.
>> 이러한 유형의 이전 신경 방법[8,21]은 디콘볼루션 프레임워크[37,25]를 기반으로 한다. 이는 자연스럽게 패치를 혼합하고 VGG 네트워크[30]와 같은 대규모 차별적으로 훈련된 신경망의 복잡하고 새로운 다단계 기능 표현을 재사용하여 이미지 합성을 위해 용도를 변경한다. 참고로, 우리는 나중에 이것이 고품질 결과에 실제로 중요하다는 것을 관찰할 것이다(그림 10). 게이티 외 [8] 상위 레벨 피처 벡터의 전역 가우스 모델로 패치 통계를 모델링하여 이 접근 방식을 개척한다. [21] 신경 활성화의 확장된 로컬 패치 사전, 시각적 사실성을 위한 트레이드오프 유연성을 활용한다.

> Deep Markovian models are able to produce remarkable visual results, far beyond traditional pixel-level MRF methods. Unfortunately, the run-time costs of the deconvolution approach are still very high, requiring iterative back-propagation in order to estimate a pre-image (pixels) of the feature activations (higher network layer). In the most expensive case of modeling MRFs of higher-level feature patches [21], a high-end GPU needs several minutes to synthesize low-resolution images (such as a 512-by-512 pixels image). 
>> 심층 마르코프 모델은 기존의 픽셀 수준 MRF 방법을 훨씬 뛰어넘는 놀라운 시각적 결과를 생성할 수 있다. 불행히도 디콘볼루션 접근 방식의 런타임 비용은 여전히 매우 높아서 기능 활성화(더 높은 네트워크 계층)의 사전 이미지(픽셀)를 추정하기 위해 반복적인 역 전파가 필요하다. 고급 기능 패치[21]의 MRF를 모델링하는 가장 비용이 많이 드는 경우에서, 고급 GPU는 저해상도 이미지(512x512 픽셀 이미지)를 합성하는 데 몇 분이 필요하다.

> The objective of our paper is therefore to improve the efficiency of deep Markovian texture synthesis. The key idea is to precompute the inversion of the network by fitting a strided1 convolutional network [31,29] to the inversion process, which operates purely in a feed-forward fashion. Despite being trained on patches of a fixed size, the resulting network can generate continuous images of arbitrary dimension without any additional optimization or blending, yielding a high-quality texture synthesizer of a specific style and high performance2.
>> 따라서 본 논문의 목표는 심층 마르코프 텍스처 합성의 효율성을 향상시키는 것이다. 핵심 아이디어는 순수하게 피드포워드 방식으로 작동하는 반전 프로세스에 stried1 컨볼루션 네트워크[31,29]를 적합시켜 네트워크의 반전을 사전 계산하는 것이다. 고정된 크기의 패치에 대해 훈련되었음에도 불구하고, 결과 네트워크는 추가 최적화나 혼합 없이 임의의 차원의 연속적인 이미지를 생성할 수 있어 특정 스타일의 고품질 텍스처 합성기와 고성능2를 산출한다.

> We train the convolutional network using adversarial training [29], which permits maintaining image quality similar to the original, expensive optimization approach. As result, we obtain significant speed-up: Our GPU implementation computes 512×512 images within 40ms (on an nVidia TitanX). The key limitation, of course, is to precompute the feed-forward convolutional network for each texture style. Nonetheless, this is still an attractive trade-off for many potential applications, for example from the area of artistic image or video stylization. We explore some of these applications in our experiments.
>> 우리는 원래 고가의 최적화 접근법과 유사한 이미지 품질을 유지할 수 있는 적대적 훈련[29]을 사용하여 컨볼루션 네트워크를 훈련시킨다. 그 결과 상당한 속도 향상을 얻을 수 있었습니다. 우리의 GPU 구현은 (nVidia TitanX에서) 40ms 내에 512×512 이미지를 계산한다. 물론 핵심 한계는 각 텍스처 스타일에 대한 피드포워드 컨볼루션 네트워크를 미리 계산하는 것이다. 그럼에도 불구하고, 이는 예술적 이미지 또는 비디오 스타일화 영역과 같은 많은 잠재적 응용 분야에서 여전히 매력적인 트레이드오프이다. 우리는 실험에서 이러한 응용 프로그램 중 일부를 탐색한다.

### $\mathbf{2\;Related\;Work}$

> Deconvolutional neural networks have been introduced to visualize deep features and object classes. Zeiler et al. [37] back-project neural activations to pixels. Mahendran et al. [23] reconstruct images from the neural encoding in intermediate layers. Recently, effort are made to improve the efficiency and accuracy of deep visualization [36,26]. Mordvintsev et al. have raised wide attention by showing how deconvolution of class-specifc activations can create hallucinogenic imagery from discriminative networks [25]. The astonishing complexity of the obtained visual patterns has immediately spurred hope for new generative models: Gatys et al. [8,7] drove deconvolution by global  covariance statistics of feature vectors on higher network layers, obtaining unprecedented results in artistic style transfer. The statistical model has some limitations: Enforcing per-feature-vector statistics permits a mixing of feature patterns that never appear in actual images and limit plausibility of the learned texture. This can be partially addressed by replacing point-wise feature statistics by statistics of spatial patches of feature activations [21]. This permits photo-realistic synthesis in some cases, but also reduces invariance because the simplistic dictionary of patches introduces rigidity. On the theory side, Xie et al. [34] have proved that a generative random field model can be derived from used discriminative networks, and show applications to unguided texture synthesis.
>> 심층 특징과 객체 클래스를 시각화하기 위해 디콘볼루션 신경망이 도입되었다. Zeiler 외 [37] 신경 활성화를 픽셀로 역프로젝트합니다. 마헨드란 외 [23] 중간 레이어에서 신경 인코딩으로부터 이미지를 재구성한다. 최근, 심층 시각화의 효율성과 정확도를 개선하기 위한 노력이 이루어지고 있다[36,26]. 모르드빈트세브 외 클래스별 활성화의 디콘볼루션(deconvolution)이 차별적 네트워크에서 환각 이미지를 생성하는 방법을 보여줌으로써 광범위한 관심을 끌었다[25]. 얻어진 시각적 패턴의 놀라운 복잡성은 즉시 새로운 생성 모델에 대한 희망을 불러일으켰다: 게이티스 외. [8,7]은 더 높은 네트워크 계층에서 특징 벡터의 전역 공분산 통계에 의해 디콘볼루션(deconvolution)을 유도하여 예술적 스타일 전송에서 전례 없는 결과를 얻었다. 통계 모델에는 몇 가지 한계가 있다. 기능별 벡터 통계를 적용하면 실제 이미지에 나타나지 않는 기능 패턴을 혼합할 수 있고 학습된 텍스처의 신뢰성을 제한할 수 있다. 이는 점별 형상 통계를 형상 활성화 공간 패치의 통계로 대체함으로써 부분적으로 해결할 수 있다[21]. 이는 경우에 따라 사진 사실적인 합성을 허용하지만 간단한 패치 사전이 강성을 도입하기 때문에 불변성을 감소시킨다. 이론 측면에서, Xie 등[34]은 사용된 차별적 네트워크에서 생성 무작위 필드 모델이 도출될 수 있다는 것을 증명했으며, 안내되지 않은 텍스처 합성에 대한 응용 프로그램을 보여준다.

> Full image methods employ specially trained auto-encoders as generative networks [16]. For example, the Generative Adversarial Networks (GANs) use two networks, one as the discriminator and other as the generator, to iteratively improve the model by playing a minimax game [10]. This model is extended to work with a Laplacian pyramid [9], and with a conditional setting [3]. Very recently, Radford et al. [29] propose a set of architectural refinements3 that stabilized the performance of this model, and show that the generators have vector arithmetic properties. One important strength of adversarial networks is that it offers perceptual metrics [20,4] that allows auto-encoders to be training more efficiently. These models can also be augmented semantic attributes [35], image captions [24], 3D data [5,17], spatial/temporal status [11,13,27] etc.
>> 전체 이미지 방법은 특별히 훈련된 자동 인코더를 생성 네트워크로 사용한다[16]. 예를 들어, GAN(Generative Adversarial Networks)은 미니맥스 게임을 하여 모델을 반복적으로 개선하기 위해 판별기로서 그리고 생성기로서 두 개의 네트워크를 사용한다[10]. 이 모델은 라플라시안 피라미드[9]와 조건부 설정[3]으로 작동하도록 확장되었다. 아주 최근에, 래드포드 외 [29] 이 모델의 성능을 안정화시키는 일련의 구조적 개선 3을 제안하고, 생성자가 벡터 산술 특성을 가지고 있음을 보여준다. 적대적 네트워크의 한 가지 중요한 강점은 자동 인코더가 더 효율적으로 훈련할 수 있는 지각 메트릭[20,4]을 제공한다는 것이다. 이러한 모델은 또한 증강 의미 속성 [35], 이미지 캡션 [24], 3D 데이터 [5,17], 공간/시간 상태 [11,13,27] 등이 될 수 있다.

> In very recent, two concurrent work, Ulyanov et al. [32] and Johnson et al. [14] propose fast implementations of Gatys et al’s approach. Both of their methods employ precomputed decoders trained with a perceptual texture loss and obtain significant run-time benefits (higher decoder complexity reduces their speed-up a bit). The main conceptual difference in our paper is the use of Li et al.’s [21] feature-patch statistics as opposed to learning Gaussian distributions of individual feature vectors, which provides some benefits in terms of reproducing textures more faithfully.
>> 매우 최근에, Ulyanov et al. 두 개의 동시 연구가 있었다. [32]와 Johnson 등[14]은 Gatys 등의 접근 방식의 빠른 구현을 제안한다. 두 가지 방법 모두 지각 텍스처 손실로 훈련된 사전 계산된 디코더를 사용하고 상당한 런타임 이점을 얻는다(디코더 복잡성이 높을수록 속도가 약간 감소한다). 본 논문에서 주요 개념적 차이는 개별 피처 벡터의 가우스 분포를 학습하는 것과 반대로 Li 등의 [21] 피처 패치 통계를 사용하는 것으로 텍스처를 보다 충실하게 재생하는 측면에서 몇 가지 이점을 제공한다.

![Figure 1](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)MGAN/Figure-1.JPG)

> Fig. 1: Motivation: real world data does not always comply with a Gaussian distribution (first), but a complex nonlinear manifold (second). We adversarially learn a mapping to project contextually related patches to that manifold.
>> 그림 1: 동기: 실제 데이터는 항상 가우스 분포(첫 번째)를 따르는 것이 아니라 복잡한 비선형 매니폴드(두 번째)를 따른다. 우리는 해당 매니폴드에 상황별 관련 패치를 투영하기 위한 매핑을 적대적으로 학습한다.

### $\mathbf{3\;Model}$

> Let us first conceptually motive our method. Statistics based methods [8,32] match the distributions of source (input photo or noise signal) and target (texture) with a Gaussian model (Figure 1, first). They do not further improve the result once two distributions match. However, real world data does not always comply with a Gaussian distribution. For example, it can follow a complicated non-linear manifold. Adversarial training [10] can recognize such manifold with its discriminative network (Figure 1, second), and strengthen its generative power with a projection on the manifold (Figure 1, third). We improve adversarial training with contextually corresponding Markovian patches (Figure 1, fourth). This allows the learning to focus on the mapping between different depictions of the same context, rather than the mixture of context and depictions.
>> 먼저 우리의 방법을 개념적으로 동기부여해 보자. 통계 기반 방법 [8,32]은 소스(입력 사진 또는 노이즈 신호)와 대상( 텍스처)의 분포를 가우스 모델과 일치시킨다(그림 1, 첫 번째). 두 분포가 일치하면 결과가 더 이상 개선되지 않습니다. 그러나 실제 데이터가 항상 가우스 분포를 따르는 것은 아니다. 예를 들어, 복잡한 비선형 다양체를 따를 수 있다. 적대적 훈련[10]은 차별적 네트워크(그림 1, 2차)로 이러한 매니폴드를 인식하고(그림 1, 3차) 매니폴드에 투영하여 생성력을 강화할 수 있다. 우리는 문맥적으로 대응하는 마르코프 패치를 통해 적대적 훈련을 개선한다(그림 1, 4). 이를 통해 학습은 문맥과 묘사의 혼합보다는 동일한 맥락의 다른 묘사 간의 매핑에 집중할 수 있다.

![Figure 2](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)MGAN/Figure-2.JPG)

> Fig. 2: Our model contains a generative network (blue blocks) and a discriminative network (green blocks). We apply the discriminative training on Markovian neural patches (purple block as the input of the discriminative network.).
>> 그림 2: 우리의 모델은 생성 네트워크(파란색 블록)와 차별적 네트워크(녹색 블록)를 포함한다. 마르코프 신경 패치(차별 네트워크의 입력으로 보라색 블록)에 차별적 훈련을 적용한다.

> Figure 2 visualizes our pipeline, which extends the patch-based synthesis algorithm of Li et al. [21]. We first replace their patch dictionary (including the iterative nearest-neighbor search) with a continuous discriminative network $D$ (green blocks) that learns to distinguish actual feature patches (on VGG 19 layer Relu3 1, purple block) from inappropriately synthesized ones. A second comparison (pipeline below D) with a VGG 19 encoding of the same image on the higher, more abstract layer Relu5 1 can be optionally used for guidance. If we run deconvolution on the VGG networks (from the discriminator and optionally from the guidance content), we obtain deconvolutional image synthesizer, which we call Markovian Deconvolutional Adversarial Networks (MDANs).
>> 그림 2는 Li 등의 패치 기반 합성 알고리듬을 확장하는 우리의 파이프라인을 시각화한다. [21. 우리는 먼저 그들의 패치 사전(반복적인 가장 가까운 이웃 검색 포함)을 (VGG 19층 Relu31, 보라색 블록의) 실제 기능 패치를 부적절하게 합성된 것과 구별하는 방법을 배우는 연속적인 차별적 네트워크 $D$(녹색 블록)로 대체한다. 더 높고 추상적인 레이어 Relu51에서 동일한 이미지의 VGG 19 인코딩과 두 번째 비교(D 아래의 파이프라인)는 선택적으로 지침으로 사용될 수 있다. VGG 네트워크에서 디콘볼루션(deconvolution)을 실행하면(판별기에서 그리고 선택적으로 안내 콘텐츠에서), 우리는 디콘볼루션 이미지 합성기를 얻는데, 이를 마르코프 디콘볼루션 적대적 네트워크(MDANs)라고 한다.

> MDANs are still very slow; therefore, we aim for an additional generative network $G$ (blue blocks; a strided convolutional network). It takes a VGG 19 layer Relu4 1 encoding of an image and directly decodes it to pixels of the synthesis image. During all of the training we do not change the VGG 19 network (gray blocks), and only optimize $D$ and $G$. Importantly, both $D$ and $G$ are trained simultaneously to maximize the quality of G; $D$ acts here as adversary to $G$. We denote the overall architecture by Markovian Generative Adversarial Networks (MGANs).
>> MDAN은 여전히 매우 느리기 때문에 추가 생성 네트워크 $G$(블루 블록; 스트라이드 컨볼루션 네트워크)를 목표로 한다. 이미지의 VGG 19 레이어 Relu41 인코딩을 사용하여 합성 이미지의 픽셀로 직접 디코딩합니다. 모든 훈련 동안 VGG 19 네트워크(회색 블록)를 변경하지 않고 $D$와 $G$만 최적화한다. 중요한 것은 $D$와 $G$가 모두 G의 품질을 최대화하기 위해 동시에 훈련된다는 것이다. $D$는 여기서 $G$에 대항하는 역할을 한다. 우리는 마르코프 생성 적대적 네트워크(MGAN)에 의한 전체 아키텍처를 나타낸다.

#### $\mathbf{3.1\;Markovian\;Deconvolutional\;Adversarial\;Networks\;(MDANs)}$

> Our MDANs synthesize textures with a deconvolutional process that is driven by adversarial training: a discriminative network $D$ (green blocks in Figure 2) is trained to distinguish between “neural patches” sampled from the synthesis image and sampled from the example image. We use regular sampling on layer relu3 1 of VGG 19 output (purple block). It outputs a classification score $s = ±1$ for each neural patch, indicating how “real” the patch is (with $s = 1$ being real). For each patch sampled from the synthesized image, $1 − s$ is its texture loss to minimize. The deconvolution process back-propagates this loss to pixels. Like Radford et al. [29] we use batch normalization (BN) and leaky ReLU (LReLU) to improve the training of $D$.
>> 우리의 MDAN은 적대적 훈련에 의해 구동되는 디콘볼루션 프로세스로 질감을 합성한다. 차별적 네트워크 $D$(그림 2의 녹색 블록)는 합성 이미지에서 샘플링되고 예제 이미지에서 샘플링된 "신경 패치"를 구별하도록 훈련된다. 우리는 VGG 19 출력(보라색 블록)의 레이어 relu31에 대해 정기적인 샘플링을 사용한다. 이것은 각 신경 패치에 대한 분류 점수 $s = ±1$을 출력하여 패치가 얼마나 "실제"인지를 나타낸다($s = 1$가 실제임). 합성 이미지에서 샘플링된 각 패치의 경우 $1 - s$는 최소화할 텍스처 손실이다. 디콘볼루션 프로세스는 이 손실을 픽셀로 역전파한다. 래드포드 등. [29] 우리는 배치 정규화(BN)와 누출 ReLU(LReLU)를 사용하여 $D$의 훈련을 개선한다.

> Formally, we denote the example texture image by $x_{t}\in{R^{w_{t}\times{h_{t}}}}$, and the synthesized image by $x\in{R^{w\times{h}}}$. We initialize $x$ with random noise for un-guided synthesis, or an content image $x_{c}\in{R}$ w×h for guided synthesis. The deconvolutio iteratively updates $x$ so the following energy is minimized:
>> 형식적으로는 샘플 텍스처 이미지를 $x_{t}\in{R^{w_{t}\times{h_{t}}}}$로, 합성 이미지를 $x\in{R^{w\times{h}}}$로 나타낸다. 우리는 안내되지 않은 합성을 위해 무작위 노이즈를 사용하여 $x$를 초기화하거나 안내된 합성을 위해 콘텐츠 이미지 $x_{c}\in{R}$ w×h를 초기화한다. 디콘볼루션에서 $x$를 반복적으로 업데이트하여 다음과 같은 에너지를 최소화한다.

$$x=\underset{x}{\arg\min}E_{t}(\phi{(x)},\phi{(x_{t})})+\alpha_{1}E_{c}(\phi{(x)},\phi{(x_{c})})+\alpha_{2}Y(x)$$
 
> Here $E_{t}$ denotes the texture loss, in which $\phi{(x)}$ is $x$’s feature map output from layer relu3 1 of VGG 19. We sample patches from $\phi{(x)}$, and compute $E_{t}$ as the Hinge loss with their labels fixed to one:
>> 여기서 $E_{t}$는 텍스처 손실을 나타내며, 여기서 $\phi{(x)}$는 VGG 19의 레이어 relu31에서 $x$의 피처 맵 출력이다. 우리는 $\phi{(x)}$에서 패치를 샘플링하고 $E_{t}$를 힌지 손실로 계산하고 레이블은 다음 중 하나에 고정한다.

$$E_{t}(\phi{(x)},\phi{(x_{t})}=\frac{1}{N}\sum_{i=1}^{N}\max{(0,1-1\times{s_{i}})}$$

> Here $s_{i}$ denotes the classification score of $i$-th neural patch, and $N$ is the total number of sampled patches in $\phi{(x)}$. The discriminative network is trained on the fly: Its parameters are randomly initialized, and then updated after each deconvolution, so it becomes increasingly smarter as synthesis results improve.
>> 여기서 $s_{i}$는 $i$-th 신경 패치의 분류 점수를 나타내며, $N$은 $\phi{(x)}$에서 샘플링된 패치의 총 수입니다. 차별적 네트워크는 즉시 학습됩니다. 그것의 매개 변수는 무작위로 초기화되었다가 각 디콘볼루션 후에 업데이트되므로 합성 결과가 향상될수록 점점 더 똑똑해진다.

> The additional regularizer $Y(x)$ in $E_{q}$. 1 is a smoothness prior for pixels [23]. Using $E_{t}$ and $Y(x)$ can synthesize random textures (Figure 3). By minimizing an additional content loss $E_{c}$, the network can generate an image that is contextually related to a guidance image $x_{c}$(Figure 4). This content loss is the Mean Squared Error between two feature maps $\phi{(x)}$ and $\phi{(x_{c})}$. We set the weights with $α_{1} = 1$ and $α_{2} = 0.0001$, and minimize Equation 1 using back-propagation with ADAM [15] (learning rate 0.02, momentum 0.5). Notice each neural patch receives its own output gradient through the back-propagation of $D$. In order to have a coherent transition between adjacent patches, we blend their output gradient like texture optimization [18] did.
>> $E_{q}$의 추가 정규화기 $Y(x)$는 픽셀에 대한 이전 평활도이다 [23]. $E_{t}$와 $Y(x)$를 사용하면 무작위 텍스처를 합성할 수 있다(그림 3). 추가 콘텐츠 손실 $E_{c}$를 최소화함으로써 네트워크는 안내 이미지 $x_{c}$(그림 4)와 문맥적으로 관련된 이미지를 생성할 수 있다. 이 콘텐츠 손실은 두 피쳐 맵 $\phi{(x)}$와 $\phi{(x_{c})}$ 사이의 평균 제곱 오차이다. 우리는 $α_{1} = 1$와 $α_{2} = 0.0001$로 가중치를 설정하고 ADAM [15]로 역 전파를 사용하여 방정식 1을 최소화한다(학습률 0.02, 운동량 0.5). 각 신경 패치는 $D$의 역 전파를 통해 자체 출력 기울기를 수신한다는 점에 주목하십시오. 인접 패치 간의 일관된 전환을 위해 텍스처 최적화[18]가 그랬던 것처럼 출력 기울기를 혼합한다.

![Figure 3](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)MGAN/Figure-3.JPG)

> Fig. 3: Un-guided texture synthesis using MDANs. For each case the first image is the example texture, and the other two are the synthesis results. Image credits: [34]’s “Ivy”, flickr user erwin brevis’s “gell”, Katsushika Hokusai’s “The Great Wave off Kanagawa”, Kandinsky’s “Composition VII”.
>> 그림 3: MDAN을 이용한 무유도 텍스처 합성 각 경우에 대해 첫 번째 이미지는 샘플 텍스처이고, 나머지 두 이미지는 합성 결과입니다. 이미지 크레딧: [34]의 "Ivy", 플리커 유저 어윈 브레비스의 "젤", 카츠시카 호쿠사이 감독의 "가나가와 앞바다의 대파", 칸딘스키의 "Composition VII".

![Figure 4](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)MGAN/Figure-4.JPG)

> Fig. 4: Guided texture synthesis using MDANs. The reference textures are the same as in Figure 3.
>> 그림 4: MDAN을 이용한 유도 텍스처 합성 참조 텍스처는 그림 3과 동일합니다.

#### $\mathbf{3.2\;Markovian\;Generative\;Adversarial\;Networks\;(MGANs)}$

> MDANs require many iterations and a separate run for each output image. We now train a variational auto-encoder (VAE) that decodes a feature map directly to pixels. The target examples (textured photos) are obtained from the MDANs. Our generator $G$ (blue blocks in Figure 2) takes the layer relu4 1 of VGG 19 as the input, and decodes a picture through a ordinary convolution followed by a cascade of fractional-strided convolutions (FS Conv). Although being trained with fixed size input, the generator naturally extends to arbitrary size images.
>> MDAN은 각 출력 이미지에 대해 많은 반복과 별도의 실행이 필요합니다. 우리는 이제 특징 맵을 픽셀로 직접 디코딩하는 변형 자동 인코더(VAE)를 훈련한다. 대상 예(질감 있는 사진)는 MDAN에서 얻는다. 우리의 생성기 $G$(그림 2의 파란색 블록)는 VGG 19의 레이어 relu41을 입력으로 취하고, 일반적인 컨볼루션과 이어지는 일련의 분수 스트라이드 컨볼루션(FS Conv)을 통해 사진을 디코딩한다. 고정 크기 입력으로 훈련되지만 생성기는 자연스럽게 임의의 크기 이미지로 확장된다.

> As Dosovitskiy $E_{t}$ al. [4] point out, it is crucially important to find a good metric for training an auto-encoder: Using the Euclidean distance between the synthesized image and the target image at the pixel level (Figure 5, pixel VAE) yields an over-smoothed image. Comparing at the neural encoding level improves results (Figure 5, neural VAE), and adversarial training improves the reproduction of the intended style further (Figure 5, MGANs).
>> As Dosovitsky $E_{t}$ al. [4] 자동 검사 교육을 위한 좋은 측정 기준을 찾는 것이 매우 중요합니다. 픽셀 레벨(그림 5, 픽셀 VAE)에서 합성 이미지와 대상 이미지 사이의 유클리드 거리를 사용하면 지나치게 매끄러운 이미지가 생성됩니다. 신경 인코딩 수준에서 비교하면 결과가 개선되고(그림 5, 신경 VAE), 적대적 훈련은 의도한 스타일의 재생산을 더욱 향상시킨다(그림 5, MGANs).

> Our approach is similar to classical Generative Adversarial Networks (GANs) [10], with the key difference of not operating on full images, but neural patches from the same image. Doing so utilizes the contextual correspondence between the patches, and makes learning easier and more effective in contrast to learning the distribution of a object class [10] or a mapping between contextually irrelevant data [32]. In additional we also replace the Sigmoid function and the binary cross entropy criteria from [29] by a max margin criteria (Hinge loss). This avoids the vanishing gradient problem when learning $D$. This is more problematic in our case than in Radfort et al.’s [29] because of less diversity in our training data. Thus, the Sigmoid function can be easily saturated.
>> 우리의 접근 방식은 고전적인 GAN(Generative Adversarial Networks)[10]과 유사하며, 주요 차이점은 전체 이미지에서 작동하는 것이 아니라 동일한 이미지의 신경 패치이다. 그렇게 하면 패치 간의 상황별 대응을 활용하고, 객체 클래스[10]의 분포나 상황별로 관련이 없는 데이터 간의 매핑을 학습하는 것과 대조적으로 학습을 더 쉽고 효과적으로 만든다[32]. 또한 시그모이드 함수와 [29]의 이진 교차 엔트로피 기준을 최대 여유 기준(힌지 손실)으로 대체한다. 이는 $D$를 학습할 때 사라지는 그레이디언트 문제를 방지한다. 훈련 데이터의 다양성이 낮기 때문에 이것은 Radfort 등의 [29]에서보다 우리의 경우에 더 문제가 된다. 따라서 시그모이드 함수는 쉽게 포화될 수 있다.

![Figure 5](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)MGAN/Figure-5.JPG)

> Figure 5 (MGANs) shows the results of a network that is trained to produce paintings in the style of Picasso’s “Self-portrait 1907”. For training, we randomly selected 75 faces photos from the CelebA data set [22], and in additional to it 25 non-celebrity photos from the public domain. We resize all photos so that the maximum dimension is 384 pixels. We augmented the training data by generating 9 copies of each photo with different rotations and scales. We regularly sample subwindows of 128-by-128 croppings from them for batch processing. In total we have 24,506 training examples, each is treated as a training image where neural patches are sampled from its relu3 1 encoding as the input of $D$. 
>> 그림 5(MGANs)는 피카소의 "자화상 1907" 스타일로 그림을 제작하도록 훈련된 네트워크의 결과를 보여준다. 교육을 위해 CelebA 데이터 세트[22]에서 75장의 얼굴 사진을 무작위로 선택했으며, 퍼블릭 도메인에서 25장의 유명인이 아닌 사진을 추가로 선택했다. 우리는 최대 치수가 384픽셀이 되도록 모든 사진의 크기를 조정합니다. 우리는 회전과 척도가 다른 각 사진의 9개의 복사본을 생성하여 훈련 데이터를 강화했다. 배치 처리를 위해 128 x 128 크롭의 하위 창을 정기적으로 샘플링한다. 총 24,506개의 훈련 예가 있으며, 각각은 $D$의 입력으로 relu31 인코딩에서 신경 패치를 샘플링하는 훈련 이미지로 처리된다.

> Figure 5 (top row, MGANs) shows the decoding result of our generative network for a training photo. The bottom row shows the network generalizes well to test data. Notice the MDANs image for the test image is never used in the training. Nonetheless, direct decoding with $G$ produces very good approximation of it. The main difference between MDANs and MGANs is: MDANs preserve the content of the input photo better and MGANs produce results that are more stylized. This is because MGANs was trained with many images, hence learned the most frequent features. Another noticeable difference is MDANs create more natural backgrounds (such as regions with flat color), due to its iterative refinement. Despite such flaws, the MGANs model produces comparable results with a speed that is 25,000 times faster. 
>> 그림 5(맨 위 행, MGANs)는 훈련 사진에 대한 생성 네트워크의 디코딩 결과를 보여준다. 맨 아래 행은 네트워크가 데이터를 테스트하기 위해 잘 일반화되었음을 나타냅니다. 테스트 이미지에 대한 MDAN 이미지는 교육에서 사용되지 않습니다. 그럼에도 불구하고 $G$를 사용한 직접 디코딩은 매우 좋은 근사치를 생성한다. MDAN과 MGAN의 주요 차이점은 MDAN은 입력 사진의 내용을 더 잘 보존하고 MGAN은 보다 양식화된 결과를 생성한다는 것이다. 이는 MGAN이 많은 이미지로 훈련되었기 때문에 가장 빈번한 기능을 학습했기 때문이다. 또 다른 눈에 띄는 차이점은 MDAN은 반복적인 정교함으로 인해 더 자연스러운 배경(예: 평평한 색상의 영역)을 생성한다는 것이다. 이러한 결함에도 불구하고, MGANs 모델은 25,000배 더 빠른 속도로 비슷한 결과를 산출한다.

![Figure 6](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)MGAN/Figure-6.JPG)

> Fig. 6: Intermediate decoding results during the training of MGANs. The reference style texture for MDANs is Pablo Picasso’s “self portrait 1907”.
>> 그림 6: MGAN 훈련 중 중간 디코딩 결과. MDAN을 위한 참조 스타일 질감은 파블로 피카소의 "자기 초상화 1907"이다.

![Figure 7](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)MGAN/Figure-7.JPG)

> Fig. 7: Visualizing the learned features in the generative networks. Image credits: [34]’s “Ivy”, flickr user erwin brevis’s “gell”, Katsushika Hokusai’s “The Great Wave off Kanagawa”.
>> 그림 7: 생성 네트워크에서 학습된 기능 시각화. 이미지 크레딧: [34]의 "아이비", 플리커 유저 에러윈 브레비스의 "젤", 호쿠사이 카츠시카의 "가나가와 앞바다의 대파".

> Figure 6 shows some intermediate results MGANs. It is clear that the decoder gets better with more training. After 100 batches, the network is able to learn the overall color, and where the regions of strong contrast are. After 300 batches the network started to produce textures for brush strokes. After 1000 batches it learns how to paint eyes. Further training is able to remove some of the ghosting artifacts in the results. Notice the model generalizes well to testing data (right).
>> 그림 6은 몇 가지 중간 결과 MGAN을 보여준다. 더 많은 훈련을 받으면 디코더가 좋아지는 것은 분명하다. 100개의 배치 후에 네트워크는 전체적인 색상과 강한 대비의 영역이 어디에 있는지 학습할 수 있다. 300배치 후에 네트워크는 브러시 스트로크를 위한 텍스처를 생산하기 시작했다. 1000번의 배치 후에 그것은 눈을 그리는 방법을 배운다. 추가 교육을 통해 결과에서 일부 고스팅 아티팩트를 제거할 수 있습니다. 모형은 검정 데이터에 잘 일반화됩니다(오른쪽).

### $\mathbf{4\;Experimental\;Analysis}$

> We conduct empirical experiments with our model: we study parameter influence (layers for classification, patch size) and the complexity of the model (number of layers in the network, number of channels in each layer). While there may not be a universal optimal design for all textures, our study shed some light on how the model behaves for different cases. For fair comparison, we scale the example textures in this study to fixed size (128-by-128 pixels), and demand the synthesis output to be 256-by-256 pixels.
>> 우리는 모델에 대한 경험적 실험을 수행한다. 우리는 매개 변수 영향(분류를 위한 계층, 패치 크기)과 모델의 복잡성(네트워크의 계층 수, 각 계층의 채널 수)을 연구한다. 모든 텍스처에 대한 보편적인 최적 설계는 아닐 수 있지만, 우리의 연구는 모델이 다양한 경우에 어떻게 동작하는지에 대해 몇 가지 조명한다. 공정한 비교를 위해 본 연구의 예제 텍스처를 고정 크기(128x128픽셀)로 확장하고 합성 출력을 256x256픽셀로 요구한다.

![Figure 8](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)MGAN/Figure-8.JPG)

> Fig. 8: Different layers and patch sizes for training the discriminative netowrk. Input image credit: “ropenet” from the project link of [19].
>> 그림 8: 차별적 네트워크를 훈련시키기 위한 다양한 레이어 및 패치 크기. 입력 이미지 크레딧: [19]의 프로젝트 링크에서 "로프넷"을 참조하십시오.

> **Visualizing decoder features:** We visualize the learned filters of decoder $G$ in Figure 7. These features are directly decoded from a one-hot input vector. Individual patches are similar to, but not very faithfully matching the example textures (reconfirming the semi-distributed and non-linear nature of the encoding). Nonetheless, visual similarity of such artificial responses seems strong enough for synthesizing new images. 
>> **디코더 기능 시각화:** 우리는 그림 7에서 디코더 $G$의 학습된 필터를 시각화한다. 이러한 기능은 원핫 입력 벡터에서 직접 디코딩됩니다. 개별 패치는 예제 텍스처와 유사하지만 매우 충실하게 일치하지는 않는다(인코딩의 반분산 및 비선형 특성을 재확인). 그럼에도 불구하고, 그러한 인공 반응의 시각적 유사성은 새로운 이미지를 합성하기에 충분히 강한 것으로 보인다.

> **Parameters:** Next, we study the influence of changing the input layers for the discriminative network. To do so we run unguided texture synthesis with discriminator $D$ taking layer relu2 1, relu3 1, and relu4 1 of VGG 19 as the input. We use patch sizes of 16, 8 and 4 respectively for the three options, so they have the same receptive field of 32 image pixels (approximately; ignoring padding). The first three results in Fig. 8 shows the results of these three settings. Lower layers (relu2 1 ) produce sharper appearances but at the cost of losing form and structure of the texture. Higher layer (relu4 1 ) preserves coarse structure better (such as regularity) but at the risk of being too rigid for guided scenarios. Layer relu3 1 offers a good balance between quality and flexibility. We then show the influence of patch size: We fix the input layer of $D$ to be relu3 1, and compare patch size of 4 and 16 to with the default setting of 8. The last two results in Fig. 8 shows that such change will also affect the rigidity of the model: smaller patches increase the flexibility and larger patches preserve better structure.
>> **모수:** 다음으로, 우리는 차별적 네트워크에 대한 입력 계층 변경의 영향을 연구한다. 이를 위해 VGG 19의 레이어 relu21, relu31 및 relu41을 입력으로 하는 판별기 $D$를 사용하여 무유도 텍스처 합성을 실행한다. 우리는 세 가지 옵션에 대해 각각 16, 8 및 4의 패치 크기를 사용하므로, 그들은 32개의 이미지 픽셀의 동일한 수용 필드를 갖는다(대략; 패딩 무시). 그림 8의 처음 세 가지 결과는 이 세 가지 설정의 결과를 보여준다. 하위 레이어(relu 21)는 더 날카로운 외관을 만들어내지만 텍스처의 형태와 구조를 잃게 된다. 상위 계층(relu 41)은 거친 구조를 더 잘 보존하지만(예: 규칙성) 유도 시나리오에 너무 강직할 위험이 있다. 레이어 relu31은 품질과 유연성 사이에서 좋은 균형을 제공합니다. 그런 다음 패치 크기의 영향을 보여 줍니다. $D$의 입력 계층을 relu31로 고정하고 패치 크기 4와 16을 기본 설정 8과 비교한다. 그림 8의 마지막 두 결과는 그러한 변화가 모델의 강성에도 영향을 미칠 것이라는 것을 보여준다. 즉, 패치가 작을수록 유연성이 증가하고 패치가 클수록 구조가 더 잘 보존된다는 것이다.

> **Complexity:** We now study the influence of 1) the number of layers in the networks and 2) the number of channels in each layer. We first vary the $D$ by removing the convolutional layer. Doing so reduces the depth of the network and in consequence the synthesis quality (first column, Fig. 9). Bringing this convolutional layer back produces smoother synthesis (second column, Fig. 9). However, in these examples the quality does not obviously improves with more additional layers (third column, Fig. 9). 
>> **복잡도:** 우리는 이제 1) 네트워크의 계층 수와 2) 각 계층의 채널 수의 영향을 연구한다. 먼저 컨볼루션 레이어를 제거하여 $D$를 변화시킨다. 그렇게 하면 네트워크의 깊이가 줄어들고 결과적으로 합성 품질(첫 번째 열, 그림 9)이 감소합니다. 이 컨볼루션 레이어를 다시 가져오면 합성이 더 원활해집니다(두 번째 열, 그림 9). 그러나, 이러한 예에서, 층이 더 추가되어도 품질은 분명히 향상되지 않는다(세 번째 열, 그림 9).

> Testing the $D$ with 4, 64, and 128 channels for the convolutional layer, we observe in general that decreasing the number of channels leads to worse results (fourth column, Fig. 9), but there is no significance difference between 64 channels and 128 channels (second column v.s. fifth column). The complexity requirements also depend on the actual texture. For example, the ivy texture is a rather simple MRF, so the difference between 4 channel and 64 channel are marginal, unlike in the other two cases. 
>> 컨볼루션 레이어에 대한 4, 64 및 128 채널을 사용하여 $D$를 테스트하면서, 일반적으로 채널 수를 줄이면 더 나쁜 결과가 발생한다는 것을 관찰한다(네 번째 열, 그림 9). 그러나 64개 채널과 128개 채널(두 번째 열 대 다섯 번째 열) 사이에는 유의한 차이가 없다. 복잡도 요구사항은 실제 질감에 따라 다릅니다. 예를 들어 아이비 텍스처는 다소 단순한 MRF이므로 다른 두 경우와는 달리 4채널과 64채널의 차이는 미미하다.

![Figure 9](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)MGAN/Figure-9.JPG)

> Fig. 9: Different depths for training the discriminative netowrk. The input textures are “ropenet” from the project link of [19], [34]’s “Ivy”, and Pablo Picasso’s “self portrait 1907”.
>> 그림 9: 차별적 네트워크를 훈련시키기 위한 다양한 깊이. 입력 텍스처는 [19], [34]의 "아이비", 파블로 피카소의 "자기 초상화 1907"의 프로젝트 링크에서 나온 "로페넷"이다.

> Next, we fix the discriminative network and vary the complexity of the generative network. We notice some quality loss when removing the first convolutional layer from the decoder, or reducing the number of channels for all layers, and only very limited improvement from a more complex design. However the difference is not very significant. This is likely because the networks are all driven by the same discriminative network, and the reluctance of further improvement indicates there are some non-trivial information from the deconvolutional process that can not be recovered by a feed forward process. In particular, the fractionally-strided convolutions does not model the nonlinear behaviour of the max-pooling layer, hence often produces alias patterns. These become visible in homogeneous, texture-less area. To avoid artifacts but encourage texture variability, we can optionally add Perlin noise [28] to the input image. 
>> 다음으로, 우리는 차별적 네트워크를 고치고 생성 네트워크의 복잡성을 변화시킨다. 우리는 디코더에서 첫 번째 컨볼루션 레이어를 제거하거나 모든 레이어에 대한 채널 수를 줄일 때 일부 품질 손실을 알아차리고, 더 복잡한 설계에서 매우 제한된 개선만 본다. 그러나 그 차이는 그다지 크지 않다. 이는 네트워크가 모두 동일한 차별적 네트워크에 의해 구동되기 때문일 수 있으며, 추가 개선에 대한 거부감은 피드포워드 프로세스에 의해 복구할 수 없는 디콘볼루션 프로세스의 일부 중요하지 않은 정보가 있음을 나타낸다. 특히, 부분 가닥이 있는 컨볼루션은 최대 풀링 계층의 비선형 동작을 모델링하지 않으므로 종종 별칭 패턴을 생성한다. 이들은 균질하고 질감이 없는 영역에서 볼 수 있다. 아티팩트를 피하지만 텍스처의 가변성을 높이기 위해 입력 영상에 Perlin 노이즈 [28]를 선택적으로 추가할 수 있습니다.

> **Initialization** Usually, networks are initialized with random values. However we found $D$ has certain generalization ability. Thus, for transferring the same texture to different images with MDANs, a previously trained network can serve as initialization. Figure 10 shows initialization with pre-trained discriminative network (that has already transferred 50 face images) produces good result with only 50 iterations. In comparison, random initialization does not produce comparable quality even after the first 500 iterations. It is useful to initialize $G$ with an auto-encoder that directly decodes the input feature to the original input photo. Doing so essentially approximates the process of inverting VGG 19, and let the whole adversarial network to be trained more stably. 
>> **초기화** 일반적으로 네트워크는 임의의 값으로 초기화됩니다. 그러나 우리는 $D$가 특정 일반화 능력을 가지고 있다는 것을 발견했다. 따라서, MDAN을 사용하여 동일한 질감을 다른 이미지로 전송하는 경우, 이전에 훈련된 네트워크가 초기화 역할을 할 수 있다. 그림 10은 사전 훈련된 차별적 네트워크(이미 50개의 얼굴 이미지를 전송한 경우)를 사용한 초기화가 50회만 반복해도 좋은 결과를 낳는 것을 보여준다. 이에 비해 랜덤 초기화는 처음 500회 반복 후에도 동등한 품질을 제공하지 않는다. 입력 기능을 원본 입력 사진으로 직접 디코딩하는 자동 인코더로 $G$를 초기화하는 것이 유용하다. 그렇게 하는 것은 본질적으로 VGG 19를 반전시키는 과정에 근사하며, 전체 적대 네트워크가 더 안정적으로 훈련되도록 한다.

![Figure 10](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)MGAN/Figure-10.JPG)

> Fig. 10: Different initializations of the discriminative networks. The reference texture is Pablo Picasso’s “self portrait 1907”.
>> 그림 10: 차별적 네트워크의 다양한 초기화. 참고문헌은 파블로 피카소의 "자화상 1907년"이다.

> **The role of VGG:** We also validate the importance of the pre-trained VGG 19 network. As the last two pictures in Figure 10 show, training a discriminative network from scratch (from pixel to class label [29]) yields significantly worse results. This has also been observed by Ulyanov et al. [32]. Our explanation is that much of the statistical power of VGG 19 stems from building shared feature cascades for a diverse set of images, thereby approaching human visual perception more closely than a network trained with a limited example set.
>> **VGG의 역할:** 사전 훈련된 VGG 19 네트워크의 중요성도 검증한다. 그림 10의 마지막 두 그림에서 알 수 있듯이, 차별적 네트워크를 처음부터 훈련(픽셀에서 클래스 레이블로 [29])하면 훨씬 더 나쁜 결과가 나온다. 이는 울리야노프 외 연구진도 관찰했다. [32. 우리의 설명은 VGG 19의 통계적 힘의 상당 부분이 다양한 이미지 세트에 대한 공유 기능 캐스케이드를 구축하여 제한된 예제 세트로 훈련된 네트워크보다 인간의 시각적 인식에 더 가깝게 접근한 데서 비롯된다는 것이다.

### $\mathbf{5\;Results}$

> This section shows examples of our MGANs synthesis results. We train each model with 100 randomly selected images from ImageNet, and a single example texture. We first produce 100 transferred images using the MDANs model, then regularly sample 128-by-128 image croppings as training data for MGANs. In  total we have around 16k samples for each model. The training take as about 12 min per epoch. Each epoch min-batches through all samples in random order. We train each texture for upto five epochs. 
>> 이 섹션은 우리의 MGANs 합성 결과의 예를 보여준다. 우리는 ImageNet에서 무작위로 선택한 100개의 이미지와 단일 예제 질감으로 각 모델을 훈련시킨다. 우리는 먼저 MDANs 모델을 사용하여 100개의 전송된 이미지를 생성한 다음 MGAN에 대한 훈련 데이터로 128x128 이미지 자르기 샘플을 정기적으로 샘플링한다. 모델별로 총 16k개 정도의 샘플이 있습니다. 훈련 시간은 에폭당 약 12분이다. 각 에폭은 임의의 순서로 모든 샘플을 최소 배치한다. 우리는 각 질감을 최대 5세기 동안 훈련시킨다.

![Figure 11](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)MGAN/Figure-11.JPG)

> Fig. 11: Comparisons with previous methods. See more examples in our supplementary report. Results of Ulyanov et al. [32], Gatys et al. [8] and input images are from [32].
>> 그림 11: 이전 방법과의 비교 보충 보고서에서 더 많은 예를 참조하십시오. Ulyanov 등의 결과. [32], 게이티 외 [8] 입력 영상은 [32]에서 가져온 것입니다.

> Figure 11 compares our synthesis results with previous methods. First, our method has a very different character in comparison to the methods that use global statistics [32,8]: It transfers texture more coherently, such as the hair of Lena was consistently mapped to dark textures. In contrast, the Gaussian model [32,8] failed to keep such consistency, and have difficulty in transferring complicated image content. For example the eyes in [32]’s result and the entire face in [8]’s result are not textured. Since these features do not fit a Gaussian distribution, they are difficult to be constrained by a Gram matrix. The other local patch based approach [21] produces the most coherent synthesis, due to the  use of non-parametric sampling. However, their method requires patch matching so is significantly slower (generate this 384-by-384 picture in 110 seconds). Our method and Ulyanov et al. [32] run at the same level of speed; both bring significantly improvement of speed over Gatys et al. [8] (500 times faster) and Li et al. [21] (5000 times faster).
>> 그림 11은 우리의 합성 결과를 이전의 방법과 비교한다. 첫째, 우리의 방법은 글로벌 통계를 사용하는 방법과 비교할 때 매우 다른 특성을 갖는다[32,8]. 레나의 머리카락이 어두운 질감에 지속적으로 매핑되는 등 질감을 보다 일관성 있게 전달한다. 대조적으로, 가우스 모델 [32,8]은 그러한 일관성을 유지하지 못했고, 복잡한 이미지 콘텐츠를 전송하는데 어려움을 겪었다. 예를 들어 [32] 결과의 눈과 [8] 결과의 전체 얼굴은 텍스처링되지 않는다. 이러한 특징은 가우스 분포에 맞지 않기 때문에 그램 행렬에 의해 제약되기 어렵다. 다른 로컬 패치 기반 접근 방식[21]은 비모수 샘플링의 사용으로 인해 가장 일관된 합성을 생성한다. 그러나 이들의 방법은 패치 매칭을 필요로 하므로 속도가 상당히 느립니다(이 384 x 384 사진을 110초 만에 생성합니다). 우리의 방법과 Ulyanov 외. [32] 같은 수준의 속도로 주행하며, 두 가지 모두 Gatys 등에 비해 상당히 향상된 속도를 제공합니다. [8] (500배 더 빠름) 및 Li 등. [21] (배 더 빠릅니다.)

![Figure 12](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)MGAN/Figure-12.JPG)

> Fig. 12: More comparisons with Texture Networks [32]. Results of [32] and input images are from [32].
>> 그림 12: Texture Networks와 더 많은 비교 [32] [32] 및 입력 영상의 결과는 [32]에서 가져온 것입니다.

> Figure 12 further discuss the difference between the Gaussian based method [32] and our method4 . In general [32] produces more faithful color distributions in respect to the style image. It also texture background better (see the starry night example) due to the learning of mapping from noise to Gaussian distribution. On the other hand, our method produces more coherent texture transfer and does not suffer the incapability of Gaussian model for more complex scenarios, such as the facade in both examples. In comparison [32] produces either too much or too little textures in such complex regions.
>> 그림 12는 가우스 기반 방법[32]과 우리의 방법4의 차이를 더 자세히 설명한다. 일반적으로 [32]는 스타일 이미지와 관련하여 더 충실한 색 분포를 생성한다. 또한 노이즈에서 가우스 분포로 매핑하는 학습으로 인해 배경을 더 잘 텍스처링한다(별이 빛나는 밤의 예 참조). 반면, 우리의 방법은 보다 일관된 텍스처 전송을 생성하며 두 예제의 파사드와 같은 더 복잡한 시나리오에 대한 가우스 모델의 능력을 겪지 않는다. 이와 비교하여 [32]는 그러한 복잡한 영역에서 너무 많거나 너무 적은 질감을 생성한다.

![Figure 13](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)MGAN/Figure-13.JPG)

> Fig. 13: Generate random textures by decoding from Brown noise
>> 그림 13: 브라운 노이즈를 디코딩하여 랜덤 텍스처 생성

![Figure 14](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)MGAN/Figure-14.JPG)

> Fig. 14: Decoding a 1080-by-810 video. We achieved the speed of 8Hz. Input video is credited to flickr user macro antonio torres.
>> 그림 14: 1080 x 810 비디오 디코딩 우리는 8Hz의 속도를 달성했다. 입력 비디오는 플리커 사용자 매크로 안토니오 토레스의 것으로 인정된다.

> Figure 13 shows that unguided texture synthesis is possible by using the trained model to decode noise input. In this case, Perlin noise5 images are forwarded through VGG 19 to generate feature maps for the decoder. To our surprise, the model that was trained with random ImageNet images is able to decode such features maps to plausible textures. This again shows the generalization ability of our model. Last, Figure 13 shows our video decoding result. As a feed-forward process our method is not only faster but also relatively more temporally coherent than the deconvolutional methods.
>> 그림 13은 훈련된 모델을 사용하여 노이즈 입력을 디코딩함으로써 무유도 텍스처 합성이 가능하다는 것을 보여준다. 이 경우 Perlin 노이즈 5 이미지는 VGG 19를 통해 전달되어 디코더에 대한 피쳐 맵을 생성합니다. 놀랍게도, 무작위 ImageNet 이미지로 훈련된 모델은 그러한 특징 맵을 그럴듯한 질감으로 디코딩할 수 있다. 이것은 우리 모델의 일반화 능력을 다시 보여준다. 마지막으로, 그림 13은 비디오 디코딩 결과를 보여줍니다. 피드포워드 프로세스로서 우리의 방법은 디콘볼루션 방법보다 더 빠를 뿐만 아니라 상대적으로 더 시간적으로 일관적이다.

![Figure 15](https://raw.githubusercontent.com/maizer2/gitblog_img/main/img/1.%20Computer%20Engineering/1.7.%20Literature%20Review/2022-06-12-(GAN)MGAN/Figure-15.JPG)

> Fig. 15: Left: speed comparison between our method and Ulyanov et al. [32]. Right: speed comparison (in log space) between our method and Gatys et al. [8], Li et al. [21], and Ulyanov et al. [32]. The feed-forward methods (ours and [32]) are significantly faster than Gatys et al. [8] (500 times speed up) and Li et al. [21] (5000 times speed up).
>> 그림 15: 왼쪽: 우리의 방법과 Ulyanov 외 연구진 사이의 속도 비교. [32. 맞습니다. 우리의 방법과 Gatys 등 사이의 속도 비교(로그 공간) [8], Lie et al. [21] 및 Ulyanov 외 [32. 피드포워드 방법(우리와 [32])은 Gatys 등에 비해 상당히 빠르다. [8] (500배 속도 향상) 및 Li 외 [21] (속도의 10배)

> Last but not the least, we provide details for the time/memory usage of our method. The time measurement is based on a standard benchmark framework [2] (Figure 15): Our speed is at the same level as the concurrent work by Ulyanov et al. [32], who also use a feed-forward approach, perform significantly faster than previous deconvolution based approaches [8,21]. More precisely, both our method and Ulyanov et al. [32] are able to decode 512-by-512 images at 25Hz (Figure 15, left), while [32] leads the race by a very small margin. The time cost of both methods scale linearly with the number of pixels in the image. For example, our method cost 10 ms for a 256-by-256 image, 40 ms for a 512-by-512 image, and 160 ms for a 1024-by-1024 image. Both methods show a very significant improvement in speed over previous deconvolutional methods such as Gatys et al. [8] and Li et al. [21] (Figure 15 right): about 500 times faster than Gatys et al. [8], and 5000 times faster than Li et al. [21]. In the meantime our method is also faster than most traditional pixel based texture synthesizers (which rely on expensive nearest-neighbor searching). A possible exceptions would be a GPU implementation of “Patch Match” [1], which could run at comparable speed. However, it provides the quality benefits (better blending, invariance) of a deepneural-network method (as established in previous work [8,21]).
>> 마지막으로, 방법의 시간/메모리 사용에 대한 세부 정보를 제공합니다. 시간 측정은 표준 벤치마크 프레임워크[2]를 기반으로 합니다(그림 15). 우리의 속도는 Ulyanov 등의 동시 작업과 동일한 수준이다. 피드포워드 접근 방식을 사용하는 [32]는 이전의 디콘볼루션 기반 접근 방식보다 훨씬 더 빠른 성능을 발휘한다[8,21]. 더 정확히 말하면, 우리의 방법과 Ulyanov 등. [32]는 25Hz에서 512x512 이미지를 디코딩할 수 있으며(그림 15, 왼쪽) [32]는 매우 작은 차이로 선두를 달리고 있습니다. 두 방법의 시간 비용은 이미지의 픽셀 수에 따라 선형적으로 확장된다. 예를 들어, 우리의 방법은 256x256 이미지의 경우 10ms, 512x512 이미지의 경우 40ms, 1024x1024 이미지의 경우 160ms가 소요된다. 두 방법 모두 Gatys 등과 같은 이전 디콘볼루션 방법에 비해 속도가 매우 크게 향상되었음을 보여준다. [8] 그리고 Lie et al. [21] (오른쪽 그림 15): Gatys 외 연구진보다 약 500배 더 빠릅니다. [8], 그리고 Li 외 연구진보다 5000배 더 빠릅니다. [21. 한편, 우리의 방법은 대부분의 전통적인 픽셀 기반 텍스처 신시사이저(고가의 가장 가까운 이웃 검색에 의존하는)보다 빠르다. 가능한 예외는 비슷한 속도로 실행될 수 있는 "패치 매치"[1]의 GPU 구현일 수 있다. 그러나, 그것은 (이전 연구에서 확립된 바와 같이) 심층 신경망 방법의 품질 이점(더 나은 혼합, 불변성)을 제공한다.

> Memory-wise, our generative model takes 70 Mb memory for its parameters(including the VGG network till layer Relu4 1). At runtime, the required memory to decode a image linearly depends on the image’s size: for a 256-by256 picture it takes about 600 Mb, and for a 512-by-512 picture it requires about 2.5 Gb memory. Notice memory usage can be reduced by subdividing the input photo into blocks and run the decoding in a scanline fashion. However, we do not further explore the optimization of memory usage in this paper.
>> 메모리 측면에서, 우리의 생성 모델은 매개 변수(Relu41 계층까지 VGG 네트워크 포함)를 위해 70Mb 메모리를 사용한다. 256 x 256 사진의 경우 약 600Mb, 512 x 512 사진의 경우 약 2.5Gb 메모리가 필요하다. 입력된 사진을 블록으로 분할하고 스캔라인 방식으로 디코딩을 실행하여 메모리의 사용을 줄일 수 있습니다. 그러나 본 논문에서는 메모리 사용의 최적화를 추가로 탐구하지 않는다.

### $\mathbf{6\;Limitation}$

> Our current method works less well with non-texture data. For example, it failed to transfer facial features between two difference face photos. This is because facial features can not be treated as textures, and need semantic understanding (such as expression, pose, gender etc.). A possible solution is to couple our model with the learning of object class [29] so the local statistics is better conditioned. For synthesizing photo-realistic textures, Li et al. [21] often produces better results due to its non-parametric sampling that prohibits data distortion. However, the rigidity of their model restricts its application domain. Our method works better with deformable textures, and runs significantly faster. 
>> 우리의 현재 방법은 텍스처가 아닌 데이터에서는 덜 잘 작동한다. 예를 들어, 그것은 두 개의 다른 얼굴 사진 사이에 얼굴 특징을 전달하는 데 실패했다. 얼굴 특징을 질감으로 다룰 수 없고, 의미론적 이해(표정, 자세, 성별 등)가 필요하기 때문이다. 가능한 해결책은 모델을 객체 클래스 [29]의 학습과 결합하여 로컬 통계를 더 잘 조정하는 것이다. 포토 리얼한 텍스처를 합성하기 위해, Li 외. [21] 데이터 왜곡을 방지하는 비모수 샘플링으로 인해 더 나은 결과를 생성하는 경우가 많다. 그러나 그들의 모델의 강성은 그것의 응용 영역을 제한한다. 우리의 방법은 변형 가능한 질감에서 더 잘 작동하고, 훨씬 더 빨리 실행된다.

> Our model has a very different character compared to Gaussian based models [8,32]. By capturing a global feature distribution, these other methods are able to better preserve the global “look and feels” of the example texture. In contrast, our model may deviate from the example texture in, for example, the global color distribution. However, such deviation may not always be bad when the content image is expected to play a more important role. 
>> 우리의 모델은 가우스 기반 모델[8,32]과 비교할 때 매우 다른 특성을 가지고 있다. 전역 피쳐 분포를 캡처함으로써 이러한 다른 방법은 예제 텍스처의 전역 "모양과 느낌"을 더 잘 보존할 수 있다. 대조적으로, 우리의 모델은 예를 들어, 전체 색상 분포에서 예제 질감에서 벗어날 수 있다. 그러나 콘텐츠 이미지가 더 중요한 역할을 할 것으로 예상되는 경우 이러한 편차가 항상 나쁜 것은 아닙니다.

> Since our model learns the mapping between different depictions of the same content, it requires features highly invariant features. For this reason we use the pre-trained VGG 19 network. This makes our method weaker in dealing with highly stationary backgrounds (sky, out of focus region etc.) due to their weak activation from VGG 19. We observed that in general statistics based methods [32,8] generate better textures for areas that has weak content, and our method works better for areas that consist of recognizable features. We believe it is valuable future work to combine the strength of both methods. 
>> 우리의 모델은 동일한 콘텐츠의 서로 다른 묘사 간의 매핑을 학습하기 때문에 매우 불변한 특징이 필요하다. 이러한 이유로 우리는 사전 훈련된 VGG 19 네트워크를 사용한다. 이는 VGG 19로부터의 약한 활성화로 인해 고정도가 높은 배경(하늘, 초점 이탈 영역 등)을 처리하는 데 우리의 방법이 약해진다. 우리는 일반적인 통계 기반 방법[32,8]에서 내용이 약한 영역에서 더 나은 질감을 생성하고, 우리의 방법은 인식할 수 있는 기능으로 구성된 영역에서 더 잘 작동한다는 것을 관찰했다. 우리는 두 가지 방법의 장점을 결합하는 것이 미래의 가치 있는 일이라고 믿는다.

> Finally, we discuss the noticeable difference between the results of MDANs and MGANs. The output of MGANs is often more consistent with the example texture, this shows MGANs’ strength of learning from big data. MGANs has weakness in flat regions due to the lack of iterative optimization. More sophisticated architectures such as the recurrent neural networks can bring in state information that may improve the result.
>> 마지막으로, 우리는 MDAN과 MGAN의 결과 사이의 눈에 띄는 차이에 대해 논의한다. MGAN의 출력은 종종 예제 텍스처와 더 일치하며, 이는 MGAN의 빅 데이터 학습 강도를 보여준다. MGAN은 반복적 최적화의 부족으로 인해 평탄한 지역에서 약점이 있다. 반복 신경망과 같은 보다 정교한 아키텍처는 결과를 개선할 수 있는 상태 정보를 가져올 수 있다.

### $\mathbf{7\;Conclusion}$

> The key insight of this paper is that adversarial generative networks can be applied in a Markovian setting to learn the mapping between different depictions of the same content. We develop a fully generative model that is trained from a single texture example and randomly selected images from ImageNet. Once trained, our model can decode brown noise to realistic texture, or photos into artworks. We show our model has certain advantages over the statistics based methods [32,8] in preserving coherent texture for complex image content. Once trained (which takes about an hour per example), synthesis is extremely fast and offers very attractive invariance for style transfer.
>> 이 논문의 핵심 통찰력은 적대적 생성 네트워크를 마르코프 환경에서 적용하여 동일한 콘텐츠의 서로 다른 묘사 간의 매핑을 학습할 수 있다는 것이다. 단일 텍스처 예제와 ImageNet에서 무작위로 선택한 이미지에서 훈련된 완전 생성 모델을 개발한다. 일단 훈련을 받으면, 우리의 모델은 갈색 소음을 사실적인 질감으로 디코딩하거나 사진을 예술 작품으로 만들 수 있다. 우리는 우리의 모델이 복잡한 이미지 콘텐츠에 대한 일관성 있는 질감을 보존하는 데 통계 기반 방법[32,8]보다 특정한 이점을 가지고 있음을 보여준다. 일단 훈련되면(예당 약 1시간 소요), 합성은 매우 빠르고 스타일 전달에 매우 매력적인 불변성을 제공한다.

> Our method is only one step in the direction of learning generative models for images. An important avenue for future work would be to study the broader framework in a big-data scenario to learn not only Markovian models but also include coarse-scale structure models. This additional invariance to image layout could, as a side effect, open up ways to also use more training data for the Markovian model, thus permitting more complex decoders with stronger generalization capability over larger classes. The ultimate goal would be a directly decoding, generative image model of large classes of real-world images. 
>> 우리의 방법은 이미지에 대한 생성 모델을 학습하는 방향으로의 한 단계일 뿐이다. 향후 작업의 중요한 방법은 마르코프 모델뿐만 아니라 거친 규모의 구조 모델도 학습하기 위해 빅 데이터 시나리오에서 더 넓은 프레임워크를 연구하는 것이다. 이미지 레이아웃에 대한 이러한 추가적인 불변성은 부작용으로 마르코프 모델에 더 많은 훈련 데이터를 사용할 수 있는 방법을 열 수 있으며, 따라서 더 큰 클래스에 비해 더 강력한 일반화 기능을 가진 더 복잡한 디코더를 허용할 수 있다. 궁극적인 목표는 대규모 클래스의 실제 이미지를 직접 디코딩하고 생성하는 이미지 모델이 될 것이다.

### $\mathbf{Acknowledgments}$

> This work has been partially supported by the Intel Visual Computing Institute and the Center for Computational Science Mainz. We like to thank Bertil Schmidt and Christian Hundt for providing additional computational resources.
>> 이 작업은 인텔 비주얼 컴퓨팅 인스티튜트와 컴퓨터 과학 메인즈 센터에서 부분적으로 지원되었다. 우리는 추가적인 계산 자원을 제공한 베르틸 슈미트와 크리스찬 헌트에게 감사한다.